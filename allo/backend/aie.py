# Copyright Allo authors. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
# mlir-aie commit: 8329b6
# pylint: disable=consider-using-with, bad-builtin, no-name-in-module, too-many-branches, too-many-nested-blocks

import os
import subprocess
import re
import math
import copy
import numpy as np
from .._mlir.ir import (
    IntegerAttr,
    IntegerType,
    DenseI64ArrayAttr,
    RankedTensorType,
    MemRefType,
    FunctionType,
    TypeAttr,
    Location,
    StridedLayoutAttr,
    InsertionPoint,
    FlatSymbolRefAttr,
    StringAttr,
)
from .._mlir.dialects import (
    memref as memref_d,
    func as func_d,
)
from .._mlir.passmanager import PassManager as mlir_pass_manager

from .vitis import read_tensor_from_file
from ..utils import (
    get_func_inputs_outputs,
    get_dtype_and_shape_from_type,
    get_element_type_from_str,
)
from .utils import format_str, format_code
from .vitis import ctype_map


host_header = """
//=============================================================================
// Auto generated by Allo
//=============================================================================

#include <boost/program_options.hpp>
#include <cstdint>
#include <fstream>
#include <iostream>
#include <sstream>
#include <string>
#include <vector>

#include "xrt/xrt_bo.h"
#include "xrt/xrt_device.h"
#include "xrt/xrt_kernel.h"

#include "test_utils.h"

namespace po = boost::program_options;

int main(int argc, const char *argv[]) {

  // ------------------------------------------------------
  // Parse program arguments
  // ------------------------------------------------------
  po::options_description desc("Allowed options");
  po::variables_map vm;
  test_utils::add_default_options(desc);

  test_utils::parse_options(argc, argv, desc, vm);
  int verbosity = vm["verbosity"].as<int>();
  int do_verify = vm["verify"].as<bool>();
  int n_iterations = vm["iters"].as<int>();
  int n_warmup_iterations = vm["warmup"].as<int>();
  int trace_size = vm["trace_sz"].as<int>();

  // Load instruction sequence
  std::vector<uint32_t> instr_v =
      test_utils::load_instr_sequence(vm["instr"].as<std::string>());
  if (verbosity >= 1)
    std::cout << "Sequence instr count: " << instr_v.size() << "\\n";

  // ------------------------------------------------------
  // Get device, load the xclbin & kernel and register them
  // ------------------------------------------------------
  // Get a device handle
  unsigned int device_index = 0;
  auto device = xrt::device(device_index);

  // Load the xclbin
  if (verbosity >= 1)
    std::cout << "Loading xclbin: " << vm["xclbin"].as<std::string>() << "\\n";
  auto xclbin = xrt::xclbin(vm["xclbin"].as<std::string>());

  // Load the kernel
  if (verbosity >= 1)
    std::cout << "Kernel opcode: " << vm["kernel"].as<std::string>() << "\\n";
  std::string Node = vm["kernel"].as<std::string>();

  // Get the kernel from the xclbin
  auto xkernels = xclbin.get_kernels();
  auto xkernel = *std::find_if(xkernels.begin(), xkernels.end(),
                               [Node, verbosity](xrt::xclbin::kernel &k) {
                                 auto name = k.get_name();
                                 if (verbosity >= 1) {
                                   std::cout << "Name: " << name << std::endl;
                                 }
                                 return name.rfind(Node, 0) == 0;
                               });
  auto kernelName = xkernel.get_name();

  // Register xclbin
  if (verbosity >= 1)
    std::cout << "Registering xclbin: " << vm["xclbin"].as<std::string>()
              << "\\n";
  device.register_xclbin(xclbin);

  // Get a hardware context
  if (verbosity >= 1)
    std::cout << "Getting hardware context.\\n";
  xrt::hw_context context(device, xclbin.get_uuid());

  // Get a kernel handle
  if (verbosity >= 1)
    std::cout << "Getting handle to kernel:" << kernelName << "\\n";
  auto kernel = xrt::kernel(context, kernelName);

  // ------------------------------------------------------
  // Initialize input/ output buffer sizes and sync them
  // ------------------------------------------------------
  auto bo_instr = xrt::bo(device, instr_v.size() * sizeof(int),
                          XCL_BO_FLAGS_CACHEABLE, kernel.group_id(1));
  void *bufInstr = bo_instr.map<void *>();
  memcpy(bufInstr, instr_v.data(), instr_v.size() * sizeof(int));

  std::ofstream ofile("output.data");
  if (!ofile.is_open()) {
      std::cerr << "Error: Could not open output file.\\n";
      return 1;
  }

"""

file_close_str = """  ofile.close();
  if (verbosity >= 1)
    std::cout << "Array has been written to output.data.\\n";
  return 0;
}
"""


def codegen_host(kernel_input_args):
    code = host_header
    input_args = [
        input_arg
        for sublist in kernel_input_args.values()
        for input_arg in sublist[:-1]
    ]
    output_args = [sublist[-1] for sublist in kernel_input_args.values()]
    with format_code(indent=2):
        # write input data
        for i, (dtype, shape) in enumerate(input_args):
            dtype = ctype_map[dtype]
            code += format_str(f'std::ifstream ifile{i}("input{i}.data");')
            code += format_str(f"if (!ifile{i}.is_open()) {{")
            code += format_str(
                '  std::cerr << "Error: Could not open input file.\\n";', strip=False
            )
            code += format_str("  return 1;", strip=False)
            code += format_str("}")
            size = np.prod(shape)
            code += format_str(
                f"auto bo_in{i} = xrt::bo(device, {size} * sizeof({dtype}),"
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({i + 3}));"
                )
            code += format_str(f"{dtype} *bufIn{i} = bo_in{i}.map<{dtype} *>();")
            code += format_str(f"std::vector<{dtype}> srcVec{i};")
            code += format_str(f"for (int i = 0; i < {size}; i++) {{")
            with format_code(indent=4):
                code += format_str(f"{dtype} num;")
                code += format_str(f"ifile{i} >> num;")
                code += format_str(f"srcVec{i}.push_back(num);")
            code += format_str("}")
            code += format_str(
                f"memcpy(bufIn{i}, srcVec{i}.data(), (srcVec{i}.size() * sizeof({dtype})));"
            )
        for i, (dtype, shape) in enumerate(output_args):
            dtype = ctype_map[dtype]
            out_size = np.prod(shape)
            code += format_str(
                f"\nauto bo_out{i} = xrt::bo(device, {out_size} * sizeof({dtype}),",
                strip=False,
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({len(input_args) + 2 + i}));"
                )
        code += format_str("if (verbosity >= 1)")
        code += format_str(
            '  std::cout << "Writing data into buffer objects.\\n";', strip=False
        )
        code += format_str("\nbo_instr.sync(XCL_BO_SYNC_BO_TO_DEVICE);", strip=False)
        for i in range(len(input_args)):
            code += format_str(f"bo_in{i}.sync(XCL_BO_SYNC_BO_TO_DEVICE);")
        # run kernels
        code += format_str("if (verbosity >= 1)")
        code += format_str('  std::cout << "Running Kernel.\\n";', strip=False)
        code += format_str(
            "\nauto start = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str("unsigned int opcode = 3;", strip=False)
        inbufs = ", ".join([f"bo_in{i}" for i in range(len(input_args))])
        outbufs = ", ".join([f"bo_out{i}" for i in range(len(output_args))])
        code += format_str("// gid: (opcode, instr, instr_size, ...)")
        code += format_str(
            f"auto run = kernel(opcode, bo_instr, instr_v.size(), {inbufs}, {outbufs});"
        )
        code += format_str("run.wait();")
        code += format_str(
            "\nauto end = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str(
            "float npu_time = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();"
        )
        code += format_str(
            'std::cout << "NPU execution time: " << npu_time << "us\\n";'
        )
        # get results
        for i, (dtype, shape) in enumerate(output_args):
            dtype = ctype_map[dtype]
            out_size = np.prod(shape)
            code += format_str(
                f"\nbo_out{i}.sync(XCL_BO_SYNC_BO_FROM_DEVICE);", strip=False
            )
            code += format_str(f"{dtype} *bufOut{i} = bo_out{i}.map<{dtype} *>();")
            code += format_str(f"for (uint32_t i = 0; i < {out_size}; i++) {{")
            code += format_str(f'  ofile << *(bufOut{i} + i) << "\\n";', strip=False)
            code += format_str("}")
        code += format_str("\n// Close files", strip=False)
        for i in range(len(input_args)):
            code += format_str(f"ifile{i}.close();")
        code += file_close_str
    return code


def get_position_str(mapping, index):
    if len(mapping) == 1:
        return index
    indices = []
    for size in reversed(mapping):
        indices.append(index % size)
        index //= size
    return "_".join(map(str, reversed(indices)))


def get_public_funcs(mod):
    funcs = []
    for func in mod.body.operations:
        if (
            isinstance(func, func_d.FuncOp)
            and func.attributes["sym_name"].value != "top"
            and (
                "sym_visibility" not in func.attributes
                or func.attributes["sym_visibility"].value != "private"
            )
        ):
            funcs.append(func)
    return funcs


def inject_aie_kernels(mod):
    external_kernels = {}
    injected_kernels = set()
    with mod.context, Location.unknown():
        for func in mod.body.operations:
            external_kernels[func.attributes["sym_name"].value] = []
            for block in func.regions[0].blocks:
                for op in block.operations:
                    if (
                        op.operation.name == "linalg.add"
                        and len(MemRefType(op.inputs[0].type).shape) == 1
                    ):
                        # Inject AIE kernel
                        func_type = func_d.FunctionType.get(
                            [op.inputs[0].type, op.inputs[1].type, op.outputs[0].type],
                            [],
                        )
                        dtype = str(op.inputs[0].type.element_type)
                        if f"eltwise_add_{dtype}_vector" in injected_kernels:
                            continue
                        injected_kernels.add(f"eltwise_add_{dtype}_vector")
                        kernel = func_d.FuncOp(
                            f"eltwise_add_{dtype}_vector",
                            func_type,
                            ip=InsertionPoint(func),
                        )
                        kernel.attributes["sym_visibility"] = StringAttr.get("private")
                        func_d.CallOp(
                            [],
                            FlatSymbolRefAttr.get(f"eltwise_add_{dtype}_vector"),
                            [op.inputs[0], op.inputs[1], op.outputs[0]],
                            ip=InsertionPoint(op),
                        )
                        op.erase()
                        external_kernels[func.attributes["sym_name"].value].append(
                            "add"
                        )
    return external_kernels


def codegen_aie_mlir(
    mod,
    kernel_mappings,
    kernel_index_ranges,
    orig_kernel_input_args,
    kernel_func_arg_sizes,
    kernel_func_buf_dicts,
    kernel_dist_allocs,
    external_kernels,
):
    """
    Generates MLIR-AIE code with MLIR module and extra information

    Parameters
    ----------
    mod: allo._mlir.ir.Module
        The MLIR module built by allo.

    kernel_mappings: Dict[str, List[int]]
        The mapping of each kernel in the module.
        The key is the name of the kernel, and the value is a list of integers representing the mapping of the kernel.

    kernel_index_ranges: Dict[str, Tuple[int, int]]
        The index range of each kernel in the module.
        The key is the name of the kernel, and the value is a tuple of integers representing the start and end index of the kernel.

    orig_kernel_input_args: Dict[str, List[Tuple[str, List[int]]]]
        The original types of the argument of the kernels.
        The key is the name of the kernel, and the value is a list of tuples.
        Each tuple in the list stands for the type of each argument. e.g. ('i32', [16, 16]).
        For current version, we assume all function in the module have the same input arguments.

    kernel_func_arg_sizes: Dict[str, List[List[List[int]]]]
        The actual size of each argument that each function in each kernel will be using.
        The key is the name of the kernel, and the value is a list of lists.
        The first dim in each list stands for each function, the second dim stands for each argument, the last dim stands for shape.

    kernel_func_buf_dicts: Dict[str, List[Dict[str, Tuple[str, List[int]]]]]
        The local buffer each function creates.
        Each function in the list is a dictionary, where the key is the name of the buffer, and the value is the type of
        the element. e.g. ('i32', [16, 16]).

    kernel_dist_allocs: Dict[str, List[bool]]
        The allocation strategy for each argument in each kernel.
        More info can be find in function check_usage_intersection.

    external_kernels: Dict[str, List[str]]
        The external kernels that will be injected into the module.
        The key is the name of the function, and the value is a list of names of the external kernels.
    """
    kernel_input_args = copy.deepcopy(orig_kernel_input_args)
    code = format_str("module {", indent=0)
    num_tensors = sum(len(v) for v in kernel_input_args.values())
    mem_tile_size = 2 if num_tensors > 2 else 1
    device = "npu1_2col" if num_tensors > 2 else "npu1_1col"
    code += format_str(f"aie.device({device}) {{", indent=2)
    # external functions
    for func in mod.body.operations:
        if (
            isinstance(func, func_d.FuncOp)
            and "sym_visibility" in func.attributes
            and func.attributes["sym_visibility"].value == "private"
        ):
            code += format_str(str(func), indent=4)
    # create tiles
    code += format_str("%tile_shim = aie.tile(0, 0)")
    for mid in range(mem_tile_size):
        code += format_str(f"%tile_mem{mid} = aie.tile({mid}, 1)")
    # number of function declaration except top
    funcs = get_public_funcs(mod)
    buf_name_dicts = []
    # create compute tiles and buffers
    for kernel_name, (start, end) in kernel_index_ranges.items():
        mapping = kernel_mappings[kernel_name]
        func_buf_dicts = kernel_func_buf_dicts[kernel_name]
        for fid in range(start, end):
            suffix = get_position_str(mapping, fid - start)
            tile_name = f"%tile_comp_{kernel_name}_{suffix}"
            code += format_str(f"{tile_name} = aie.tile(0, {fid + 2})")
            buf_dict = func_buf_dicts[fid - start]
            buf_name_dict = {}
            for i, name in enumerate(buf_dict.keys()):
                new_name = f"{tile_name}_buf{i}"
                buf_name_dict[name] = new_name
                ele_type, shape = buf_dict[name]
                str_list = list(map(str, shape))
                str_list.append(ele_type)
                buf_type = f"memref<{'x'.join(map(str, str_list))}>"
                code += format_str(f"{new_name} = aie.buffer({tile_name}) : {buf_type}")
            buf_name_dicts.append(buf_name_dict)
    # update module and args
    for kernel_name in kernel_index_ranges.keys():
        input_args = kernel_input_args[kernel_name]
        func_arg_sizes = kernel_func_arg_sizes[kernel_name]
        for arg_id, (ele_type, orig_shape) in enumerate(input_args):
            orig_ele_type = f"memref<{'x'.join(map(str, orig_shape))}x{ele_type}>"
            # assume all functions of the same kernel have the same input args
            shape = func_arg_sizes[0][arg_id]
            ele_type = f"memref<{'x'.join(map(str, shape))}x{ele_type}>"
            input_args[arg_id] = (ele_type, orig_ele_type, shape, orig_shape)
        kernel_input_args[kernel_name] = input_args
    func_strs = list(map(str, funcs))
    # update buffers
    for fid, func_str in enumerate(func_strs):
        buf_name_dict = buf_name_dicts[fid]
        # remove memref.alloc
        pattern_alloc = re.compile(r"^.*memref\.alloc.*\n?", re.MULTILINE)
        func_str = re.sub(pattern_alloc, "", func_str)
        # replace new buffer name
        pattern_boundary = r"(?<![\w.]){old}(?![\w.])"
        for name, new_name in buf_name_dict.items():
            escaped_name = re.escape(name)
            pattern = pattern_boundary.format(old=escaped_name)
            func_str = re.sub(pattern, new_name, func_str)
        func_strs[fid] = func_str
    # create input object fifos
    # connect each argument to a separate mem tile
    for kernel_name, (start, end) in kernel_index_ranges.items():
        input_args = kernel_input_args[kernel_name]
        func_arg_sizes = kernel_func_arg_sizes[kernel_name]
        mapping = kernel_mappings[kernel_name]
        dist_allocs = kernel_dist_allocs[kernel_name]
        for arg_id, (in_type, orig_in_type, shape, orig_shape) in enumerate(
            input_args[:-1]
        ):
            if dist_allocs[arg_id]:
                # depth=2 means double buffer
                code += format_str(
                    f"aie.objectfifo @in_sh_{kernel_name}_{arg_id}(%tile_shim, {{%tile_mem{arg_id}}}, 2 : i32) : !aie.objectfifo<{orig_in_type}>"
                )
                for fid in range(start, end):
                    suffix = get_position_str(mapping, fid - start)
                    code += format_str(
                        f"aie.objectfifo @in_{kernel_name}_{arg_id}_{suffix}(%tile_mem{arg_id}, {{%tile_comp_{kernel_name}_{suffix}}}, 2 : i32) : !aie.objectfifo<{in_type}>"
                    )
                in_mem_str = ", ".join(
                    [
                        f"@in_{kernel_name}_{arg_id}_{get_position_str(mapping, fid - start)}"
                        for fid in range(start, end)
                    ]
                )
                pe_size = end - start
                shape_prod = np.prod(shape)
                in_mem_stride = list(range(0, shape_prod * pe_size, shape_prod))
                # (src_offsets, dst_offsets)
                code += format_str(
                    f"aie.objectfifo.link [@in_sh_{kernel_name}_{arg_id}] -> [{in_mem_str}]([] {in_mem_stride})"
                )
            else:
                code += format_str(
                    f"aie.objectfifo @in_sh_{kernel_name}_{arg_id}(%tile_shim, {{%tile_mem{arg_id}}}, 2 : i32) : !aie.objectfifo<{orig_in_type}>"
                )
                in_tile_str = ", ".join(
                    [
                        f"%tile_comp_{kernel_name}_{get_position_str(mapping, fid - start)}"
                        for fid in range(start, end)
                    ]
                )
                code += format_str(
                    f"aie.objectfifo @in_{kernel_name}_{arg_id}_0(%tile_mem{arg_id}, {{{in_tile_str}}}, 2 : i32) : !aie.objectfifo<{orig_in_type}>"
                )
                code += format_str(
                    f"aie.objectfifo.link [@in_sh_{kernel_name}_{arg_id}] -> [@in_{kernel_name}_{arg_id}_0]([] [])"
                )
        kernel_dist_allocs[kernel_name] = dist_allocs
    # create output object fifos
    for kernel_name, (start, end) in kernel_index_ranges.items():
        dist_allocs = kernel_dist_allocs[kernel_name]
        input_args = kernel_input_args[kernel_name]
        mapping = kernel_mappings[kernel_name]
        out_type, orig_out_type, out_shape, orig_out_shape = input_args[-1]
        if dist_allocs[-1]:
            # output uses tile_mem0
            for fid in range(start, end):
                suffix = get_position_str(mapping, fid - start)
                code += format_str(
                    f"aie.objectfifo @out_{kernel_name}_{suffix}(%tile_comp_{kernel_name}_{suffix}, {{%tile_mem0}}, 2 : i32) : !aie.objectfifo<{out_type}>"
                )
            code += format_str(
                f"aie.objectfifo @out_sh_{kernel_name}(%tile_mem0, {{%tile_shim}}, 2 : i32) : !aie.objectfifo<{orig_out_type}>"
            )
            out_mem_str = ", ".join(
                [
                    f"@out_{kernel_name}_{get_position_str(mapping, fid - start)}"
                    for fid in range(start, end)
                ]
            )
            pe_size = end - start
            shape_prod = np.prod(out_shape)
            out_mem_stride = list(range(0, shape_prod * pe_size, shape_prod))
            code += format_str(
                f"aie.objectfifo.link [{out_mem_str}] -> [@out_sh_{kernel_name}]({out_mem_stride} [])"
            )
        else:
            out_tile_str = ", ".join(
                [
                    f"%tile_comp_{kernel_name}_{get_position_str(mapping, fid - start)}"
                    for fid in range(start, end)
                ]
            )
            code += format_str(
                f"aie.objectfifo @out_sh_{kernel_name}(%tile_mem0, {{%tile_shim}}, 2 : i32) : !aie.objectfifo<{orig_out_type}>"
            )
            code += format_str(
                f"aie.objectfifo @out_{kernel_name}_0({{{out_tile_str}}}, {{%tile_mem0}}, 2 : i32) : !aie.objectfifo<{orig_out_type}>"
            )
            code += format_str(
                f"aie.objectfifo.link [@out_{kernel_name}_0] -> [@out_sh_{kernel_name}]([] [])"
            )
    # create core computation
    in_args = []
    out_args = []
    for kernel_name, (start, end) in kernel_index_ranges.items():
        mapping = kernel_mappings[kernel_name]
        input_args = kernel_input_args[kernel_name]
        dist_allocs = kernel_dist_allocs[kernel_name]
        out_id = len(input_args) - 1
        for fid in range(start, end):
            func_str = func_strs[fid]
            suffix = get_position_str(mapping, fid - start)
            code += format_str(
                f"%core_0_{fid + 2} = aie.core(%tile_comp_{kernel_name}_{suffix}) {{"
            )
            with format_code(indent=6):
                code += format_str("%global_c0 = arith.constant 0 : index")
                code += format_str("%global_c1 = arith.constant 1 : index")
                code += format_str(
                    "%c9223372036854775807 = arith.constant 9223372036854775807 : index"
                )
                code += format_str(
                    "scf.for %arg0 = %global_c0 to %c9223372036854775807 step %global_c1 {"
                )
                with format_code(indent=8):
                    for arg_id, (in_type, orig_in_type, _, _) in enumerate(
                        input_args[:-1]
                    ):
                        dtype = in_type if dist_allocs[arg_id] else orig_in_type
                        code += format_str(
                            f"%fifo{arg_id} = aie.objectfifo.acquire @in_{kernel_name}_{arg_id}_{suffix if dist_allocs[arg_id] else 0}(Consume, 1) : !aie.objectfifosubview<{dtype}>"
                        )
                        code += format_str(
                            f"%local{arg_id} = aie.objectfifo.subview.access %fifo{arg_id}[0] : !aie.objectfifosubview<{dtype}> -> {dtype}"
                        )
                        func_str = func_str.replace(f"%arg{arg_id}", f"%local{arg_id}")
                    out_type, orig_out_type, _, _ = input_args[-1]
                    dtype = out_type if dist_allocs[-1] else orig_out_type
                    code += format_str(
                        f"%fifo_out = aie.objectfifo.acquire @out_{kernel_name}_{suffix if dist_allocs[-1] else 0}(Produce, 1) : !aie.objectfifosubview<{dtype}>"
                    )
                    code += format_str(
                        f"%local_out = aie.objectfifo.subview.access %fifo_out[0] : !aie.objectfifosubview<{dtype}> -> {dtype}"
                    )
                    func_str = func_str.replace(f"%arg{out_id}", "%local_out")
                    while " call @" in func_str:
                        func_str = func_str.replace(" call @", " func.call @")
                    # main body
                    with format_code(indent=6):
                        for line in func_str.splitlines()[1:-2]:
                            code += format_str(line, strip=False)
                    # release fifos
                    for arg_id in range(len(input_args[:-1])):
                        code += format_str(
                            f"aie.objectfifo.release @in_{kernel_name}_{arg_id}_{suffix if dist_allocs[arg_id] else 0}(Consume, 1)"
                        )
                    code += format_str(
                        f"aie.objectfifo.release @out_{kernel_name}_{suffix if dist_allocs[-1] else 0}(Produce, 1)"
                    )
                code += format_str("}")
                code += format_str("aie.end")
            code += "    }"
            if len(external_kernels[f"{kernel_name}_{suffix}"]) > 0:
                ext = external_kernels[f"{kernel_name}_{suffix}"][0]
                code += f' {{link_with = "{ext}.o"}}\n'
        in_args += [
            f"%arg{i}: {orig_in_type}"
            for i, (_, orig_in_type, _, _) in enumerate(input_args[:-1])
        ]
        out_args.append(f"%arg{out_id}: {orig_out_type}")
    code += format_str(
        f"aiex.runtime_sequence({",".join(in_args)}, {",".join(out_args)}) {{"
    )
    with format_code(indent=6):
        for kernel_name, (start, end) in kernel_index_ranges.items():
            input_args = kernel_input_args[kernel_name]
            dist_allocs = kernel_dist_allocs[kernel_name]
            mapping = kernel_mappings[kernel_name]
            out_id = len(input_args) - 1
            for arg_id, (_, orig_in_type, _, orig_shape) in enumerate(input_args[:-1]):
                # (x, y, memref[offset][size][stride])
                # issue_token: MM2S-false, S2MM-true
                if len(orig_shape) == 1:
                    size_n_stride = f"[1, 1, 1, {orig_shape[0]}][0, 0, 0, 1]"
                elif len(mapping) == 2 and len(orig_shape) == 2 and dist_allocs[arg_id]:
                    # now only support 2D mapping and 2D tensor
                    size_n_stride = f"[{mapping[0]}, {mapping[1]}, {orig_shape[0] // mapping[0]}, {orig_shape[1] // mapping[1]}][{orig_shape[0] // mapping[0] * orig_shape[1]}, {orig_shape[1] // mapping[1]}, {orig_shape[1]}, 1]"
                else:
                    size_n_stride = f"[1, 1, {orig_shape[0]}, {orig_shape[1]}][0, 0, {orig_shape[1]}, 1]"
                code += format_str(
                    f"aiex.npu.dma_memcpy_nd(0, 0, %arg{arg_id}[0, 0, 0, 0]{size_n_stride}) {{id = {arg_id + 1} : i64, issue_token = true, metadata = @in_sh_{kernel_name}_{arg_id}}} : {orig_in_type}"
                )
            orig_out_shape = input_args[-1][-1]
            if len(orig_out_shape) == 1:
                out_size_n_stride = f"[1, 1, 1, {orig_out_shape[0]}][0, 0, 0, 1]"
            elif len(mapping) == 2 and len(orig_out_shape) == 2 and dist_allocs[-1]:
                # now only support 2D mapping and 2D tensor
                out_size_n_stride = f"[{mapping[0]}, {mapping[1]}, {orig_out_shape[0] // mapping[0]}, {orig_out_shape[1] // mapping[1]}][{orig_out_shape[0] // mapping[0] * orig_out_shape[1]}, {orig_out_shape[1] // mapping[1]}, {orig_out_shape[1]}, 1]"
            else:
                out_size_n_stride = f"[1, 1, {orig_out_shape[0]}, {orig_out_shape[1]}][0, 0, {orig_out_shape[1]}, 1]"
            code += format_str(
                f"aiex.npu.dma_memcpy_nd(0, 0, %arg{out_id}[0, 0, 0, 0]{out_size_n_stride}) {{id = 0 : i64, metadata = @out_sh_{kernel_name}}} : {orig_out_type}"
            )
            for arg_id in range(len(input_args) - 1):
                code += format_str(
                    f"aiex.npu.dma_wait {{symbol = @in_sh_{kernel_name}_{arg_id}}}"
                )
            code += format_str(f"aiex.npu.dma_wait {{symbol = @out_sh_{kernel_name}}}")
    code += format_str("}")
    code += format_str("}", indent=2)
    code += "}"
    return code


def get_kernel_index_ranges_from_mappings(kernel_mappings):
    kernel_index_ranges = {}
    index = 0
    for func_name, mapping in kernel_mappings.items():
        start = index
        end = index = start + math.prod(mapping)
        kernel_index_ranges[func_name] = (start, end)
    return kernel_index_ranges


def check_usage_intersection(func_arg_sizes, func_arg_lower_bounds, orig_shapes):
    """
    Check if there is a non-empty intersection among all functions' usage regions for each parameter.

    Parameters:
    ----------
    func_arg_sizes: list
        Each element represents a function's usage sizes for all arguments.
        For example, [[8, 16], [16, 8], [8, 8]] means the function has three parameters,
        each being a 2D tensor with usage sizes 8x16, 16x8, and 8x8 respectively.
    func_arg_lower_bounds: list
        Has the same structure as func_arg_sizes, representing the lower bounds
        of the usage regions for each parameter for each function.
    orig_shapes: list
        Each element represents the original shape of the corresponding parameter,
        e.g., [[16, 16], [16, 16], [16, 16]]. It is assumed that all provided usage regions
        are within the bounds of the original shape.

    Returns:
    -------
    dist_allocs: list
        dist_allocs define the allocation strategy for each argument. Currently, there are only two options: True or False.
        - If True: The memory tile divides the memory of the argument and distributes it among all compute tiles using
        `aie.objectfifo.link`. This is only feasible if the total memory consumed by all compute tiles does not exceed
        the original memory allocated to the argument.
        Example: If there are 3 compute tiles, each consuming 1/3 of matrix A, this strategy can be applied.
        - If False: The memory tile assigns the entire memory of the argument to each compute tile.
    """
    num_funcs = len(func_arg_sizes)
    num_params = len(orig_shapes)
    dist_allocs = []
    for param in range(num_params):
        boxes = []
        has_intersection = False
        dims = len(orig_shapes[param])
        for f in range(num_funcs):
            box = []
            for d in range(dims):
                lower = func_arg_lower_bounds[f][param][d]
                upper = lower + func_arg_sizes[f][param][d]
                box.append((lower, upper))
            for b in boxes:
                if all(
                    max(b[d][0], box[d][0]) < min(b[d][1], box[d][1])
                    for d in range(dims)
                ):
                    has_intersection = True
                    break
            if has_intersection:
                break
            boxes.append(box)
        dist_allocs.append(not has_intersection)
    return dist_allocs


def reindex_tensor_access(mod, kernel_index_ranges, kernel_input_args):
    ctx = mod.context
    funcs = get_public_funcs(mod)
    # kernel->func -> arg -> dim
    kernel_func_arg_lower_bounds = {}
    kernel_func_arg_sizes = {}
    for kernel_name, (start, end) in kernel_index_ranges.items():
        func_arg_lower_bounds = []
        func_arg_sizes = []
        for fid in range(start, end):
            func = funcs[fid]
            entry_block = func.regions[0].blocks[0]
            args = entry_block.arguments
            arg_types = args.types
            # TODO: might need some specialization for scalar input arg
            lower_bounds = [
                [float("inf") for _ in range(len(arg_type.shape))]
                for arg_type in arg_types
            ]
            sizes = [[0 for _ in range(len(arg_type.shape))] for arg_type in arg_types]
            for block in func.regions[0].blocks:
                for op in block.operations:
                    if op.operation.name in {
                        "tensor.extract_slice",
                        "tensor.insert_slice",
                        "memref.subview",
                    }:
                        operand_idx = (
                            1 if op.operation.name == "tensor.insert_slice" else 0
                        )
                        if op.operands[operand_idx] not in args:
                            continue
                        arg_id = list(args).index(op.operands[operand_idx])
                        static_offsets = op.attributes["static_offsets"]
                        static_sizes = op.attributes["static_sizes"]
                        for i, (offset, size) in enumerate(
                            zip(static_offsets, static_sizes)
                        ):
                            lower_bounds[arg_id][i] = min(
                                lower_bounds[arg_id][i], offset
                            )
                            sizes[arg_id][i] = max(sizes[arg_id][i], size)
            for i, lower_bound in enumerate(lower_bounds):
                # Arguments never used with slice
                if lower_bound[0] == float("inf"):
                    # If ever used, assume using entire tensor
                    if len(list(args[i].uses)) > 0:
                        lower_bounds[i] = [0] * len(lower_bound)
                        sizes[i] = args[i].type.shape
                    else:
                        lower_bounds[i] = [0] * len(lower_bound)
                        sizes[i] = [0] * len(lower_bound)

            func_arg_lower_bounds.append(lower_bounds)
            func_arg_sizes.append(sizes)
        kernel_func_arg_lower_bounds[kernel_name] = func_arg_lower_bounds
        kernel_func_arg_sizes[kernel_name] = func_arg_sizes

    kernel_dist_allocs = {}
    for kernel_name in kernel_index_ranges.keys():
        orig_shapes = [input_arg[-1] for input_arg in kernel_input_args[kernel_name]]
        kernel_dist_allocs[kernel_name] = check_usage_intersection(
            func_arg_sizes, func_arg_lower_bounds, orig_shapes
        )

    for kernel_name, (start, end) in kernel_index_ranges.items():
        func_arg_lower_bounds = kernel_func_arg_lower_bounds[kernel_name]
        func_arg_sizes = kernel_func_arg_sizes[kernel_name]
        dist_allocs = kernel_dist_allocs[kernel_name]
        for fid in range(start, end):
            func = funcs[fid]
            entry_block = func.regions[0].blocks[0]
            args = entry_block.arguments
            lower_bounds = func_arg_lower_bounds[fid - start]
            sizes = func_arg_sizes[fid - start]
            for block in func.regions[0].blocks:
                for op in block.operations:
                    if op.operation.name in {
                        "tensor.extract_slice",
                        "tensor.insert_slice",
                        "memref.subview",
                    }:
                        operand_idx = (
                            1 if op.operation.name == "tensor.insert_slice" else 0
                        )
                        if op.operands[operand_idx] not in args:
                            continue
                        arg_id = list(args).index(op.operands[operand_idx])
                        static_offsets = op.attributes["static_offsets"]
                        new_offsets = []
                        for i, offset in enumerate(static_offsets):
                            new_offset = (
                                offset - lower_bounds[arg_id][i]
                                if dist_allocs[arg_id]
                                else offset
                            )
                            new_offset_attr = IntegerAttr.get(
                                IntegerType.get_signless(64, ctx), new_offset
                            )
                            new_offsets.append(new_offset_attr)
                        op.attributes["static_offsets"] = DenseI64ArrayAttr.get(
                            new_offsets, ctx
                        )
    return kernel_func_arg_lower_bounds, kernel_func_arg_sizes, kernel_dist_allocs


def update_func_op_arg_types(
    func_op, input_args, new_shapes, context, dist_allocs, enable_tensor
):
    old_func_type = func_op.function_type
    old_result_types = old_func_type.value.results
    new_input_types = []
    for arg_id, ((ele_type_str, _), shape) in enumerate(zip(input_args, new_shapes)):
        elem_ty = get_element_type_from_str(ele_type_str, context)
        old_ty = old_func_type.value.inputs[arg_id]
        memref_ty = (
            RankedTensorType.get(shape, elem_ty)
            if enable_tensor
            else MemRefType.get(shape, elem_ty)
        )
        new_input_types.append(memref_ty if dist_allocs[arg_id] else old_ty)
        # Update subview results memory layout
        if not enable_tensor and dist_allocs[arg_id]:
            entry_block = func_op.regions[0].blocks[0]
            args = entry_block.arguments
            for block in func_op.regions[0].blocks:
                for op in block.operations:
                    if (
                        op.operation.name == "memref.subview"
                        and op.operands[0] == args[arg_id]
                    ):
                        old_result_type = op.results.types[0]
                        strides = []
                        times = 1
                        for size in reversed(shape):
                            strides.append(times)
                            times *= size
                        strides = list(reversed(strides))
                        layout = StridedLayoutAttr.get(0, strides)
                        result = MemRefType.get(
                            old_result_type.shape,
                            old_result_type.element_type,
                            layout=layout,
                        )
                        subview = memref_d.SubViewOp(
                            source=op.source,
                            result=result,
                            static_offsets=op.static_offsets,
                            static_sizes=op.static_sizes,
                            static_strides=op.static_strides,
                            offsets=[],
                            sizes=[],
                            strides=[],
                            ip=InsertionPoint(op),
                        )
                        op.result.replace_all_uses_with(subview.result)
                        op.erase()
    new_func_type = FunctionType.get(new_input_types, old_result_types, context)
    new_type = TypeAttr.get(new_func_type, context)
    func_op.operation.attributes["function_type"] = new_type
    entry_block = func_op.entry_block
    for i, block_arg in enumerate(entry_block.arguments):
        block_arg.set_type(new_input_types[i])


def lower_tensor_to_memref(mod, enable_tensor):
    passes = (
        [
            # "linalg-generalize-named-ops",
            # "linalg-fuse-elementwise-ops",
            "one-shot-bufferize{bufferize-function-boundaries function-boundary-type-conversion=identity-layout-map}",
            "func.func(convert-linalg-to-affine-loops),lower-affine",
        ]
        if enable_tensor
        else [
            # "linalg-generalize-named-ops",
            # "linalg-fuse-elementwise-ops",
            "func.func(convert-linalg-to-affine-loops),lower-affine",
        ]
    )
    pipeline = f'builtin.module({",".join(passes)})'
    with mod.context:
        mlir_pass_manager.parse(pipeline).run(mod.operation)


def record_local_buffer(mod, kernel_index_ranges):
    kernel_func_buf_dicts = {}
    funcs = get_public_funcs(mod)
    for kernel_name, (start, end) in kernel_index_ranges.items():
        func_buf_dicts = []
        for fid in range(start, end):
            func = funcs[fid]
            buf_dict = {}
            for block in func.regions[0].blocks:
                for op in block.operations:
                    if op.operation.name == "memref.alloc":
                        name = op.result.get_name()
                        dtype, shape = get_dtype_and_shape_from_type(op.result.type)
                        buf_dict[name] = (dtype, shape)
            func_buf_dicts.append(buf_dict)
        kernel_func_buf_dicts[kernel_name] = func_buf_dicts
    return kernel_func_buf_dicts


class AIEModule:
    def __init__(self, module, top_func_name, project, kernel_mappings, enable_tensor):
        self.module = module
        self.top_func_name = top_func_name
        self.project = project
        self.module = module
        self.kernel_mappings = kernel_mappings
        self.enable_tensor = enable_tensor
        self.kernel_funcs = {}
        self.kernel_inputs = {}
        self.kernel_outputs = {}
        self.kernel_input_args = {}

    def build(self):
        assert "MLIR_AIE_INSTALL_DIR" in os.environ, "Please set MLIR_AIE_INSTALL_DIR"
        assert "PEANO_INSTALL_DIR" in os.environ, "Please set PEANO_INSTALL_DIR"
        self.kernel_index_ranges = get_kernel_index_ranges_from_mappings(
            self.kernel_mappings
        )
        for kernel_name, (start, _) in self.kernel_index_ranges.items():
            kernel_func = self.module.body.operations[start]
            self.kernel_funcs[kernel_name] = kernel_func
            inputs, outputs = get_func_inputs_outputs(kernel_func)
            self.kernel_inputs[kernel_name] = inputs
            self.kernel_outputs[kernel_name] = outputs
            self.kernel_input_args[kernel_name] = inputs + outputs
        # Assume all functions of the same kernel have the same input and output arguments
        _, kernel_func_arg_sizes, kernel_dist_allocs = reindex_tensor_access(
            self.module, self.kernel_index_ranges, self.kernel_input_args
        )
        with self.module.context as ctx, Location.unknown():
            for kernel_name, (start, end) in self.kernel_index_ranges.items():
                func_arg_sizes = kernel_func_arg_sizes[kernel_name]
                for fid in range(start, end):
                    func = self.module.body.operations[fid]
                    dist_allocs = kernel_dist_allocs[kernel_name]
                    shapes = func_arg_sizes[fid - start]
                    update_func_op_arg_types(
                        func,
                        self.kernel_input_args[kernel_name],
                        shapes,
                        ctx,
                        dist_allocs,
                        enable_tensor=self.enable_tensor,
                    )
        external_kernels = inject_aie_kernels(self.module)
        lower_tensor_to_memref(self.module, self.enable_tensor)
        kernel_func_buf_dicts = record_local_buffer(
            self.module, self.kernel_index_ranges
        )
        code = codegen_aie_mlir(
            self.module,
            self.kernel_mappings,
            self.kernel_index_ranges,
            self.kernel_input_args,
            kernel_func_arg_sizes,
            kernel_func_buf_dicts,
            kernel_dist_allocs,
            external_kernels,
        )
        os.makedirs(os.path.join(self.project, "build"), exist_ok=True)
        with open(os.path.join(self.project, "top.mlir"), "w", encoding="utf-8") as f:
            f.write(code)
        # compile external kernels
        compiled_kernels = set()
        for _, kernel_lst in external_kernels.items():
            for kernel in kernel_lst:
                if kernel in compiled_kernels:
                    continue
                path = os.path.dirname(__file__)
                path = os.path.join(path, "aie_kernels")
                cmd = f"cd {self.project} && $PEANO_INSTALL_DIR/bin/clang++ -O2 -v -std=c++20 --target=aie2-none-unknown-elf -Wno-parentheses -Wno-attributes -Wno-macro-redefined -DNDEBUG -I $(dirname $(which aie-opt))/../include -c {path}/{kernel}.cc -o {kernel}.o"
                compiled_kernels.add(kernel)
                process = subprocess.Popen(cmd, shell=True)
                process.wait()
        # build mlir-aie
        cmd = f"cd {self.project} && PYTHONPATH=$MLIR_AIE_INSTALL_DIR/python aiecc.py --aie-generate-cdo --aie-generate-npu --no-compile-host --no-xchesscc --no-xbridge --xclbin-name=build/final.xclbin --npu-insts-name=insts.txt top.mlir"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to compile the MLIR-AIE code")
        path = os.path.dirname(__file__)
        path = os.path.join(path, "../harness/aie")
        os.system(f"cp -r {path}/* {self.project}")
        host_code = codegen_host(self.kernel_input_args)
        with open(os.path.join(self.project, "test.cpp"), "w", encoding="utf-8") as f:
            f.write(host_code)
        cmd = f"cd {self.project}/build && cmake .. -DTARGET_NAME=top -DMLIR_AIE_DIR=$MLIR_AIE_INSTALL_DIR/.. && cmake --build . --config Release"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to build AIE project.")
        return self

    def __call__(self, *args):
        # suppose the last argument is output
        for i, arg in enumerate(args[:-1]):
            with open(
                os.path.join(self.project, f"input{i}.data"), "w", encoding="utf-8"
            ) as f:
                f.write("\n".join([str(i) for i in arg.flatten()]))
        cmd = f"cd {self.project} && ./build/top -x build/final.xclbin -i insts.txt -k MLIR_AIE"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to execute AIE code.")
        # TODO: need to complete multiple outputs rules
        for i, inputs in enumerate(self.kernel_inputs.values()):
            result = read_tensor_from_file(
                inputs[-1][0], args[-1 * (i + 1)].shape, f"{self.project}/output.data"
            )
        args[-1][:] = result
