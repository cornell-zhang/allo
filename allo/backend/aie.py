# Copyright Allo authors. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
# mlir-aie commit: 8329b6
# pylint: disable=bad-builtin, no-name-in-module, too-many-branches, too-many-nested-blocks, consider-using-with, cell-var-from-loop


import os
import subprocess
import re
import numpy as np
from .._mlir.ir import (
    MemRefType,
    Location,
    InsertionPoint,
    FlatSymbolRefAttr,
    StringAttr,
)
from .._mlir.dialects import (
    allo as allo_d,
    func as func_d,
)
from .._mlir.passmanager import PassManager as mlir_pass_manager

from .vitis import read_tensor_from_file
from ..utils import get_dtype_and_shape_from_type
from .utils import format_str, format_code
from .vitis import ctype_map
from ..passes import analyze_read_write_patterns


host_header = """
//=============================================================================
// Auto generated by Allo
//=============================================================================

#include <boost/program_options.hpp>
#include <cstdint>
#include <fstream>
#include <iostream>
#include <sstream>
#include <string>
#include <vector>

#include "xrt/xrt_bo.h"
#include "xrt/xrt_device.h"
#include "xrt/xrt_kernel.h"

#include "test_utils.h"

namespace po = boost::program_options;

int main(int argc, const char *argv[]) {

  // ------------------------------------------------------
  // Parse program arguments
  // ------------------------------------------------------
  po::options_description desc("Allowed options");
  po::variables_map vm;
  test_utils::add_default_options(desc);

  test_utils::parse_options(argc, argv, desc, vm);
  int verbosity = vm["verbosity"].as<int>();
  int do_verify = vm["verify"].as<bool>();
  int n_iterations = vm["iters"].as<int>();
  int n_warmup_iterations = vm["warmup"].as<int>();
  int trace_size = vm["trace_sz"].as<int>();

  // Load instruction sequence
  std::vector<uint32_t> instr_v =
      test_utils::load_instr_sequence(vm["instr"].as<std::string>());
  if (verbosity >= 1)
    std::cout << "Sequence instr count: " << instr_v.size() << "\\n";

  // ------------------------------------------------------
  // Get device, load the xclbin & kernel and register them
  // ------------------------------------------------------
  // Get a device handle
  unsigned int device_index = 0;
  auto device = xrt::device(device_index);

  // Load the xclbin
  if (verbosity >= 1)
    std::cout << "Loading xclbin: " << vm["xclbin"].as<std::string>() << "\\n";
  auto xclbin = xrt::xclbin(vm["xclbin"].as<std::string>());

  // Load the kernel
  if (verbosity >= 1)
    std::cout << "Kernel opcode: " << vm["kernel"].as<std::string>() << "\\n";
  std::string Node = vm["kernel"].as<std::string>();

  // Get the kernel from the xclbin
  auto xkernels = xclbin.get_kernels();
  auto xkernel = *std::find_if(xkernels.begin(), xkernels.end(),
                               [Node, verbosity](xrt::xclbin::kernel &k) {
                                 auto name = k.get_name();
                                 if (verbosity >= 1) {
                                   std::cout << "Name: " << name << std::endl;
                                 }
                                 return name.rfind(Node, 0) == 0;
                               });
  auto kernelName = xkernel.get_name();

  // Register xclbin
  if (verbosity >= 1)
    std::cout << "Registering xclbin: " << vm["xclbin"].as<std::string>()
              << "\\n";
  device.register_xclbin(xclbin);

  // Get a hardware context
  if (verbosity >= 1)
    std::cout << "Getting hardware context.\\n";
  xrt::hw_context context(device, xclbin.get_uuid());

  // Get a kernel handle
  if (verbosity >= 1)
    std::cout << "Getting handle to kernel:" << kernelName << "\\n";
  auto kernel = xrt::kernel(context, kernelName);

  // ------------------------------------------------------
  // Initialize input/ output buffer sizes and sync them
  // ------------------------------------------------------
  auto bo_instr = xrt::bo(device, instr_v.size() * sizeof(int),
                          XCL_BO_FLAGS_CACHEABLE, kernel.group_id(1));
  void *bufInstr = bo_instr.map<void *>();
  memcpy(bufInstr, instr_v.data(), instr_v.size() * sizeof(int));

  std::ofstream ofile("output.data");
  if (!ofile.is_open()) {
      std::cerr << "Error: Could not open output file.\\n";
      return 1;
  }

"""

file_close_str = """  ofile.close();
  if (verbosity >= 1)
    std::cout << "Array has been written to output.data.\\n";
  return 0;
}
"""


def codegen_host(inputs, outputs):
    code = host_header
    with format_code(indent=2):
        # write input data
        for i, dtensor in enumerate(inputs):
            shape = dtensor.shape
            dtype = ctype_map[str(dtensor.dtype)]
            code += format_str(f'std::ifstream ifile{i}("input{i}.data");')
            code += format_str(f"if (!ifile{i}.is_open()) {{")
            code += format_str(
                '  std::cerr << "Error: Could not open input file.\\n";', strip=False
            )
            code += format_str("  return 1;", strip=False)
            code += format_str("}")
            size = np.prod(shape)
            code += format_str(
                f"auto bo_in{i} = xrt::bo(device, {size} * sizeof({dtype}),"
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({i + 3}));"
                )
            code += format_str(f"{dtype} *bufIn{i} = bo_in{i}.map<{dtype} *>();")
            code += format_str(f"std::vector<{dtype}> srcVec{i};")
            code += format_str(f"for (int i = 0; i < {size}; i++) {{")
            with format_code(indent=4):
                code += format_str(f"{dtype} num;")
                code += format_str(f"ifile{i} >> num;")
                code += format_str(f"srcVec{i}.push_back(num);")
            code += format_str("}")
            code += format_str(
                f"memcpy(bufIn{i}, srcVec{i}.data(), (srcVec{i}.size() * sizeof({dtype})));"
            )
        for i, dtensor in enumerate(outputs):
            shape = dtensor.shape
            dtype = ctype_map[str(dtensor.dtype)]
            out_size = np.prod(shape)
            code += format_str(
                f"\nauto bo_out{i} = xrt::bo(device, {out_size} * sizeof({dtype}),",
                strip=False,
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({len(inputs) + 2 + i}));"
                )
        code += format_str("if (verbosity >= 1)")
        code += format_str(
            '  std::cout << "Writing data into buffer objects.\\n";', strip=False
        )
        code += format_str("\nbo_instr.sync(XCL_BO_SYNC_BO_TO_DEVICE);", strip=False)
        for i in range(len(inputs)):
            code += format_str(f"bo_in{i}.sync(XCL_BO_SYNC_BO_TO_DEVICE);")
        # run kernels
        code += format_str("if (verbosity >= 1)")
        code += format_str('  std::cout << "Running Kernel.\\n";', strip=False)
        code += format_str(
            "\nauto start = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str("unsigned int opcode = 3;", strip=False)
        inbufs = ", ".join([f"bo_in{i}" for i in range(len(inputs))])
        outbufs = ", ".join([f"bo_out{i}" for i in range(len(outputs))])
        code += format_str("// gid: (opcode, instr, instr_size, ...)")
        code += format_str(
            f"auto run = kernel(opcode, bo_instr, instr_v.size(), {inbufs}, {outbufs});"
        )
        code += format_str("run.wait();")
        code += format_str(
            "\nauto end = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str(
            "float npu_time = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();"
        )
        code += format_str(
            'std::cout << "NPU execution time: " << npu_time << "us\\n";'
        )
        # get results
        for i, dtensor in enumerate(outputs):
            shape = dtensor.shape
            dtype = ctype_map[str(dtensor.dtype)]
            out_size = np.prod(shape)
            code += format_str(
                f"\nbo_out{i}.sync(XCL_BO_SYNC_BO_FROM_DEVICE);", strip=False
            )
            code += format_str(f"{dtype} *bufOut{i} = bo_out{i}.map<{dtype} *>();")
            code += format_str(f"for (uint32_t i = 0; i < {out_size}; i++) {{")
            code += format_str(f'  ofile << *(bufOut{i} + i) << "\\n";', strip=False)
            code += format_str("}")
        code += format_str("\n// Close files", strip=False)
        for i in range(len(inputs)):
            code += format_str(f"ifile{i}.close();")
        code += file_close_str
    return code


def get_stream_in_out(stream_info):
    """
    Computes the mapping of FIFO names to their producer (output) and consumer (input) cores.

    Parameters
    ----------
    stream_info (Dict[int, List[Tuple[str, str]]]):
        A dictionary where the key is a core ID (int), and the value is a list of tuples.
        Each tuple contains:
        - fifo_name (str): The name of the FIFO.
        - direction (str): The direction of data flow, either "in" (consumer) or "out" (producer).

    Returns
    -------
    Dict[str, Tuple[Optional[int], Optional[int]]]]:
        A dictionary where the key is a FIFO name, and the value is a tuple:
        - The first element is the producer function and the second element is the consumer function.
    """
    stream_in_out = {}
    for core, fifos in stream_info.items():
        for fifo_name, direction in fifos:
            if fifo_name not in stream_in_out:
                stream_in_out[fifo_name] = (None, None)
            if direction == "in":
                stream_in_out[fifo_name] = (stream_in_out[fifo_name][0], core)
            elif direction == "out":
                stream_in_out[fifo_name] = (core, stream_in_out[fifo_name][1])
    return stream_in_out


def get_public_funcs(mod):
    funcs = []
    top_func = None
    for func in mod.body.operations:
        if isinstance(func, func_d.FuncOp) and (
            "sym_visibility" not in func.attributes
            or func.attributes["sym_visibility"].value != "private"
        ):
            if func.attributes["sym_name"].value == "top":
                top_func = func
            else:
                funcs.append(func)
    return top_func, funcs


def inject_aie_kernels(mod):
    external_kernels = {}
    injected_kernels = set()
    with mod.context, Location.unknown():
        for func in mod.body.operations:
            external_kernels[func.attributes["sym_name"].value] = []
            for block in func.regions[0].blocks:
                for op in block.operations:
                    if (
                        op.operation.name in {"linalg.add", "linalg.mul"}
                        and len(MemRefType(op.inputs[0].type).shape) == 1
                    ) or op.operation.name == "linalg.matmul":
                        op_name = op.operation.name.split(".")[1]
                        # Inject AIE kernel
                        func_type = func_d.FunctionType.get(
                            [op.inputs[0].type, op.inputs[1].type, op.outputs[0].type],
                            [],
                        )
                        dtype = str(op.inputs[0].type.element_type)
                        shape = MemRefType(op.inputs[0].type).shape
                        if op.operation.name in {"linalg.add", "linalg.mul"}:
                            kernel_name = f"{op_name}_{dtype}_vector"
                            external_kernels[func.attributes["sym_name"].value].append(
                                (op_name, dtype, shape)
                            )
                        else:  # linalg.matmul
                            M, K = MemRefType(op.inputs[0].type).shape
                            _, N = MemRefType(op.inputs[1].type).shape
                            out_dtype = str(op.outputs[0].type.element_type)
                            if (dtype, out_dtype) not in [
                                ("i8", "i8"),
                                ("i16", "i16"),
                                ("i16", "i32"),
                                ("bf16", "bf16"),
                                ("bf16", "f32"),
                            ]:
                                # f"Unsupported in_dtype {dtype} and out_dtype {out_dtype} pair"
                                continue
                            kernel_name = f"matmul_scalar_{dtype}_{out_dtype}"
                            external_kernels[func.attributes["sym_name"].value].append(
                                (op_name, dtype, out_dtype, M, N, K)
                            )
                        func_d.CallOp(
                            [],
                            FlatSymbolRefAttr.get(kernel_name),
                            [op.inputs[0], op.inputs[1], op.outputs[0]],
                            ip=InsertionPoint(op),
                        )
                        op.erase()

                        if kernel_name in injected_kernels:
                            continue
                        injected_kernels.add(kernel_name)
                        kernel = func_d.FuncOp(
                            kernel_name,
                            func_type,
                            ip=InsertionPoint(func),
                        )
                        kernel.attributes["sym_visibility"] = StringAttr.get("private")
    return external_kernels


def codegen_external_kernels(external_kernels):
    code = ""
    code += "// External kernels generated by Allo\n\n"
    code += "#include <stdint.h>\n"
    code += "#include <stdio.h>\n"
    code += "#include <stdlib.h>\n"
    code += "#include <type_traits>\n"
    code += "#include <aie_api/aie.hpp>\n\n"
    generated_kernels = {}
    kernel_code = ""
    for _, kernel_lst in external_kernels.items():
        for items in kernel_lst:
            kernel, dtype = items[0], items[1]
            if kernel in generated_kernels:
                continue
            ctype = ctype_map[dtype]
            if "bfloat" in ctype:
                ctype = "bfloat16"
            if kernel == "matmul":
                pass
            else:
                kernel, dtype, shape = items
                kernel_code += f"void {kernel}_{dtype}_vector"
                kernel_code += f"({ctype} *A_in, {ctype} *B_in, {ctype} *C_out)"
                kernel_code += " {\n"
                kernel_code += f"  eltwise_v{kernel}<{ctype}, {ctype}, {np.prod(shape)}>(A_in, B_in, C_out);\n"
                kernel_code += "}\n\n"
            generated_kernels[kernel] = items
    for kernel, items in generated_kernels.items():
        match kernel:
            case "add":
                code += '#include "add.cc"\n'
            case "mul":
                code += '#include "mul.cc"\n'
            case "matmul":
                _, dtype, out_dtype, M, N, K = items
                code += f"#define DIM_M {M}\n"
                code += f"#define DIM_N {N}\n"
                code += f"#define DIM_K {K}\n"
                code += f"#define {dtype}_{out_dtype}_ONLY\n"
                code += '#include "mm.cc"\n'
    code += '\nextern "C" {\n\n'
    code += kernel_code
    code += '} // extern "C"\n'
    return code, generated_kernels


def process_stream_operations(func_str, streams, start_id, stream_ele_types):
    """
    Process a function string by replacing stream_get and stream_put calls with
    corresponding formatted FIFO code.

    Parameters
    ----------
        func_str: str
            The input function string to be processed.
        streams: List[Tuple]
            Each stream is a tuple/list where the first element is the stream name.
        start_id: int
            The starting ID for the streams, used to map the argument IDs to stream indices.
        stream_ele_types: Dict
            Dictionary mapping stream names to their element types.

    Returns
    -------
        str: The resulting formatted code.
    """
    code = ""
    with format_code(indent=6):
        lines = func_str.splitlines()
        for i in range(1, len(lines) - 2):
            line = lines[i]
            # Process stream_get
            if "stream_get" in line:
                # extract the argument id from the pattern "stream_get(%arg<digits>"
                m_get = re.search(r"stream_get\(%arg(\d+)", line)
                if m_get:
                    arg_id = int(m_get.group(1))
                else:
                    continue
                # Extract the return variable
                return_var = line.split("=")[0].strip()
                stream_index = arg_id - start_id
                stream_name = streams[stream_index][0]
                current_indent = 6 + (len(line) - len(line.lstrip(" ")))
                with format_code(indent=current_indent):
                    # Acquire the FIFO
                    ele_type = stream_ele_types[stream_name]
                    code += format_str(
                        f"%fifo_{stream_name} = aie.objectfifo.acquire @{stream_name}(Consume, 1) : !aie.objectfifosubview<{ele_type}>"
                    )
                    code += format_str(
                        f"%local_{stream_name} = aie.objectfifo.subview.access %fifo_{stream_name}[0] : !aie.objectfifosubview<{ele_type}> -> {ele_type}"
                    )
                    # Load the value into a local variable if the element type is scalar
                    if "x" not in ele_type:
                        code += format_str(
                            f"{return_var} = memref.load %local_{stream_name}[] : {ele_type}"
                        )
                    else:
                        # Otherwise, replace the return variable with the local stream variable
                        func_str = func_str.replace(return_var, f"%local_{stream_name}")
                        lines = func_str.splitlines()
                    # Release the FIFO
                    code += format_str(
                        f"aie.objectfifo.release @{stream_name}(Consume, 1)"
                    )
            # Process stream_put
            elif "stream_put" in line:
                # Extract the argument id from the pattern "stream_put(%arg<digits>"
                m_put_id = re.search(r"stream_put\(%arg(\d+)", line)
                if m_put_id:
                    arg_id = int(m_put_id.group(1))
                else:
                    continue
                # Extract the put variable
                search_start = m_put_id.end()
                m_put_var = re.search(r"(%[^)]+)", line[search_start:])
                if m_put_var:
                    put_var = m_put_var.group(1)
                else:
                    continue
                stream_index = arg_id - start_id
                stream_name = streams[stream_index][0]
                ele_type = stream_ele_types[stream_name]
                current_indent = 6 + (len(line) - len(line.lstrip(" ")))
                with format_code(indent=current_indent):
                    # Acquire the FIFO
                    code += format_str(
                        f"%fifo_{stream_name} = aie.objectfifo.acquire @{stream_name}(Produce, 1) : !aie.objectfifosubview<{ele_type}>"
                    )
                    code += format_str(
                        f"%local_{stream_name} = aie.objectfifo.subview.access %fifo_{stream_name}[0] : !aie.objectfifosubview<{ele_type}> -> {ele_type}"
                    )
                    # Depending on the element type, either perform a memref.copy or a memref.store
                    if "x" in ele_type:
                        code += format_str(
                            f"memref.copy {put_var}, %local_{stream_name} : {ele_type} to {ele_type}"
                        )
                    else:
                        code += format_str(
                            f"memref.store {put_var}, %local_{stream_name}[] : {ele_type}"
                        )
                    # Release the FIFO
                    code += format_str(
                        f"aie.objectfifo.release @{stream_name}(Produce, 1)"
                    )
            else:
                code += format_str(line, strip=False)
    return code, func_str


def get_memref_type_str(ele_type, shape):
    return f"memref<{'x'.join(map(str, shape))}x{ele_type}>"


def calculate_tensor_access(shape, partition, device_mesh):
    """
    Calculate the size and stride for tensor access based on shape and partition method.
    Layout visualization tool:
    https://andreroesti.com/data-layout-viz/data_layout.html

    Parameters:
    -----------
    shape : tuple
        The shape of the tensor (1D or 2D)
    partition : tuple(str, int)
        The partition method for each dimension:
        - 'S': Sharded (distributed across devices)
        - 'R': Replicated (copied to each device)
    device_mesh : tuple, optional
        The mesh of devices (default: (2, 2))

    Returns:
    --------
    tuple
        A tuple containing two lists: (size, stride)
    """
    # Handle 1D tensor case
    partition_str = "".join([p[0] for p in partition])
    if len(shape) == 1:
        if partition_str == "S":
            # For 1D tensor with "S", shard across all devices
            total_devices = device_mesh[0]
            shard_size = shape[0] // total_devices
            size = [1, 1, total_devices, shard_size]
            stride = [0, 0, shard_size, 1]
        elif partition_str == "R":
            # For 1D tensor with "R", replicate across all devices
            total_devices = device_mesh[0]
            size = [1, total_devices, 1, shape[0]]
            stride = [0, 0, 0, 1]
        else:
            raise ValueError(f"Unsupported partition {partition_str} for 1D tensor.")

        return size, stride

    # Handle 2D tensor case
    if len(shape) == 2:
        m, n = shape
        if len(device_mesh) == 1:
            a, b = 1, device_mesh[0]
        elif len(device_mesh) == 2:
            if partition[0][0] == "S":
                partition[1] = (partition[1][0], 1 - partition[0][1])
            elif partition[1][0] == "S":  # partition[0][0] == "R"
                partition[0] = (partition[0][0], 1 - partition[1][1])
            else:
                partition[0] = (partition[0], 1)
                partition[1] = (partition[1], 0)
            a, b = device_mesh[-partition[0][1] - 1], device_mesh[-partition[1][1] - 1]
        else:
            a, b = device_mesh[-partition[0][1] - 1], device_mesh[-partition[1][1] - 1]

        if partition_str == "SS":
            # Both dimensions sharded
            size = [a, b, m // a, n // b]
            stride = [(m // a) * n, n // b, n, 1]

        elif partition_str == "SR":
            # First dim sharded across all devices, second replicated
            total_devices = a * b
            size = [1, total_devices, m // total_devices, n]
            stride = [0, (m // total_devices) * n, n, 1]

        elif partition_str == "RS":
            # First dim replicated, second sharded across second dim of mesh
            size = [1, b, m, n // b]
            stride = [(m * n) // (a * b), n // b, n, 1]

        elif partition_str == "RR":
            # Both dimensions replicated
            total_devices = a * b
            size = [1, 1, m, n]
            stride = [0, 0, n, 1]
        else:
            raise ValueError(f"Unsupported partition {partition_str} for 2D tensor.")

        return size, stride
    raise ValueError(f"Unsupported shape {shape} or partition {partition}.")


def extract_numbers(input_string):
    parts = input_string.split("_")
    numbers = []
    for part in parts:
        if part.isdigit():
            numbers.append(int(part))
    return tuple(numbers)


def map_kernels_to_device_mesh(kernel_shapes, device_shape):
    """
    Maps multiple kernels to a device mesh without overlapping.

    Args:
        kernel_shapes (dict): A dictionary mapping kernel names to their shapes.
                             For 3D kernels: [dim1, dim2, dim3]
                             For 2D kernels: [rows, cols]
                             For 1D kernels: [length]
        device_shape (list): The shape of the device mesh [rows, cols].

    Returns:
        dict: A dictionary mapping kernel names to their occupied device indices.
    """
    # Initialize the device mesh with 0s (unoccupied)
    rows, cols = device_shape
    device_mesh = [[0 for _ in range(cols)] for _ in range(rows)]

    # Dictionary to store the mapping of kernels to device indices
    kernel_to_indices = {}

    # Process each kernel
    for kernel_name, kernel_shape in kernel_shapes.items():
        # If kernel shape is 3D (e.g., [2, 2, 2])
        if len(kernel_shape) == 3:
            # Try both flattening options (e.g., 2x2x2 -> 4x2 or 2x4)
            option1 = [
                kernel_shape[0],
                kernel_shape[1] * kernel_shape[2],
            ]  # dim1 x (dim2*dim3)
            option2 = [
                kernel_shape[0] * kernel_shape[1],
                kernel_shape[2],
            ]  # (dim1*dim2) x dim3

            flattening_options = [option1, option2]

            # Try each flattening option
            placed = False
            for flat_kernel in flattening_options:
                kernel_rows, kernel_cols = flat_kernel

                # Skip if kernel doesn't fit in the mesh with this flattening
                if kernel_rows > rows or kernel_cols > cols:
                    continue

                # Try to place the flattened kernel
                for i in range(rows - kernel_rows + 1):
                    for j in range(cols - kernel_cols + 1):
                        # Check if the region is available
                        available = True
                        for di in range(kernel_rows):
                            for dj in range(kernel_cols):
                                if device_mesh[i + di][j + dj] == 1:
                                    available = False
                                    break
                            if not available:
                                break

                        if available:
                            # Place the kernel
                            indices = []
                            for di in range(kernel_rows):
                                for dj in range(kernel_cols):
                                    device_mesh[i + di][j + dj] = 1  # Mark as occupied
                                    indices.append((i + di, j + dj))
                            kernel_to_indices[kernel_name] = indices
                            placed = True
                            break
                    if placed:
                        break

                if placed:
                    break

        # If kernel shape is 1D (e.g., [4])
        elif len(kernel_shape) == 1:
            kernel_length = kernel_shape[0]

            # Skip if kernel doesn't fit in the mesh
            if kernel_length > rows:
                continue

            # Try to place it as a column
            placed = False
            for j in range(cols):
                if all(device_mesh[i][j] == 0 for i in range(kernel_length)):
                    # Place kernel as a column
                    indices = []
                    for i in range(kernel_length):
                        device_mesh[i][j] = 1  # Mark as occupied
                        indices.append((i, j))
                    kernel_to_indices[kernel_name] = indices
                    placed = True
                    break

        # If kernel shape is 2D (e.g., [2, 2])
        elif len(kernel_shape) == 2:
            kernel_rows, kernel_cols = kernel_shape

            # Skip if kernel doesn't fit in the mesh
            if kernel_rows > rows or kernel_cols > cols:
                continue

            # Try to place the kernel
            placed = False
            for i in range(rows - kernel_rows + 1):
                for j in range(cols - kernel_cols + 1):
                    # Check if the region is available
                    available = True
                    for di in range(kernel_rows):
                        for dj in range(kernel_cols):
                            if device_mesh[i + di][j + dj] == 1:
                                available = False
                                break
                        if not available:
                            break

                    if available:
                        # Place the kernel
                        indices = []
                        for di in range(kernel_rows):
                            for dj in range(kernel_cols):
                                device_mesh[i + di][j + dj] = 1  # Mark as occupied
                                indices.append((i + di, j + dj))
                        kernel_to_indices[kernel_name] = indices
                        placed = True
                        break
                if placed:
                    break

    return kernel_to_indices


def codegen_aie_mlir(
    mod,
    func_groups,
    inputs,
    outputs,
    kernel_buf_dicts,
    external_kernels,
    stream_info,
):
    """
    Generates MLIR-AIE code with MLIR module and extra information for multiple kernel functions

    offchip to shim: copy the whole data but reorder
    shim to mem: copy the whole data
    mem to comp: shard the data
    R: same line in object fifo (broadcast)
    S: multiple lines for object fifo, and link with offset

    Parameters
    ----------
    mod: allo._mlir.ir.Module
        The MLIR module built by allo.

    func_groups: Dict[str, List[FuncOp]]
        A dictionary mapping function names to lists of FuncOp objects.

    inputs: Dict[str, List[DTensor]]
        A dictionary mapping function names to lists of DTensor objects as inputs.

    outputs: Dict[str, List[DTensor]]
        A dictionary mapping function names to lists of DTensor objects as outputs.

    kernel_buf_dicts: Dict[str, Dict[str, Tuple[str, List[int]]]]
        The kernel buffer dictionaries for each function in the module.
        The key is the function name, and the value is a dictionary mapping buffer names to their types and shapes.

    external_kernels: Dict[str, List[str]]
        The external kernels that will be injected into the module.
        The key is the name of the function, and the value is a list of names of the external kernels.

    stream_info: Dict[str, List[Tuple[str, str]]]
        The input and output stream of each kernel.
        The key is the name of the kernel, and the value is a list of tuples.
        The first element in the tuple is the name of the stream, the second element is either 'in' or 'out'.
    """
    code = format_str("module {", indent=0)

    # Determine device based on maximum tensor arguments across all kernels
    max_num_args = 0
    for lst in (inputs, outputs):
        max_num_args += len(lst["_global"])
    mem_tile_size = max_num_args // 2 + 1
    shim_tile_size = max_num_args // 2 + 1
    device = "npu1_4col"
    code += format_str(f"aie.device({device}) {{", indent=2)

    # Add external functions
    for func in mod.body.operations:
        if (
            isinstance(func, func_d.FuncOp)
            and "sym_visibility" in func.attributes
            and func.attributes["sym_visibility"].value == "private"
        ):
            code += format_str(str(func), indent=4)

    # Create shim and memory tiles
    # mlir-aie/mlir_tutorials/tutorial-4/flow
    # | Bundle | Channels (In) | Channels (Out) |
    # |-------|---|---|
    # | DMA   | 2 | 2 |
    # | Core  | 2 | 2 |
    # | West  | 4 | 4 |
    # | East  | 4 | 4 |
    # | North | 4 | 6 |
    # | South | 6 | 4 |
    # | FIFO  | 2 | 2 |
    # | Trace | 1 | 0 |
    for shim_id in range(shim_tile_size):
        code += format_str(f"%tile_shim{shim_id} = aie.tile({shim_id}, 0)")
    for mem_id in range(mem_tile_size):
        code += format_str(f"%tile_mem{mem_id} = aie.tile({mem_id}, 1)")

    # Get top function and all other functions
    top_func, all_funcs = get_public_funcs(mod)

    # Track vertical position for tile placement
    aie_mesh = (5, 4)
    y_offset = 2

    # Create compute tiles and store kernel info
    mappings = {}
    for func_name in func_groups:
        if len(inputs[func_name]["_global"]) > 0:
            mappings[func_name] = inputs[func_name]["_global"][0].mapping
        else:
            mappings[func_name] = outputs[func_name]["_global"][0].mapping
    for func_name, tile_ids in map_kernels_to_device_mesh(mappings, aie_mesh).items():
        for idx, func in zip(tile_ids, func_groups[func_name]):
            func_full_name = func.attributes["sym_name"].value
            code += format_str(
                f"%tile_comp_{func_full_name} = aie.tile({idx[0]}, {idx[1] + y_offset})"
            )

    # Create buffers and process function strings for each kernel
    func_strs = []
    for func in all_funcs:
        func_name = func.attributes["sym_name"].value
        tile_name = f"%tile_comp_{func_name}"
        buf_dict = kernel_buf_dicts[func_name]
        buf_name_dict = {}
        for i, name in enumerate(buf_dict.keys()):
            new_name = f"{tile_name}_buf{i}"
            buf_name_dict[name] = new_name
            ele_type, shape = buf_dict[name]
            str_list = list(map(str, shape))
            str_list.append(ele_type)
            buf_type = f"memref<{'x'.join(str_list)}>"
            code += format_str(f"{new_name} = aie.buffer({tile_name}) : {buf_type}")

        # Remove memref.alloc
        pattern_alloc = re.compile(r"^.*memref\.alloc.*\n?", re.MULTILINE)
        func_str = re.sub(pattern_alloc, "", str(func))

        # Replace new buffer name
        pattern_boundary = r"(?<![\w.]){old}(?![\w.])"
        for name, new_name in buf_name_dict.items():
            escaped_name = re.escape(name)
            pattern = pattern_boundary.format(old=escaped_name)
            func_str = re.sub(pattern, new_name, func_str)

        func_strs.append(func_str)

    # Create object FIFOs for each kernel
    tile2fifo = {}
    for io, arg_lst in (("in", inputs), ("out", outputs)):
        for func_name, sub_func_lst in arg_lst.items():
            if func_name == "_global":
                continue
            for idx, dtensor in enumerate(sub_func_lst["_global"]):
                mapping = dtensor.mapping
                spec = dtensor.layout
                placement = spec.get_placement(mapping)
                global_memref_type = get_memref_type_str(dtensor.dtype, dtensor.shape)
                # shim to mem tile
                if io == "in":
                    code += format_str(
                        f"aie.objectfifo @in_shim_{dtensor.name}(%tile_shim{idx % shim_tile_size}, {{%tile_mem{idx % mem_tile_size}}}, 2 : i32) : !aie.objectfifo<{global_memref_type}>"
                    )
                else:
                    code += format_str(
                        f"aie.objectfifo @out_shim_{dtensor.name}(%tile_mem0, {{%tile_shim0}}, 2 : i32) : !aie.objectfifo<{global_memref_type}>"
                    )
                # mem to comp tile
                mem_strs = []
                mem_stride = [0]
                for tensor_tile, target_pe_tiles in placement.items():
                    arg_name = dtensor.name + "_" + tensor_tile
                    tile_strs = []
                    mem_strs.append(f"@{io}_mem_{arg_name}")
                    for tile in target_pe_tiles:
                        if dtensor not in sub_func_lst[tile]:
                            # this arg is not used in this kernel
                            continue
                        idx_str = "_".join(map(str, tile))
                        core_name = f"%tile_comp_{func_name}_{idx_str}"
                        if core_name not in tile2fifo:
                            tile2fifo[core_name] = [mem_strs[-1]]
                        else:
                            tile2fifo[core_name].append(mem_strs[-1])
                        tile_strs.append(core_name)
                    tile_str = ", ".join(tile_strs)
                    local_memref_type = get_memref_type_str(
                        dtensor.dtype, dtensor.get_local_shape()
                    )
                    mem_stride.append(
                        mem_stride[-1] + np.prod(dtensor.get_local_shape())
                    )
                    if io == "in":
                        code += format_str(
                            f"aie.objectfifo {mem_strs[-1]}(%tile_mem{idx % mem_tile_size}, {{{tile_str}}}, 2 : i32) : !aie.objectfifo<{local_memref_type}>"
                        )
                    else:
                        code += format_str(
                            f"aie.objectfifo {mem_strs[-1]}({tile_str}, {{%tile_mem0}}, 2 : i32) : !aie.objectfifo<{local_memref_type}>"
                        )
                # important to sort to guarantee result correctness
                # output should have spec of SN SN-1 ... S1 S0
                mem_str = ", ".join(sorted(mem_strs))
                mem_stride = mem_stride[:-1]
                if io == "in":
                    code += format_str(
                        f"aie.objectfifo.link [@in_shim_{dtensor.name}] -> [{mem_str}]([] {mem_stride})"
                    )
                else:
                    code += format_str(
                        f"aie.objectfifo.link [{mem_str}] -> [@out_shim_{dtensor.name}]({mem_stride} [])"
                    )

    # Create stream object FIFOs from top_func
    stream_ele_types = {}
    stream_in_out = get_stream_in_out(stream_info)
    for op in top_func.entry_block.operations:
        if isinstance(op, allo_d.StreamConstructOp):
            stream_name = op.attributes["name"].value
            if stream_name in stream_in_out:
                in_out = stream_in_out[stream_name]
                stream_type_str = str(op.results.types[0])
                start = stream_type_str.find("<") + 1
                end = stream_type_str.rfind(">")
                type_str, depth_str = stream_type_str[start:end].split(",")
                type_str = type_str.strip()
                if not type_str.startswith("memref"):
                    type_str = f"memref<{type_str}>"
                depth = int(depth_str.strip())
                # Create the stream object FIFO between the two kernels
                code += format_str(
                    f"aie.objectfifo @{stream_name}(%tile_comp_{in_out[0]}, {{%tile_comp_{in_out[1]}}}, {depth} : i32) : !aie.objectfifo<{type_str}>"
                )
                stream_ele_types[stream_name] = type_str

    # Create core computation for each kernel function
    for func_gid, (func, func_str) in enumerate(zip(all_funcs, func_strs)):
        func_name_w_id = func.attributes["sym_name"].value
        func_name = re.match(r"^(.*?)_\d", func_name_w_id).group(1)
        func_id = tuple(map(int, func_name_w_id.split(func_name + "_")[-1].split("_")))
        core_name = f"tile_comp_{func_name_w_id}"
        streams = stream_info[func_name_w_id]

        # Generate core computation
        code += format_str(
            f"%core_0_{func_gid + 2}_{core_name} = aie.core(%{core_name}) {{"
        )
        with format_code(indent=6):
            code += format_str("%global_c0 = arith.constant 0 : index")
            code += format_str("%global_c1 = arith.constant 1 : index")
            code += format_str(
                "%c9223372036854775807 = arith.constant 9223372036854775807 : index"
            )
            code += format_str(
                "scf.for %arg0 = %global_c0 to %c9223372036854775807 step %global_c1 {"
            )
            with format_code(indent=8):
                # Helper function to process FIFOs (acquire or release)
                def process_fifos(is_input, is_acquire):
                    nonlocal code, func_str
                    tensors = (
                        inputs[func_name][func_id]
                        if is_input
                        else outputs[func_name][func_id]
                    )
                    operation = "Consume" if is_input else "Produce"
                    arg_offset = 0 if is_input else len(inputs[func_name][func_id])

                    for arg_id, tensor in enumerate(tensors):
                        # Calculate fifo index and get fifo name
                        fifo_idx = (
                            arg_id
                            if is_input
                            else arg_id + len(inputs[func_name][func_id])
                        )
                        fifo_name = tile2fifo[f"%{core_name}"][fifo_idx]

                        if is_acquire:
                            # Acquire FIFO and access subview
                            dtype = get_memref_type_str(
                                tensor.dtype, tensor.get_local_shape()
                            )
                            var_prefix = "fifo" if is_input else "fifo_out"
                            local_prefix = "local" if is_input else "local_out"

                            code += format_str(
                                f"%{var_prefix}{arg_id} = aie.objectfifo.acquire {fifo_name}({operation}, 1) : !aie.objectfifosubview<{dtype}>"
                            )
                            code += format_str(
                                f"%{local_prefix}{arg_id} = aie.objectfifo.subview.access %{var_prefix}{arg_id}[0] : !aie.objectfifosubview<{dtype}> -> {dtype}"
                            )

                            # Replace argument in function string
                            arg_name = f"%arg{arg_id + arg_offset}"
                            local_name = f"%{local_prefix}{arg_id}"
                            func_str = func_str.replace(arg_name, local_name)
                        else:
                            # Release FIFO
                            code += format_str(
                                f"aie.objectfifo.release {fifo_name}({operation}, 1)"
                            )

                # Acquire input and output FIFOs
                process_fifos(is_input=True, is_acquire=True)
                process_fifos(is_input=False, is_acquire=True)

                # Fix call operations
                while " call @" in func_str:
                    func_str = func_str.replace(" call @", " func.call @")

                # Process stream operations
                stream_code, func_str = process_stream_operations(
                    func_str,
                    streams,
                    len(inputs[func_name]["_global"])
                    + len(outputs[func_name]["_global"]),
                    stream_ele_types,
                )
                code += stream_code

                # Release input and output FIFOs
                process_fifos(is_input=True, is_acquire=False)
                process_fifos(is_input=False, is_acquire=False)

            code += format_str("}")
            code += format_str("aie.end")

        # Add linking information if needed
        code += "    }"
        if len(external_kernels.get(func_name_w_id, [])) > 0:
            code += ' {link_with = "external.o"}\n'
        else:
            code += "\n"

    # Create runtime sequence with all inputs and outputs
    in_args = []
    out_args = []
    global_idx = 0
    for io_lst, io_args, io in ((inputs, in_args, "in"), (outputs, out_args, "out")):
        for i, dtensor in enumerate(
            io_lst["_global"], start=global_idx if io == "out" else 0
        ):
            global_memref_type = get_memref_type_str(dtensor.dtype, dtensor.shape)
            io_args.append(f"%arg{i}: {global_memref_type}")
            if io == "in":
                global_idx += 1

    # Create dma transfer from off-chip mem to shim tile
    code += format_str(
        f"aiex.runtime_sequence({', '.join(in_args)}, {', '.join(out_args)}) {{"
    )

    with format_code(indent=6):
        # Helper function to process DMA operations for inputs and outputs
        def process_dma_operations(tensor_lst, is_input):
            nonlocal code
            prefix = "in" if is_input else "out"
            start_idx = 0 if is_input else global_idx
            # Calculate access pattern
            for idx, dtensor in enumerate(tensor_lst, start=start_idx):
                name = dtensor.name
                size, stride = calculate_tensor_access(
                    dtensor.shape, dtensor.layout.placement, dtensor.mapping
                )
                global_memref_type = get_memref_type_str(dtensor.dtype, dtensor.shape)
                # Build DMA attributes - inputs need issue_token, outputs don't
                if is_input:
                    dma_attrs = f"id = {idx} : i64, issue_token = true, metadata = @{prefix}_shim_{name}"
                else:
                    dma_attrs = f"id = {idx} : i64, metadata = @{prefix}_shim_{name}"
                # Create DMA operation
                code += format_str(
                    f"aiex.npu.dma_memcpy_nd(0, 0, %arg{idx}[0, 0, 0, 0]{size}{stride}) {{{dma_attrs}}} : {global_memref_type}"
                )

        # Process DMA operations for inputs and outputs
        process_dma_operations(inputs["_global"], is_input=True)
        process_dma_operations(outputs["_global"], is_input=False)

        # Helper function for DMA wait operations
        def process_dma_wait(tensor_lst, is_input):
            nonlocal code
            prefix = "in" if is_input else "out"
            for dtensor in tensor_lst:
                code += format_str(
                    f"aiex.npu.dma_wait {{symbol = @{prefix}_shim_{dtensor.name}}}"
                )

        # Wait for all DMAs to complete
        process_dma_wait(inputs["_global"], is_input=True)
        process_dma_wait(outputs["_global"], is_input=False)

    code += format_str("}")
    code += format_str("}", indent=2)
    code += "}"
    return code


def lower_tensor_to_memref(mod):
    passes = [
        # "linalg-generalize-named-ops",
        # "linalg-fuse-elementwise-ops",
        "func.func(convert-linalg-to-affine-loops),lower-affine",
    ]
    pipeline = f'builtin.module({",".join(passes)})'
    with mod.context:
        mlir_pass_manager.parse(pipeline).run(mod.operation)


def record_local_buffer(mod):
    kernel_buf_dicts = {}
    _, funcs = get_public_funcs(mod)

    def traverse_operations(operations, buf_dict):
        for op in operations:
            if op.operation.name == "memref.alloc":
                name = op.result.get_name()
                dtype, shape = get_dtype_and_shape_from_type(op.result.type)
                buf_dict[name] = (dtype, shape)

            # Recursively traverse into all regions and blocks of the operation
            for region in op.regions:
                for block in region.blocks:
                    traverse_operations(block.operations, buf_dict)

    for func in funcs:
        func_name = func.attributes["sym_name"].value
        buf_dict = {}

        # Start traversal from the top-level operations of the function
        for block in func.regions[0].blocks:
            traverse_operations(block.operations, buf_dict)

        kernel_buf_dicts[func_name] = buf_dict

    return kernel_buf_dicts


class AIEModule:
    def __init__(
        self,
        module,
        top_func_name,
        func_args,
        project,
        stream_info,
    ):
        self.module = module
        self.top_func_name = top_func_name
        self.project = project
        self.module = module
        self.func_args = func_args
        self.stream_info = stream_info

    def build(self):
        assert "MLIR_AIE_INSTALL_DIR" in os.environ, "Please set MLIR_AIE_INSTALL_DIR"
        assert "PEANO_INSTALL_DIR" in os.environ, "Please set PEANO_INSTALL_DIR"
        os.makedirs(os.path.join(self.project, "build"), exist_ok=True)
        external_kernels = inject_aie_kernels(self.module)
        with open(
            os.path.join(self.project, "original.mlir"), "w", encoding="utf-8"
        ) as f:
            f.write(str(self.module))
        lower_tensor_to_memref(self.module)
        kernel_buf_dicts = record_local_buffer(self.module)
        _, all_funcs = get_public_funcs(self.module)
        # Create a dictionary to store the kernel functions
        func_groups = {}
        for func in all_funcs:
            func_name_w_id = func.attributes["sym_name"].value
            func_name = re.match(r"^(.*?)_\d", func_name_w_id).group(1)
            if func_name not in func_groups:
                func_groups[func_name] = []
            func_groups[func_name].append(func)
        inputs = {}
        outputs = {}
        for func_name, funcs in func_groups.items():
            inputs[func_name] = {}
            outputs[func_name] = {}
            inputs[func_name]["_global"] = []
            outputs[func_name]["_global"] = []
            for func in funcs:
                # Even for functions inside the same group, the in/out arguments may be different
                func_name_w_id = func.attributes["sym_name"].value
                func_id = tuple(
                    map(int, func_name_w_id.split(func_name + "_")[-1].split("_"))
                )
                in_idx, out_idx = analyze_read_write_patterns(func)
                for io_lst, io_idx in ((inputs, in_idx), (outputs, out_idx)):
                    io_lst[func_name][func_id] = []
                    for idx in io_idx:
                        dtensor = self.func_args[func_name_w_id][idx]
                        if dtensor not in io_lst[func_name]["_global"]:
                            io_lst[func_name]["_global"].append(dtensor)
                        io_lst[func_name][func_id].append(dtensor)
        for io_lst in (inputs, outputs):
            io_lst["_global"] = []
            for func_name, sub_func_lst in io_lst.items():
                if func_name == "_global":
                    continue
                for tensor in sub_func_lst["_global"]:
                    if tensor not in io_lst["_global"]:
                        io_lst["_global"].append(tensor)
        self.inputs = inputs
        self.outputs = outputs
        code = codegen_aie_mlir(
            self.module,
            func_groups,
            inputs,
            outputs,
            kernel_buf_dicts,
            external_kernels,
            self.stream_info,
        )
        with open(os.path.join(self.project, "top.mlir"), "w", encoding="utf-8") as f:
            f.write(code)
        # compile external kernels
        kernel_code, generated_kernels = codegen_external_kernels(external_kernels)
        if len(generated_kernels) > 0:
            with open(
                os.path.join(self.project, "external.cc"), "w", encoding="utf-8"
            ) as f:
                f.write(kernel_code)
            path = os.path.join(os.path.dirname(__file__), "aie_kernels")
            cmd = f"cd {self.project} && $PEANO_INSTALL_DIR/bin/clang++ -O2 -v -std=c++20 --target=aie2-none-unknown-elf -Wno-parentheses -Wno-attributes -Wno-macro-redefined -DNDEBUG -I $(dirname $(which aie-opt))/../include -I $MLIR_AIE_INSTALL_DIR/../aie_kernels/aie2 -c external.cc -o external.o"
            process = subprocess.Popen(cmd, shell=True)
            process.wait()
            if process.returncode != 0:
                raise RuntimeError("Failed to compile external kernels.")
        # build mlir-aie
        cmd = f"cd {self.project} && PYTHONPATH=$MLIR_AIE_INSTALL_DIR/python aiecc.py --aie-generate-cdo --aie-generate-npu --no-compile-host --no-xchesscc --no-xbridge --xclbin-name=build/final.xclbin --npu-insts-name=insts.txt top.mlir"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to compile the MLIR-AIE code")
        path = os.path.dirname(__file__)
        path = os.path.join(path, "../harness/aie")
        os.system(f"cp -r {path}/* {self.project}")
        host_code = codegen_host(inputs["_global"], outputs["_global"])
        with open(os.path.join(self.project, "test.cpp"), "w", encoding="utf-8") as f:
            f.write(host_code)
        cmd = f"cd {self.project}/build && cmake .. -DTARGET_NAME=top -DMLIR_AIE_DIR=$MLIR_AIE_INSTALL_DIR/.. && cmake --build . --config Release"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to build AIE project.")
        return self

    def __call__(self, *args):
        # suppose the last argument is output
        for i, arg in enumerate(args[:-1]):
            with open(
                os.path.join(self.project, f"input{i}.data"), "w", encoding="utf-8"
            ) as f:
                f.write("\n".join([str(i) for i in arg.flatten()]))
        cmd = f"cd {self.project} && ./build/top -x build/final.xclbin -i insts.txt -k MLIR_AIE"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to execute AIE code.")
        # TODO: need to complete multiple outputs rules
        result = read_tensor_from_file(
            self.outputs["_global"][-1].dtype,
            args[-1].shape,
            f"{self.project}/output.data",
        )
        args[-1][:] = result
