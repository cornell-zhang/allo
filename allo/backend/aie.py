# Copyright Allo authors. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
# mlir-aie commit: 8329b6
# pylint: disable=bad-builtin, no-name-in-module, unnecessary-lambda-assignment, cell-var-from-loop, too-many-branches, consider-using-with

import os
import subprocess
import re
from collections import defaultdict
import numpy as np
from .._mlir.ir import (
    RankedTensorType,
    MemRefType,
    FunctionType,
    TypeAttr,
    Location,
    InsertionPoint,
    FlatSymbolRefAttr,
    StringAttr,
    BlockArgument,
)
from .._mlir.dialects import (
    allo as allo_d,
    func as func_d,
)
from .._mlir.passmanager import PassManager as mlir_pass_manager

from .vitis import read_tensor_from_file
from ..utils import (
    get_dtype_and_shape_from_type,
    get_element_type_from_str,
)
from .utils import format_str, format_code
from .vitis import ctype_map
from ..passes import analyze_read_write_patterns


class DTensor:
    """
    A class to represent a distributed tensor.
    """

    def __init__(self, rank, mapping, shape, dtype):
        self.rank = rank
        self.mapping = mapping
        # global shape
        self.shape = shape
        self.dtype = dtype
        # "S": Sharded
        # "R": Replicated
        self.placement = "R" * len(shape)
        self.offset = [0] * len(shape)
        self.local_shape = list(shape)

    def set_placement(self, placement):
        """
        Set placement strategy for each dimension.

        Args:
            placement (str): String of 'S' and 'R' characters, one per dimension
        """
        if len(placement) != len(self.shape):
            raise ValueError(
                f"Placement length {len(placement)} doesn't match shape dimensions {len(self.shape)}"
            )
        self.placement = placement
        return self

    def set_subview(self, offset, local_shape):
        """
        Set the local subview parameters.

        Args:
            offset (list): Starting offsets for each dimension
            local_shape (list): Sizes for each dimension
        """
        if len(offset) != len(self.shape) or len(local_shape) != len(self.shape):
            raise ValueError(
                "Offset and local_shape must have same dimensions as shape"
            )
        self.offset = offset
        self.local_shape = local_shape
        return self

    def __str__(self):
        placement_desc = ", ".join(
            [
                f"{dim}: {'Sharded' if p == 'S' else 'Replicated'}"
                for dim, p in enumerate(self.placement)
            ]
        )
        return (
            f"DTensor(shape={self.shape}, dtype={self.dtype}, placement=[{placement_desc}], "
            f"offset={self.offset}, local_shape={self.local_shape})"
        )


class KernelFunction:
    """
    A class to represent a kernel function in the MLIR module.
    """

    def __init__(self, name, mapping, inputs, outputs):
        self.name = name
        self.mapping = mapping
        self.inputs = inputs  # List of tuples (dtype, shape)
        self.outputs = outputs  # List of tuples (dtype, shape)
        self.funcs = [None] * np.prod(
            mapping
        )  # List of MLIR functions for each mapping
        self.dtensors = [None] * np.prod(mapping)  # List of DTensors for each mapping

    def set_mlir_funcs(self, funcs):
        if len(funcs) != np.prod(self.mapping):
            raise ValueError("Number of MLIR functions must match the mapping size")
        self.funcs = funcs

    def set_dtensors(self, dtensors, idx):
        # Each function is associated with a list of dtensors
        if idx < 0 or idx >= np.prod(self.mapping):
            raise IndexError("Index out of range for the mapping size")
        self.dtensors[idx] = dtensors

    def __repr__(self):
        return f"KernelFunction(name={self.name}, mapping={self.mapping}, inputs={self.inputs}, outputs={self.outputs})"


host_header = """
//=============================================================================
// Auto generated by Allo
//=============================================================================

#include <boost/program_options.hpp>
#include <cstdint>
#include <fstream>
#include <iostream>
#include <sstream>
#include <string>
#include <vector>

#include "xrt/xrt_bo.h"
#include "xrt/xrt_device.h"
#include "xrt/xrt_kernel.h"

#include "test_utils.h"

namespace po = boost::program_options;

int main(int argc, const char *argv[]) {

  // ------------------------------------------------------
  // Parse program arguments
  // ------------------------------------------------------
  po::options_description desc("Allowed options");
  po::variables_map vm;
  test_utils::add_default_options(desc);

  test_utils::parse_options(argc, argv, desc, vm);
  int verbosity = vm["verbosity"].as<int>();
  int do_verify = vm["verify"].as<bool>();
  int n_iterations = vm["iters"].as<int>();
  int n_warmup_iterations = vm["warmup"].as<int>();
  int trace_size = vm["trace_sz"].as<int>();

  // Load instruction sequence
  std::vector<uint32_t> instr_v =
      test_utils::load_instr_sequence(vm["instr"].as<std::string>());
  if (verbosity >= 1)
    std::cout << "Sequence instr count: " << instr_v.size() << "\\n";

  // ------------------------------------------------------
  // Get device, load the xclbin & kernel and register them
  // ------------------------------------------------------
  // Get a device handle
  unsigned int device_index = 0;
  auto device = xrt::device(device_index);

  // Load the xclbin
  if (verbosity >= 1)
    std::cout << "Loading xclbin: " << vm["xclbin"].as<std::string>() << "\\n";
  auto xclbin = xrt::xclbin(vm["xclbin"].as<std::string>());

  // Load the kernel
  if (verbosity >= 1)
    std::cout << "Kernel opcode: " << vm["kernel"].as<std::string>() << "\\n";
  std::string Node = vm["kernel"].as<std::string>();

  // Get the kernel from the xclbin
  auto xkernels = xclbin.get_kernels();
  auto xkernel = *std::find_if(xkernels.begin(), xkernels.end(),
                               [Node, verbosity](xrt::xclbin::kernel &k) {
                                 auto name = k.get_name();
                                 if (verbosity >= 1) {
                                   std::cout << "Name: " << name << std::endl;
                                 }
                                 return name.rfind(Node, 0) == 0;
                               });
  auto kernelName = xkernel.get_name();

  // Register xclbin
  if (verbosity >= 1)
    std::cout << "Registering xclbin: " << vm["xclbin"].as<std::string>()
              << "\\n";
  device.register_xclbin(xclbin);

  // Get a hardware context
  if (verbosity >= 1)
    std::cout << "Getting hardware context.\\n";
  xrt::hw_context context(device, xclbin.get_uuid());

  // Get a kernel handle
  if (verbosity >= 1)
    std::cout << "Getting handle to kernel:" << kernelName << "\\n";
  auto kernel = xrt::kernel(context, kernelName);

  // ------------------------------------------------------
  // Initialize input/ output buffer sizes and sync them
  // ------------------------------------------------------
  auto bo_instr = xrt::bo(device, instr_v.size() * sizeof(int),
                          XCL_BO_FLAGS_CACHEABLE, kernel.group_id(1));
  void *bufInstr = bo_instr.map<void *>();
  memcpy(bufInstr, instr_v.data(), instr_v.size() * sizeof(int));

  std::ofstream ofile("output.data");
  if (!ofile.is_open()) {
      std::cerr << "Error: Could not open output file.\\n";
      return 1;
  }

"""

file_close_str = """  ofile.close();
  if (verbosity >= 1)
    std::cout << "Array has been written to output.data.\\n";
  return 0;
}
"""


def codegen_host(inputs, outputs):
    code = host_header
    with format_code(indent=2):
        # write input data
        for i, (dtype, shape) in enumerate(inputs):
            dtype = ctype_map[dtype]
            code += format_str(f'std::ifstream ifile{i}("input{i}.data");')
            code += format_str(f"if (!ifile{i}.is_open()) {{")
            code += format_str(
                '  std::cerr << "Error: Could not open input file.\\n";', strip=False
            )
            code += format_str("  return 1;", strip=False)
            code += format_str("}")
            size = np.prod(shape)
            code += format_str(
                f"auto bo_in{i} = xrt::bo(device, {size} * sizeof({dtype}),"
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({i + 3}));"
                )
            code += format_str(f"{dtype} *bufIn{i} = bo_in{i}.map<{dtype} *>();")
            code += format_str(f"std::vector<{dtype}> srcVec{i};")
            code += format_str(f"for (int i = 0; i < {size}; i++) {{")
            with format_code(indent=4):
                code += format_str(f"{dtype} num;")
                code += format_str(f"ifile{i} >> num;")
                code += format_str(f"srcVec{i}.push_back(num);")
            code += format_str("}")
            code += format_str(
                f"memcpy(bufIn{i}, srcVec{i}.data(), (srcVec{i}.size() * sizeof({dtype})));"
            )
        for i, (dtype, shape) in enumerate(outputs):
            dtype = ctype_map[dtype]
            out_size = np.prod(shape)
            code += format_str(
                f"\nauto bo_out{i} = xrt::bo(device, {out_size} * sizeof({dtype}),",
                strip=False,
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({len(inputs) + 2 + i}));"
                )
        code += format_str("if (verbosity >= 1)")
        code += format_str(
            '  std::cout << "Writing data into buffer objects.\\n";', strip=False
        )
        code += format_str("\nbo_instr.sync(XCL_BO_SYNC_BO_TO_DEVICE);", strip=False)
        for i in range(len(inputs)):
            code += format_str(f"bo_in{i}.sync(XCL_BO_SYNC_BO_TO_DEVICE);")
        # run kernels
        code += format_str("if (verbosity >= 1)")
        code += format_str('  std::cout << "Running Kernel.\\n";', strip=False)
        code += format_str(
            "\nauto start = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str("unsigned int opcode = 3;", strip=False)
        inbufs = ", ".join([f"bo_in{i}" for i in range(len(inputs))])
        outbufs = ", ".join([f"bo_out{i}" for i in range(len(outputs))])
        code += format_str("// gid: (opcode, instr, instr_size, ...)")
        code += format_str(
            f"auto run = kernel(opcode, bo_instr, instr_v.size(), {inbufs}, {outbufs});"
        )
        code += format_str("run.wait();")
        code += format_str(
            "\nauto end = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str(
            "float npu_time = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();"
        )
        code += format_str(
            'std::cout << "NPU execution time: " << npu_time << "us\\n";'
        )
        # get results
        for i, (dtype, shape) in enumerate(outputs):
            dtype = ctype_map[dtype]
            out_size = np.prod(shape)
            code += format_str(
                f"\nbo_out{i}.sync(XCL_BO_SYNC_BO_FROM_DEVICE);", strip=False
            )
            code += format_str(f"{dtype} *bufOut{i} = bo_out{i}.map<{dtype} *>();")
            code += format_str(f"for (uint32_t i = 0; i < {out_size}; i++) {{")
            code += format_str(f'  ofile << *(bufOut{i} + i) << "\\n";', strip=False)
            code += format_str("}")
        code += format_str("\n// Close files", strip=False)
        for i in range(len(inputs)):
            code += format_str(f"ifile{i}.close();")
        code += file_close_str
    return code


def get_stream_in_out(stream_info):
    """
    Computes the mapping of FIFO names to their producer (output) and consumer (input) cores.

    Parameters
    ----------
    stream_info (Dict[int, List[Tuple[str, str]]]):
        A dictionary where the key is a core ID (int), and the value is a list of tuples.
        Each tuple contains:
        - fifo_name (str): The name of the FIFO.
        - direction (str): The direction of data flow, either "in" (consumer) or "out" (producer).

    Returns
    -------
    Dict[str, Tuple[Optional[int], Optional[int]]]]:
        A dictionary where the key is a FIFO name, and the value is a tuple:
        - The first element is the producer function and the second element is the consumer function.
    """
    stream_in_out = {}
    for core, fifos in stream_info.items():
        for fifo_name, direction in fifos:
            if fifo_name not in stream_in_out:
                stream_in_out[fifo_name] = (None, None)
            if direction == "in":
                stream_in_out[fifo_name] = (stream_in_out[fifo_name][0], core)
            elif direction == "out":
                stream_in_out[fifo_name] = (core, stream_in_out[fifo_name][1])
    return stream_in_out


def get_public_funcs(mod):
    funcs = []
    top_func = None
    for func in mod.body.operations:
        if isinstance(func, func_d.FuncOp) and (
            "sym_visibility" not in func.attributes
            or func.attributes["sym_visibility"].value != "private"
        ):
            if func.attributes["sym_name"].value == "top":
                top_func = func
            else:
                funcs.append(func)
    return top_func, funcs


def inject_aie_kernels(mod):
    external_kernels = {}
    injected_kernels = set()
    with mod.context, Location.unknown():
        for func in mod.body.operations:
            external_kernels[func.attributes["sym_name"].value] = []
            for block in func.regions[0].blocks:
                for op in block.operations:
                    if (
                        op.operation.name in {"linalg.add", "linalg.mul"}
                        and len(MemRefType(op.inputs[0].type).shape) == 1
                    ):
                        # TODO: Fix matmul
                        # or op.operation.name == "linalg.matmul":
                        op_name = op.operation.name.split(".")[1]
                        # Inject AIE kernel
                        func_type = func_d.FunctionType.get(
                            [op.inputs[0].type, op.inputs[1].type, op.outputs[0].type],
                            [],
                        )
                        dtype = str(op.inputs[0].type.element_type)
                        shape = MemRefType(op.inputs[0].type).shape
                        if op.operation.name in {"linalg.add", "linalg.mul"}:
                            kernel_name = f"{op_name}_{dtype}_vector"
                        else:  # linalg.matmul
                            kernel_name = f"matmul_{dtype}_i16"
                        func_d.CallOp(
                            [],
                            FlatSymbolRefAttr.get(kernel_name),
                            [op.inputs[0], op.inputs[1], op.outputs[0]],
                            ip=InsertionPoint(op),
                        )
                        op.erase()
                        external_kernels[func.attributes["sym_name"].value].append(
                            (op_name, dtype, shape)
                        )
                        if kernel_name in injected_kernels:
                            continue
                        injected_kernels.add(kernel_name)
                        kernel = func_d.FuncOp(
                            kernel_name,
                            func_type,
                            ip=InsertionPoint(func),
                        )
                        kernel.attributes["sym_visibility"] = StringAttr.get("private")
    return external_kernels


def codegen_external_kernels(external_kernels):
    code = ""
    code += "// External kernels generated by Allo\n\n"
    code += "#include <stdint.h>\n"
    code += "#include <stdio.h>\n"
    code += "#include <stdlib.h>\n"
    code += "#include <type_traits>\n"
    code += "#include <aie_api/aie.hpp>\n\n"
    generated_kernels = set()
    kernel_code = ""
    for _, kernel_lst in external_kernels.items():
        for kernel, dtype, shape in kernel_lst:
            if kernel in generated_kernels:
                continue
            ctype = ctype_map[dtype]
            if "bfloat" in ctype:
                ctype = "bfloat16"
            if kernel == "matmul":
                pass
            else:
                kernel_code += f"void {kernel}_{dtype}_vector"
                kernel_code += f"({ctype} *A_in, {ctype} *B_in, {ctype} *C_out)"
                kernel_code += " {\n"
                kernel_code += f"  eltwise_v{kernel}<{ctype}, {ctype}, {np.prod(shape)}>(A_in, B_in, C_out);\n"
                kernel_code += "}\n\n"
            generated_kernels.add(kernel)
    for kernel in generated_kernels:
        match kernel:
            case "add":
                code += '#include "add.cc"\n'
            case "mul":
                code += '#include "mul.cc"\n'
            case "matmul":
                # code += f"#define DIM_M {8}\n"
                # code += f"#define DIM_N {8}\n"
                # code += f"#define DIM_K {16}\n"
                # code += f"#define i16_i16_ONLY\n"
                code += '#include "mm.cc"\n'
    code += '\nextern "C" {\n\n'
    code += kernel_code
    code += '} // extern "C"\n'
    return code, generated_kernels


def process_stream_operations(func_str, streams, inputs, outputs, stream_ele_types):
    """
    Process a function string by replacing stream_get and stream_put calls with
    corresponding formatted FIFO code.

    Parameters
    ----------
        func_str (str): The input function string to be processed.
        streams (list): List of streams (each stream is a tuple/list where the first element is the stream name).
        inputs (list): List of input identifiers.
        outputs (list): List of output identifiers.
        stream_ele_types (dict): Dictionary mapping stream names to their element types.

    Returns
    -------
        str: The resulting formatted code.
    """
    code = ""
    with format_code(indent=6):
        lines = func_str.splitlines()
        for i in range(1, len(lines) - 2):
            line = lines[i]
            # Process stream_get
            if "stream_get" in line:
                # extract the argument id from the pattern "stream_get(%arg<digits>"
                m_get = re.search(r"stream_get\(%arg(\d+)", line)
                if m_get:
                    arg_id = int(m_get.group(1))
                else:
                    continue
                # Extract the return variable
                return_var = line.split("=")[0].strip()
                stream_index = arg_id - len(inputs) - len(outputs)
                stream_name = streams[stream_index][0]
                current_indent = 6 + (len(line) - len(line.lstrip(" ")))
                with format_code(indent=current_indent):
                    # Acquire the FIFO
                    ele_type = stream_ele_types[stream_name]
                    code += format_str(
                        f"%{stream_name} = aie.objectfifo.acquire @{stream_name}(Consume, 1) : !aie.objectfifosubview<{ele_type}>"
                    )
                    code += format_str(
                        f"%local_{stream_name} = aie.objectfifo.subview.access %{stream_name}[0] : !aie.objectfifosubview<{ele_type}> -> {ele_type}"
                    )
                    # Load the value into a local variable if the element type is scalar
                    if "x" not in ele_type:
                        code += format_str(
                            f"{return_var} = memref.load %local_{stream_name}[] : {ele_type}"
                        )
                    else:
                        # Otherwise, replace the return variable with the local stream variable
                        func_str = func_str.replace(return_var, f"%local_{stream_name}")
                        lines = func_str.splitlines()
                    # Release the FIFO
                    code += format_str(
                        f"aie.objectfifo.release @{stream_name}(Consume, 1)"
                    )
            # Process stream_put
            elif "stream_put" in line:
                # Extract the argument id from the pattern "stream_put(%arg<digits>"
                m_put_id = re.search(r"stream_put\(%arg(\d+)", line)
                if m_put_id:
                    arg_id = int(m_put_id.group(1))
                else:
                    continue
                # Extract the put variable
                search_start = m_put_id.end()
                m_put_var = re.search(r"(%[^)]+)", line[search_start:])
                if m_put_var:
                    put_var = m_put_var.group(1)
                else:
                    continue
                stream_index = arg_id - len(inputs) - len(outputs)
                stream_name = streams[stream_index][0]
                ele_type = stream_ele_types[stream_name]
                current_indent = 6 + (len(line) - len(line.lstrip(" ")))
                with format_code(indent=current_indent):
                    # Acquire the FIFO
                    code += format_str(
                        f"%{stream_name} = aie.objectfifo.acquire @{stream_name}(Produce, 1) : !aie.objectfifosubview<{ele_type}>"
                    )
                    code += format_str(
                        f"%local_{stream_name} = aie.objectfifo.subview.access %{stream_name}[0] : !aie.objectfifosubview<{ele_type}> -> {ele_type}"
                    )
                    # Depending on the element type, either perform a memref.copy or a memref.store
                    if "x" in ele_type:
                        code += format_str(
                            f"memref.copy {put_var}, %local_{stream_name} : {ele_type} to {ele_type}"
                        )
                    else:
                        code += format_str(
                            f"memref.store {put_var}, %local_{stream_name}[] : {ele_type}"
                        )
                    # Release the FIFO
                    code += format_str(
                        f"aie.objectfifo.release @{stream_name}(Produce, 1)"
                    )
            else:
                code += format_str(line, strip=False)
    return code, func_str


def get_memref_type_str(ele_type, shape):
    return f"memref<{'x'.join(map(str, shape))}x{ele_type}>"


def calculate_tensor_access(shape, partition, device_mesh):
    """
    Calculate the size and stride for tensor access based on shape and partition method.
    Layout visualization tool:
    https://andreroesti.com/data-layout-viz/data_layout.html

    Parameters:
    -----------
    shape : tuple
        The shape of the tensor (1D or 2D)
    partition : str
        The partition method for each dimension:
        - 'S': Sharded (distributed across devices)
        - 'R': Replicated (copied to each device)
    device_mesh : tuple, optional
        The mesh of devices (default: (2, 2))

    Returns:
    --------
    tuple
        A tuple containing two lists: (size, stride)
    """
    # Handle 1D tensor case
    if len(shape) == 1:
        if partition == "S":
            # For 1D tensor with "S", shard across all devices
            total_devices = device_mesh[0]
            shard_size = shape[0] // total_devices
            size = [1, 1, total_devices, shard_size]
            stride = [0, 0, shard_size, 1]
        elif partition == "R":
            # For 1D tensor with "R", replicate across all devices
            total_devices = device_mesh[0]
            size = [1, total_devices, 1, shape[0]]
            stride = [0, 0, 0, 1]
        return size, stride

    # Handle 2D tensor case
    if len(shape) == 2:
        m, n = shape
        a, b = device_mesh

        if partition == "SS":
            # Both dimensions sharded
            size = [a, b, m // a, n // b]
            stride = [(m // a) * n, n // b, n, 1]

        elif partition == "SR":
            # First dim sharded across all devices, second replicated
            total_devices = a * b
            size = [1, total_devices, m // total_devices, n]
            stride = [0, (m // total_devices) * n, n, 1]

        elif partition == "RS":
            # First dim replicated, second sharded across second dim of mesh
            size = [1, b, m, n // b]
            stride = [(m * n) // (a * b), n // b, n, 1]

        elif partition == "RR":
            # Both dimensions replicated
            total_devices = a * b
            size = [1, 1, m, n]
            stride = [0, 0, n, 1]

        return size, stride
    raise ValueError(f"Unsupported shape {shape} or partition {partition}.")


def codegen_aie_mlir(
    mod,
    kernel_funcs,  # Changed from kernel_func to list of kernel functions
    kernel_buf_dicts,
    external_kernels,
    stream_info,
):
    """
    Generates MLIR-AIE code with MLIR module and extra information for multiple kernel functions

    Parameters
    ----------
    mod: allo._mlir.ir.Module
        The MLIR module built by allo.

    kernel_funcs: List[KernelFunction]
        A list of kernel function objects, each containing mapping, inputs, and outputs.

    kernel_buf_dicts: Dict[str, Dict[str, Tuple[str, List[int]]]]
        The kernel buffer dictionaries for each function in the module.
        The key is the function name, and the value is a dictionary mapping buffer names to their types and shapes.

    external_kernels: Dict[str, List[str]]
        The external kernels that will be injected into the module.
        The key is the name of the function, and the value is a list of names of the external kernels.

    stream_info: Dict[str, List[Tuple[str, str]]]
        The input and output stream of each kernel.
        The key is the name of the kernel, and the value is a list of tuples.
        The first element in the tuple is the name of the stream, the second element is either 'in' or 'out'.
    """

    # Helper function to create tile names for a kernel
    def get_tile_name(kernel_name, indices, map_1d):
        i, j = indices
        if map_1d:
            return f"%tile_comp_{kernel_name}_{j}_{i}"
        return f"%tile_comp_{kernel_name}_{i}_{j}"

    # Get normalized mapping for a kernel (ensure it's a 2D tuple)
    def get_normalized_mapping(mapping):
        return (mapping[0], 1) if len(mapping) == 1 else tuple(mapping)

    # Initialize code generation
    code = format_str("module {", indent=0)

    # Determine device based on maximum tensor arguments across all kernels
    max_dt_args = max(len(kf.dtensors[0]) for kf in kernel_funcs)
    mem_tile_size = 2 if max_dt_args > 2 else 1
    device = "npu1_2col" if max_dt_args > 2 else "npu1_1col"

    code += format_str(f"aie.device({device}) {{", indent=2)

    # Add external functions
    for func in mod.body.operations:
        if (
            isinstance(func, func_d.FuncOp)
            and "sym_visibility" in func.attributes
            and func.attributes["sym_visibility"].value == "private"
        ):
            code += format_str(str(func), indent=4)

    # Create shim and memory tiles
    code += format_str("%tile_shim = aie.tile(0, 0)")
    for mid in range(mem_tile_size):
        code += format_str(f"%tile_mem{mid} = aie.tile({mid}, 1)")

    # Get top function and all other functions
    top_func, all_funcs = get_public_funcs(mod)

    # Group functions by their kernel base names
    kernel_to_funcs = {}
    for func in all_funcs:
        func_name = func.attributes["sym_name"].value
        for kernel_func in kernel_funcs:
            if func_name.startswith(kernel_func.name):
                if kernel_func.name not in kernel_to_funcs:
                    kernel_to_funcs[kernel_func.name] = []
                kernel_to_funcs[kernel_func.name].append(func)
                break

    # Track vertical position for tile placement
    y_offset = 2

    # Storage for all kernel-specific data
    kernel_data = {}
    all_inputs = []
    all_outputs = []
    global_arg_id = 0  # For tracking inputs/outputs across all kernels

    # Create compute tiles and store kernel info
    for kernel_func in kernel_funcs:
        kernel_name = kernel_func.name
        mapping = kernel_func.mapping
        normalized_mapping = get_normalized_mapping(mapping)
        map_1d = len(mapping) == 1

        # Store kernel-specific data
        kernel_data[kernel_name] = {
            "y_offset": y_offset,
            "mapping": normalized_mapping,
            "map_1d": map_1d,
            "buf_name_dicts": [],
            "tile2fifo": defaultdict(list),
            "inputs": [],
            "outputs": [],
            "funcs": kernel_to_funcs.get(kernel_name, []),
            "func_strs": [],
        }

        # Create compute tiles for this kernel
        if map_1d:
            for j, i in np.ndindex((normalized_mapping[0], 1)):
                code += format_str(
                    f"%tile_comp_{kernel_name}_{j}_{i} = aie.tile({i}, {j + y_offset})"
                )
        else:
            for i, j in np.ndindex(normalized_mapping):
                code += format_str(
                    f"%tile_comp_{kernel_name}_{i}_{j} = aie.tile({i}, {j + y_offset})"
                )

        # Update y_offset for next kernel (use max value + spacing)
        y_offset += max(normalized_mapping) + 1

    # Create buffers and process function strings for each kernel
    for kernel_name, data in kernel_data.items():
        funcs = data["funcs"]
        buf_name_dicts = []

        for func in funcs:
            func_name = func.attributes["sym_name"].value

            if data["map_1d"]:
                # For 1D mapping, extract index from function name
                # Assuming format like "kernel_0"
                idx = int(func_name.split("_")[-1])
                tile_name = get_tile_name(kernel_name, (0, idx), True)
            else:
                # For 2D mapping, extract indices from function name
                # Assuming format like "kernel_0_1"
                parts = func_name.split("_")
                if len(parts) >= 3:  # At least kernel_i_j
                    i, j = int(parts[-2]), int(parts[-1])
                    tile_name = get_tile_name(kernel_name, (i, j), False)
                else:
                    # Fallback if naming convention doesn't match
                    tile_name = f"%tile_comp_{func_name}"

            # Create buffers for this function
            buf_dict = kernel_buf_dicts[func_name]
            buf_name_dict = {}

            for i, name in enumerate(buf_dict.keys()):
                new_name = f"{tile_name}_buf{i}"
                buf_name_dict[name] = new_name
                ele_type, shape = buf_dict[name]
                str_list = list(map(str, shape))
                str_list.append(ele_type)
                buf_type = f"memref<{'x'.join(str_list)}>"
                code += format_str(f"{new_name} = aie.buffer({tile_name}) : {buf_type}")

            buf_name_dicts.append(buf_name_dict)

        data["buf_name_dicts"] = buf_name_dicts

        # Update function strings
        func_strs = list(map(str, funcs))
        for func_id, func_str in enumerate(func_strs):
            buf_name_dict = buf_name_dicts[func_id]

            # Remove memref.alloc
            pattern_alloc = re.compile(r"^.*memref\.alloc.*\n?", re.MULTILINE)
            func_str = re.sub(pattern_alloc, "", func_str)

            # Replace new buffer name
            pattern_boundary = r"(?<![\w.]){old}(?![\w.])"
            for name, new_name in buf_name_dict.items():
                escaped_name = re.escape(name)
                pattern = pattern_boundary.format(old=escaped_name)
                func_str = re.sub(pattern, new_name, func_str)

            data["func_strs"].append(func_str)

    # Helper function to process tensors (inputs or outputs)
    def process_tensors(kernel_func, kernel_name, data, tensor_list, is_input):
        func_names = [func.attributes["sym_name"].value for func in data["funcs"]]
        result = []
        nonlocal global_arg_id

        offset = 0 if is_input else len(kernel_func.inputs)
        global_list = all_inputs if is_input else all_outputs

        for i, (dtype, shape) in enumerate(tensor_list, start=offset):
            dtensors = [
                (
                    func_name,
                    get_memref_type_str(dt[i].dtype, dt[i].local_shape),
                    dt[i].local_shape,
                )
                for func_name, dt in zip(func_names, kernel_func.dtensors)
            ]
            tensor_data = (get_memref_type_str(dtype, shape), shape, dtensors)
            result.append(tensor_data)
            global_list.append(
                (f"{kernel_name}_arg{global_arg_id}", global_arg_id, tensor_data)
            )
            global_arg_id += 1

        return result

    # Process inputs and outputs for each kernel
    for kernel_func in kernel_funcs:
        kernel_name = kernel_func.name
        data = kernel_data[kernel_name]

        # Process inputs and outputs
        data["inputs"] = process_tensors(
            kernel_func, kernel_name, data, kernel_func.inputs, True
        )
        data["outputs"] = process_tensors(
            kernel_func, kernel_name, data, kernel_func.outputs, False
        )

    # Create object FIFOs for each kernel
    stream_ele_types = {}

    for kernel_name, data in kernel_data.items():
        # Helper function to create input/output object FIFOs for a kernel
        def create_io_object_fifos(args, prefix, kernel_name):
            nonlocal code
            for arg_id, (
                arg_name,
                _,
                (global_memref_type, _, dtensors),
            ) in enumerate(args):
                # Create object FIFO from shim to memory tile
                if prefix == "in":
                    code += format_str(
                        f"aie.objectfifo @in_sh_{arg_name}(%tile_shim, {{%tile_mem{arg_id % mem_tile_size}}}, 2 : i32) : !aie.objectfifo<{global_memref_type}>"
                    )
                else:
                    code += format_str(
                        f"aie.objectfifo @out_sh_{arg_name}(%tile_mem0, {{%tile_shim}}, 2 : i32) : !aie.objectfifo<{global_memref_type}>"
                    )

                # Handle distribution to compute tiles based on placement
                mem_stride = []
                placement = kernel_func.dtensors[0][
                    arg_id if prefix == "in" else arg_id + len(data["inputs"])
                ].placement
                fifos = []

                # Define iteration ranges based on placement pattern
                normalized_mapping = data["mapping"]

                if placement == "S":
                    outer_range, inner_range = normalized_mapping[0], 1
                    format_name = lambda i, j: f"{prefix}_{arg_name}_{i}"
                    get_tiles = lambda i, j: [(i, 0)]
                elif placement == "R":
                    outer_range, inner_range = 1, 1
                    format_name = lambda i, j: f"{prefix}_{arg_name}_R"
                    get_tiles = lambda i, j: [
                        (ki, 0) for ki in range(normalized_mapping[0])
                    ]
                elif placement == "SR":
                    outer_range, inner_range = normalized_mapping[0], 1
                    format_name = lambda i, j: f"{prefix}_{arg_name}_{i}_R"
                    get_tiles = lambda i, j: [
                        (i, k) for k in range(normalized_mapping[1])
                    ]
                elif placement == "RS":
                    outer_range, inner_range = 1, normalized_mapping[1]
                    format_name = lambda i, j: f"{prefix}_{arg_name}_R_{j}"
                    get_tiles = lambda i, j: [
                        (k, j) for k in range(normalized_mapping[0])
                    ]
                elif placement == "SS":
                    outer_range, inner_range = (
                        normalized_mapping[0],
                        normalized_mapping[1],
                    )
                    format_name = lambda i, j: f"{prefix}_{arg_name}_{i}_{j}"
                    get_tiles = lambda i, j: [(i, j)]
                else:  # RR
                    outer_range, inner_range = 1, 1
                    format_name = lambda i, j: f"{prefix}_{arg_name}_R_R"
                    get_tiles = lambda i, j: [
                        (ki, kj)
                        for ki in range(normalized_mapping[0])
                        for kj in range(normalized_mapping[1])
                    ]

                # Generate code for object FIFOs
                for i in range(outer_range):
                    for j in range(inner_range):
                        name = format_name(i, j)
                        tiles = get_tiles(i, j)

                        for tile in tiles:
                            data["tile2fifo"][tile].append(name)

                        fifos.append(name)
                        if map_1d:
                            tile_str = ", ".join(
                                [
                                    # pylint: disable=undefined-loop-variable
                                    get_tile_name(kernel_name, (kj, ki), data["map_1d"])
                                    for ki, kj in tiles
                                ]
                            )
                        else:
                            tile_str = ", ".join(
                                [
                                    # pylint: disable=undefined-loop-variable
                                    get_tile_name(kernel_name, (ki, kj), data["map_1d"])
                                    for ki, kj in tiles
                                ]
                            )

                        if prefix == "in":
                            code += format_str(
                                f"aie.objectfifo @{name}(%tile_mem{arg_id % mem_tile_size}, {{{tile_str}}}, 2 : i32) : !aie.objectfifo<{dtensors[i][1]}>"
                            )
                        else:
                            code += format_str(
                                f"aie.objectfifo @{name}({tile_str}, {{%tile_mem0}}, 2 : i32) : !aie.objectfifo<{dtensors[i][1]}>"
                            )

                        prev_stride = mem_stride[-1] if mem_stride else 0
                        mem_stride.append(prev_stride + np.prod(dtensors[i][2]))

                mem_stride.insert(0, 0)
                mem_stride = mem_stride[:-1]  # remove the last stride
                mem_str = ", ".join([f"@{name}" for name in fifos])

                # Link object FIFOs
                if prefix == "in":
                    code += format_str(
                        f"aie.objectfifo.link [@in_sh_{arg_name}] -> [{mem_str}]([] {mem_stride})"
                    )
                else:
                    code += format_str(
                        f"aie.objectfifo.link [{mem_str}] -> [@out_sh_{arg_name}]({mem_stride} [])"
                    )

        # Create input/output object FIFOs for this kernel
        kernel_func = next(kf for kf in kernel_funcs if kf.name == kernel_name)
        create_io_object_fifos(
            [
                (f"{kernel_name}_arg{i}", global_id, input_data)
                for i, (_, global_id, input_data) in enumerate(
                    [inp for inp in all_inputs if inp[0].startswith(kernel_name)]
                )
            ],
            "in",
            kernel_name,
        )
        create_io_object_fifos(
            [
                (f"{kernel_name}_arg{i + len(all_inputs)}", global_id, output_data)
                for i, (_, global_id, output_data) in enumerate(
                    [out for out in all_outputs if out[0].startswith(kernel_name)]
                )
            ],
            "out",
            kernel_name,
        )

    # Create stream object FIFOs from top_func
    stream_in_out = get_stream_in_out(stream_info)
    for op in top_func.entry_block.operations:
        if isinstance(op, allo_d.StreamConstructOp):
            stream_name = op.attributes["name"].value
            if stream_name in stream_in_out:
                in_out = stream_in_out[stream_name]
                stream_type_str = str(op.results.types[0])
                start = stream_type_str.find("<") + 1
                end = stream_type_str.rfind(">")
                type_str, depth_str = stream_type_str[start:end].split(",")
                type_str = type_str.strip()
                if not type_str.startswith("memref"):
                    type_str = f"memref<{type_str}>"
                depth = int(depth_str.strip())

                # Determine the correct tiles for the stream
                producer_kernel, producer_id = in_out[0].split("_", 1)
                consumer_kernel, consumer_id = in_out[1].split("_", 1)

                # Create the stream object FIFO between the two kernels
                code += format_str(
                    f"aie.objectfifo @{stream_name}({get_tile_name(producer_kernel, (0, producer_id), data['map_1d'])}, {{{get_tile_name(consumer_kernel, (0, consumer_id), data["map_1d"])}}}, {depth} : i32) : !aie.objectfifo<{type_str}>"
                )
                stream_ele_types[stream_name] = type_str

    # Create core computation for each kernel function
    for kernel_name, data in kernel_data.items():
        funcs = data["funcs"]
        func_strs = data["func_strs"]

        for func_id, func in enumerate(funcs):
            func_str = func_strs[func_id]
            func_name = func.attributes["sym_name"].value

            # Determine tile coordinates
            if data["map_1d"]:
                fid_tuple = (int(func_name.split("_")[-1]), 0)
                core_name = f"tile_comp_{func_name}_0"
            else:
                parts = func_name.split("_")
                fid_tuple = (int(parts[-2]), int(parts[-1]))
                core_name = f"tile_comp_{func_name}"

            # Get streams for this function
            streams = stream_info[func_name]

            # Generate core computation
            code += format_str(
                f"%core_0_{func_id + 2}_{core_name} = aie.core(%{core_name}) {{"
            )
            with format_code(indent=6):
                code += format_str("%global_c0 = arith.constant 0 : index")
                code += format_str("%global_c1 = arith.constant 1 : index")
                code += format_str(
                    "%c9223372036854775807 = arith.constant 9223372036854775807 : index"
                )
                code += format_str(
                    "scf.for %arg0 = %global_c0 to %c9223372036854775807 step %global_c1 {"
                )
                with format_code(indent=8):
                    # Get the specific kernel function
                    kernel_func = next(
                        kf for kf in kernel_funcs if kf.name == kernel_name
                    )

                    # Helper function to process FIFOs (acquire or release)
                    def process_fifos(is_input, is_acquire):
                        nonlocal code, func_str
                        tensors = data["inputs"] if is_input else data["outputs"]
                        operation = "Consume" if is_input else "Produce"
                        arg_offset = 0 if is_input else len(data["inputs"])

                        for arg_id, _ in enumerate(tensors):
                            # Calculate fifo index and get fifo name
                            fifo_idx = (
                                arg_id if is_input else arg_id + len(data["inputs"])
                            )
                            fifo_name = data["tile2fifo"][fid_tuple][fifo_idx]

                            if is_acquire:
                                # Acquire FIFO and access subview
                                dtype = tensors[arg_id][2][func_id][1]
                                var_prefix = "fifo" if is_input else "fifo_out"
                                local_prefix = "local" if is_input else "local_out"

                                code += format_str(
                                    f"%{var_prefix}{arg_id} = aie.objectfifo.acquire @{fifo_name}({operation}, 1) : !aie.objectfifosubview<{dtype}>"
                                )
                                code += format_str(
                                    f"%{local_prefix}{arg_id} = aie.objectfifo.subview.access %{var_prefix}{arg_id}[0] : !aie.objectfifosubview<{dtype}> -> {dtype}"
                                )

                                # Replace argument in function string
                                arg_name = f"%arg{arg_id + arg_offset}"
                                local_name = f"%{local_prefix}{arg_id}"
                                func_str = func_str.replace(arg_name, local_name)
                            else:
                                # Release FIFO
                                code += format_str(
                                    f"aie.objectfifo.release @{fifo_name}({operation}, 1)"
                                )

                    # Acquire input and output FIFOs
                    process_fifos(is_input=True, is_acquire=True)
                    process_fifos(is_input=False, is_acquire=True)

                    # Fix call operations
                    while " call @" in func_str:
                        func_str = func_str.replace(" call @", " func.call @")

                    # Process stream operations
                    stream_code, func_str = process_stream_operations(
                        func_str,
                        streams,
                        data["inputs"],
                        data["outputs"],
                        stream_ele_types,
                    )
                    code += stream_code

                    # Release input and output FIFOs
                    process_fifos(is_input=True, is_acquire=False)
                    process_fifos(is_input=False, is_acquire=False)

                code += format_str("}")
                code += format_str("aie.end")

            # Add linking information if needed
            code += "    }"
            if len(external_kernels.get(func_name, [])) > 0:
                code += ' {link_with = "external.o"}\n'
            else:
                code += "\n"

    # Create runtime sequence with all inputs and outputs
    in_args = []
    out_args = []

    for i, (name, global_id, (global_memref_type, _, _)) in enumerate(all_inputs):
        in_args.append(f"%arg{global_id}: {global_memref_type}")

    for i, (name, global_id, (global_memref_type, _, _)) in enumerate(all_outputs):
        out_args.append(f"%arg{global_id}: {global_memref_type}")

    # Create dma transfer from off-chip mem to shim tile
    code += format_str(
        f"aiex.runtime_sequence({', '.join(in_args)}, {', '.join(out_args)}) {{"
    )

    with format_code(indent=6):
        # Helper function to process DMA operations for inputs and outputs
        def process_dma_operations(is_input):
            nonlocal code
            tensor_list = all_inputs if is_input else all_outputs
            prefix = "in" if is_input else "out"

            # Process each kernel's tensors for DMA transfers
            for kernel_name, data in kernel_data.items():
                kernel_func = next(kf for kf in kernel_funcs if kf.name == kernel_name)

                # Filter tensors belonging to this kernel
                filtered_tensors = [
                    t for t in tensor_list if t[0].startswith(kernel_name)
                ]

                for arg_id, (
                    name,
                    global_id,
                    (global_memref_type, global_shape, _),
                ) in enumerate(filtered_tensors):
                    # Calculate tensor index in dtensors - for inputs it's arg_id, for outputs it's offset by inputs length
                    tensor_idx = arg_id if is_input else arg_id + len(data["inputs"])
                    dtensor = kernel_func.dtensors[0][tensor_idx]

                    # Calculate access pattern
                    size, stride = calculate_tensor_access(
                        global_shape, dtensor.placement, data["mapping"]
                    )

                    # Build DMA attributes - inputs need issue_token, outputs don't
                    if is_input:
                        dma_attrs = f"id = {global_id + 1} : i64, issue_token = true, metadata = @{prefix}_sh_{name}"
                    else:
                        dma_attrs = (
                            f"id = {global_id} : i64, metadata = @{prefix}_sh_{name}"
                        )

                    # Create DMA operation
                    code += format_str(
                        f"aiex.npu.dma_memcpy_nd(0, 0, %arg{global_id}[0, 0, 0, 0]{size}{stride}) {{{dma_attrs}}} : {global_memref_type}"
                    )

        # Process DMA operations for inputs and outputs
        process_dma_operations(is_input=True)
        process_dma_operations(is_input=False)

        # Helper function for DMA wait operations
        def process_dma_wait(is_input):
            nonlocal code
            tensor_list = all_inputs if is_input else all_outputs
            prefix = "in" if is_input else "out"

            for name, _, _ in tensor_list:
                code += format_str(
                    f"aiex.npu.dma_wait {{symbol = @{prefix}_sh_{name}}}"
                )

        # Wait for all DMAs to complete
        process_dma_wait(is_input=True)
        process_dma_wait(is_input=False)

    code += format_str("}")
    code += format_str("}", indent=2)
    code += "}"

    return code


def update_func_op_arg_types(func_op, inputs, outputs, dtensors, enable_tensor):
    inputs_outputs = inputs + outputs
    new_input_types = []
    with func_op.context, Location.unknown():
        for arg_id, (_, dtensor) in enumerate(zip(inputs_outputs, dtensors)):
            elem_ty = get_element_type_from_str(dtensor.dtype, func_op.context)
            shape = dtensor.local_shape
            memref_ty = (
                RankedTensorType.get(shape, elem_ty)
                if enable_tensor
                else MemRefType.get(shape, elem_ty)
            )
            new_input_types.append(memref_ty)
        # Add remaining stream arguments
        arg_id = len(inputs_outputs)
        while arg_id < len(func_op.function_type.value.inputs):
            new_input_types.append(func_op.function_type.value.inputs[arg_id])
            arg_id += 1
        new_func_type = FunctionType.get(
            new_input_types, func_op.function_type.value.results, func_op.context
        )
        new_type = TypeAttr.get(new_func_type, func_op.context)
        func_op.operation.attributes["function_type"] = new_type
        entry_block = func_op.entry_block
        for arg_id, block_arg in enumerate(entry_block.arguments):
            if arg_id < len(new_input_types):
                block_arg.set_type(new_input_types[arg_id])
        op_to_remove = []
        for op in func_op.regions[0].blocks[0].operations:
            if op.operation.name == "memref.subview":
                op.result.replace_all_uses_with(op.operands[0])
                op_to_remove.append(op)
        for op in op_to_remove:
            op.erase()


def lower_tensor_to_memref(mod, enable_tensor):
    passes = (
        [
            # "linalg-generalize-named-ops",
            # "linalg-fuse-elementwise-ops",
            "one-shot-bufferize{bufferize-function-boundaries function-boundary-type-conversion=identity-layout-map}",
            "func.func(convert-linalg-to-affine-loops),lower-affine",
        ]
        if enable_tensor
        else [
            # "linalg-generalize-named-ops",
            # "linalg-fuse-elementwise-ops",
            "func.func(convert-linalg-to-affine-loops),lower-affine",
        ]
    )
    pipeline = f'builtin.module({",".join(passes)})'
    with mod.context:
        mlir_pass_manager.parse(pipeline).run(mod.operation)


def record_local_buffer(mod):
    kernel_buf_dicts = {}
    _, funcs = get_public_funcs(mod)

    def traverse_operations(operations, buf_dict):
        for op in operations:
            if op.operation.name == "memref.alloc":
                name = op.result.get_name()
                dtype, shape = get_dtype_and_shape_from_type(op.result.type)
                buf_dict[name] = (dtype, shape)

            # Recursively traverse into all regions and blocks of the operation
            for region in op.regions:
                for block in region.blocks:
                    traverse_operations(block.operations, buf_dict)

    for func in funcs:
        func_name = func.attributes["sym_name"].value
        buf_dict = {}

        # Start traversal from the top-level operations of the function
        for block in func.regions[0].blocks:
            traverse_operations(block.operations, buf_dict)

        kernel_buf_dicts[func_name] = buf_dict

    return kernel_buf_dicts


def parse_mlir_to_kernel_function(module):
    """
    Parse an MLIR module to extract kernel function information.
    Returns multiple kernel functions if the module contains different types.

    Parameters:
    -----------
    module: The MLIR module

    Returns:
    --------
    list[KernelFunction]: A list of KernelFunction objects with extracted information
    """
    # Find the top function and kernel functions
    top_func = None
    kernel_funcs = []

    for op in module.body.operations:
        if isinstance(op, func_d.FuncOp):
            func_name = op.name.value

            if func_name == "top":
                top_func = op
            elif "_" in func_name:
                kernel_funcs.append(op)

    if not top_func:
        raise ValueError("No top function found in the MLIR module")

    if not kernel_funcs:
        raise ValueError("No kernel functions found in the MLIR module")

    # Group kernel functions by their base names (e.g., "producer", "consumer")
    kernel_groups = {}
    for kernel in kernel_funcs:
        func_name = kernel.name.value
        base_name = func_name.split("_")[0]

        if base_name not in kernel_groups:
            kernel_groups[base_name] = []

        kernel_groups[base_name].append(kernel)

    # Process each group to create KernelFunction objects
    kernel_function_objects = []

    # pylint: disable=too-many-nested-blocks
    for base_name, group_kernels in kernel_groups.items():
        # Find maximum indices for each dimension
        mapping_indices = []
        for op in group_kernels:
            func_name = op.name.value
            if func_name.startswith(base_name):
                # Extract indices after the base_name and underscore
                suffix = func_name[len(base_name) + 1 :]
                if suffix:  # Check if there's a suffix
                    parts = suffix.split("_")
                    try:
                        indices = [
                            int(idx) for idx in parts if idx
                        ]  # Filter out empty parts

                        # Expand mapping_indices if needed
                        while len(mapping_indices) < len(indices):
                            mapping_indices.append(-1)

                        # Update maximum values
                        for i, idx in enumerate(indices):
                            mapping_indices[i] = max(mapping_indices[i], idx)
                    except ValueError:
                        # Skip if parts aren't all integers
                        continue

        # Calculate dimensions (max index + 1 in each dimension)
        mapping = (
            [idx + 1 for idx in mapping_indices] if mapping_indices else [1]
        )  # Default to [1] if no indices

        # Use a representative kernel function to analyze access patterns
        representative_kernel = group_kernels[0]
        input_indices, output_indices = analyze_read_write_patterns(
            representative_kernel
        )

        # Parse input and output types based on the access patterns
        inputs = []
        outputs = []

        # Get the top function arguments
        for i, arg in enumerate(top_func.arguments):
            # Get the type of the argument
            arg_type = arg.type

            # Check if it's a memref type
            if MemRefType.isinstance(arg_type):
                memref_type = MemRefType(arg_type)

                # Get shape and element type
                shape = memref_type.shape
                element_type = memref_type.element_type

                # Extract dtype (e.g., i32)
                dtype = str(element_type)

                # Determine if this is an input or output based on access patterns
                # Using proportional mapping from kernel args to top function args
                kernel_arg_count = len(representative_kernel.arguments)

                # Simple proportional mapping - may need refinement for complex cases
                kernel_arg_index = min(i, kernel_arg_count - 1)

                if kernel_arg_index in input_indices:
                    inputs.append((dtype, list(shape)))
                if kernel_arg_index in output_indices:
                    outputs.append((dtype, list(shape)))

        # Create and append the KernelFunction
        kernel_func = KernelFunction(base_name, mapping, inputs, outputs)

        # Set the MLIR functions
        kernel_func.set_mlir_funcs(group_kernels)

        kernel_function_objects.append(kernel_func)

    return kernel_function_objects


def create_dtensors_from_kernel(kernel_func, func_op):
    """
    Analyze the MLIR function and create DTensor objects for each argument,
    inferring placement based on subview operations.

    Parameters:
    -----------
        kernel_func (KernelFunction): The kernel function containing mapping information
        func_op: The MLIR function operation to analyze

    Returns:
    --------
        list: List of DTensor objects for each argument
    """
    # Extract information from the kernel function
    mapping = kernel_func.mapping  # e.g., [2, 2] for a 2x2 grid

    # Get rank from function name (e.g., "gemm_0_1" -> rank 1)
    func_name = str(func_op.attributes["sym_name"]).strip('"')
    coords = func_name.split("_")[1:]  # Extract coordinates, e.g., ["0", "1"]
    rank = 0
    for i, coord in enumerate(coords):
        rank += int(coord) * (mapping[i + 1 :] + [1]).pop(0)

    # Calculate rank coordinates
    rank_coords = []
    rank_copy = rank
    for dim_size in reversed(mapping):
        rank_coords.insert(0, rank_copy % dim_size)
        rank_copy //= dim_size

    # Create DTensor objects for each argument
    dtensors = []
    for i, arg in enumerate(func_op.arguments):
        # Get memref shape and type from argument
        arg_type = arg.type
        shape = []
        dtype = None

        # Extract the shape and type from the argument
        arg_type_str = str(arg_type)
        if arg_type_str.startswith("memref<"):
            # Extract the part between memref< and >
            inner_type = arg_type_str[len("memref<") : -1]
            # Parse shape and data type
            parts = inner_type.split("x")

            # Extract dimensions from each part
            for j in range(len(parts) - 1):
                shape.append(int(parts[j]))

            # Last part contains dimension and data type
            last_part = parts[-1]
            dim_end = 0
            while dim_end < len(last_part) and last_part[dim_end].isdigit():
                dim_end += 1

            if dim_end > 0:
                shape.append(int(last_part[:dim_end]))

            # Extract data type
            dtype = last_part[dim_end:]

        elif arg_type_str.startswith("!allo.stream<"):
            continue

        # Create DTensor object
        dtensor = DTensor(rank, mapping, shape, dtype)
        dtensors.append(dtensor)

    # Analyze subview operations to determine the actual tensor partitioning
    subviews = {}  # Maps arg index to (offset, size) tuples

    # Find all subview operations in the function
    def collect_subviews(block):
        for op in block.operations:
            op_name = str(op.operation.name)
            if op_name == "memref.subview":
                if len(op.operands) >= 1:
                    source = op.operands[0]

                    # If source is a block argument, record its subview information
                    if BlockArgument.isinstance(source):
                        arg_num = BlockArgument(source).arg_number

                        # Extract offset and size from operation string
                        op_str = str(op)

                        # Parse offset and size parameters from the subview operation
                        offset = []
                        size = []

                        subview_pattern = (
                            r"memref\.subview\s+%[^[]+\[([^\]]+)\]\s+\[([^\]]+)\]"
                        )
                        match = re.search(subview_pattern, op_str)

                        if match:
                            # Extract offset
                            offset_str = match.group(1)
                            offset = [int(o.strip()) for o in offset_str.split(",")]

                            # Extract size
                            size_str = match.group(2)
                            size = [int(s.strip()) for s in size_str.split(",")]

                            # Store the subview information
                            subviews[arg_num] = (offset, size)

            # Recursively process nested regions
            for region in op.regions:
                for inner_block in region.blocks:
                    collect_subviews(inner_block)

    # Collect subview information
    for block in func_op.body.blocks:
        collect_subviews(block)

    # Infer placement based on subview analysis and rank coordinates
    for i, dtensor in enumerate(dtensors):
        if i in subviews:
            offset, size = subviews[i]
            dtensor.set_subview(offset, size)

            # Determine placement based on how the tensor is partitioned
            placement = ""
            for dim, _ in enumerate(dtensor.shape):
                # If this dimension is fully utilized (size equals global shape), it's Replicated
                # Otherwise, it's Sharded
                dim_size = size[dim] if dim < len(size) else dtensor.shape[dim]
                global_dim_size = dtensor.shape[dim]

                if dim_size == global_dim_size:
                    placement += "R"  # Replicated dimension
                else:
                    placement += "S"  # Sharded dimension

            dtensor.set_placement(placement)

    return dtensors


class AIEModule:
    def __init__(
        self,
        module,
        top_func_name,
        project,
        kernel_mappings,
        enable_tensor,
        stream_info,
    ):
        self.module = module
        self.top_func_name = top_func_name
        self.project = project
        self.module = module
        self.kernel_mappings = kernel_mappings
        self.enable_tensor = enable_tensor
        self.stream_info = stream_info
        self.kernel_funcs = []

    def build(self):
        assert "MLIR_AIE_INSTALL_DIR" in os.environ, "Please set MLIR_AIE_INSTALL_DIR"
        assert "PEANO_INSTALL_DIR" in os.environ, "Please set PEANO_INSTALL_DIR"
        os.makedirs(os.path.join(self.project, "build"), exist_ok=True)
        self.kernel_funcs = parse_mlir_to_kernel_function(self.module)
        for kernel_func in self.kernel_funcs:
            for i, func_op in enumerate(kernel_func.funcs):
                kernel_func.set_dtensors(
                    create_dtensors_from_kernel(kernel_func, func_op), i
                )
                update_func_op_arg_types(
                    func_op,
                    kernel_func.inputs,
                    kernel_func.outputs,
                    kernel_func.dtensors[i],
                    self.enable_tensor,
                )
        external_kernels = inject_aie_kernels(self.module)
        with open(
            os.path.join(self.project, "original.mlir"), "w", encoding="utf-8"
        ) as f:
            f.write(str(self.module))
        lower_tensor_to_memref(self.module, self.enable_tensor)
        kernel_buf_dicts = record_local_buffer(self.module)
        code = codegen_aie_mlir(
            self.module,
            self.kernel_funcs,
            kernel_buf_dicts,
            external_kernels,
            self.stream_info,
        )
        with open(os.path.join(self.project, "top.mlir"), "w", encoding="utf-8") as f:
            f.write(code)
        # compile external kernels
        kernel_code, generated_kernels = codegen_external_kernels(external_kernels)
        if len(generated_kernels) > 0:
            with open(
                os.path.join(self.project, "external.cc"), "w", encoding="utf-8"
            ) as f:
                f.write(kernel_code)
            path = os.path.join(os.path.dirname(__file__), "aie_kernels")
            cmd = f"cd {self.project} && $PEANO_INSTALL_DIR/bin/clang++ -O2 -v -std=c++20 --target=aie2-none-unknown-elf -Wno-parentheses -Wno-attributes -Wno-macro-redefined -DNDEBUG -I $(dirname $(which aie-opt))/../include -I $MLIR_AIE_INSTALL_DIR/../aie_kernels/aie2 -c external.cc -o external.o"
            process = subprocess.Popen(cmd, shell=True)
            process.wait()
            if process.returncode != 0:
                raise RuntimeError("Failed to compile external kernels.")
        # build mlir-aie
        cmd = f"cd {self.project} && PYTHONPATH=$MLIR_AIE_INSTALL_DIR/python aiecc.py --aie-generate-cdo --aie-generate-npu --no-compile-host --no-xchesscc --no-xbridge --xclbin-name=build/final.xclbin --npu-insts-name=insts.txt top.mlir"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to compile the MLIR-AIE code")
        path = os.path.dirname(__file__)
        path = os.path.join(path, "../harness/aie")
        os.system(f"cp -r {path}/* {self.project}")
        host_code = codegen_host(
            self.kernel_funcs[0].inputs, self.kernel_funcs[-1].outputs
        )
        with open(os.path.join(self.project, "test.cpp"), "w", encoding="utf-8") as f:
            f.write(host_code)
        cmd = f"cd {self.project}/build && cmake .. -DTARGET_NAME=top -DMLIR_AIE_DIR=$MLIR_AIE_INSTALL_DIR/.. && cmake --build . --config Release"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to build AIE project.")
        return self

    def __call__(self, *args):
        # suppose the last argument is output
        for i, arg in enumerate(args[:-1]):
            with open(
                os.path.join(self.project, f"input{i}.data"), "w", encoding="utf-8"
            ) as f:
                f.write("\n".join([str(i) for i in arg.flatten()]))
        cmd = f"cd {self.project} && ./build/top -x build/final.xclbin -i insts.txt -k MLIR_AIE"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to execute AIE code.")
        # TODO: need to complete multiple outputs rules
        result = read_tensor_from_file(
            list(self.kernel_funcs[-1].outputs)[-1][0],
            args[-1].shape,
            f"{self.project}/output.data",
        )
        args[-1][:] = result
