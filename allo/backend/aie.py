# Copyright Allo authors. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
# mlir-aie commit: 8329b6
# pylint: disable=consider-using-with, bad-builtin, no-name-in-module, too-many-branches, too-many-nested-blocks

import os
import subprocess
import re
import copy
import numpy as np
from .._mlir.ir import (
    IntegerAttr,
    IntegerType,
    DenseI64ArrayAttr,
    RankedTensorType,
    MemRefType,
    FunctionType,
    TypeAttr,
    Location,
    StridedLayoutAttr,
    InsertionPoint,
    FlatSymbolRefAttr,
    StringAttr,
)
from .._mlir.dialects import (
    memref as memref_d,
    allo as allo_d,
    func as func_d,
)
from .._mlir.passmanager import PassManager as mlir_pass_manager

from .vitis import read_tensor_from_file
from ..utils import (
    get_func_inputs_outputs,
    get_dtype_and_shape_from_type,
    get_element_type_from_str,
)
from .utils import format_str, format_code
from .vitis import ctype_map


host_header = """
//=============================================================================
// Auto generated by Allo
//=============================================================================

#include <boost/program_options.hpp>
#include <cstdint>
#include <fstream>
#include <iostream>
#include <sstream>
#include <string>
#include <vector>

#include "xrt/xrt_bo.h"
#include "xrt/xrt_device.h"
#include "xrt/xrt_kernel.h"

#include "test_utils.h"

namespace po = boost::program_options;

int main(int argc, const char *argv[]) {

  // ------------------------------------------------------
  // Parse program arguments
  // ------------------------------------------------------
  po::options_description desc("Allowed options");
  po::variables_map vm;
  test_utils::add_default_options(desc);

  test_utils::parse_options(argc, argv, desc, vm);
  int verbosity = vm["verbosity"].as<int>();
  int do_verify = vm["verify"].as<bool>();
  int n_iterations = vm["iters"].as<int>();
  int n_warmup_iterations = vm["warmup"].as<int>();
  int trace_size = vm["trace_sz"].as<int>();

  // Load instruction sequence
  std::vector<uint32_t> instr_v =
      test_utils::load_instr_sequence(vm["instr"].as<std::string>());
  if (verbosity >= 1)
    std::cout << "Sequence instr count: " << instr_v.size() << "\\n";

  // ------------------------------------------------------
  // Get device, load the xclbin & kernel and register them
  // ------------------------------------------------------
  // Get a device handle
  unsigned int device_index = 0;
  auto device = xrt::device(device_index);

  // Load the xclbin
  if (verbosity >= 1)
    std::cout << "Loading xclbin: " << vm["xclbin"].as<std::string>() << "\\n";
  auto xclbin = xrt::xclbin(vm["xclbin"].as<std::string>());

  // Load the kernel
  if (verbosity >= 1)
    std::cout << "Kernel opcode: " << vm["kernel"].as<std::string>() << "\\n";
  std::string Node = vm["kernel"].as<std::string>();

  // Get the kernel from the xclbin
  auto xkernels = xclbin.get_kernels();
  auto xkernel = *std::find_if(xkernels.begin(), xkernels.end(),
                               [Node, verbosity](xrt::xclbin::kernel &k) {
                                 auto name = k.get_name();
                                 if (verbosity >= 1) {
                                   std::cout << "Name: " << name << std::endl;
                                 }
                                 return name.rfind(Node, 0) == 0;
                               });
  auto kernelName = xkernel.get_name();

  // Register xclbin
  if (verbosity >= 1)
    std::cout << "Registering xclbin: " << vm["xclbin"].as<std::string>()
              << "\\n";
  device.register_xclbin(xclbin);

  // Get a hardware context
  if (verbosity >= 1)
    std::cout << "Getting hardware context.\\n";
  xrt::hw_context context(device, xclbin.get_uuid());

  // Get a kernel handle
  if (verbosity >= 1)
    std::cout << "Getting handle to kernel:" << kernelName << "\\n";
  auto kernel = xrt::kernel(context, kernelName);

  // ------------------------------------------------------
  // Initialize input/ output buffer sizes and sync them
  // ------------------------------------------------------
  auto bo_instr = xrt::bo(device, instr_v.size() * sizeof(int),
                          XCL_BO_FLAGS_CACHEABLE, kernel.group_id(1));
  void *bufInstr = bo_instr.map<void *>();
  memcpy(bufInstr, instr_v.data(), instr_v.size() * sizeof(int));

  std::ofstream ofile("output.data");
  if (!ofile.is_open()) {
      std::cerr << "Error: Could not open output file.\\n";
      return 1;
  }

"""

file_close_str = """  ofile.close();
  if (verbosity >= 1)
    std::cout << "Array has been written to output.data.\\n";
  return 0;
}
"""


def codegen_host(kernel_inputs, kernel_outputs):
    code = host_header
    inputs = [input for sublist in kernel_inputs.values() for input in sublist]
    outputs = [output for sublist in kernel_outputs.values() for output in sublist]
    with format_code(indent=2):
        # write input data
        for i, (dtype, shape) in enumerate(inputs):
            dtype = ctype_map[dtype]
            code += format_str(f'std::ifstream ifile{i}("input{i}.data");')
            code += format_str(f"if (!ifile{i}.is_open()) {{")
            code += format_str(
                '  std::cerr << "Error: Could not open input file.\\n";', strip=False
            )
            code += format_str("  return 1;", strip=False)
            code += format_str("}")
            size = np.prod(shape)
            code += format_str(
                f"auto bo_in{i} = xrt::bo(device, {size} * sizeof({dtype}),"
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({i + 3}));"
                )
            code += format_str(f"{dtype} *bufIn{i} = bo_in{i}.map<{dtype} *>();")
            code += format_str(f"std::vector<{dtype}> srcVec{i};")
            code += format_str(f"for (int i = 0; i < {size}; i++) {{")
            with format_code(indent=4):
                code += format_str(f"{dtype} num;")
                code += format_str(f"ifile{i} >> num;")
                code += format_str(f"srcVec{i}.push_back(num);")
            code += format_str("}")
            code += format_str(
                f"memcpy(bufIn{i}, srcVec{i}.data(), (srcVec{i}.size() * sizeof({dtype})));"
            )
        for i, (dtype, shape) in enumerate(outputs):
            dtype = ctype_map[dtype]
            out_size = np.prod(shape)
            code += format_str(
                f"\nauto bo_out{i} = xrt::bo(device, {out_size} * sizeof({dtype}),",
                strip=False,
            )
            with format_code(indent=24):
                code += format_str(
                    f"XRT_BO_FLAGS_HOST_ONLY, kernel.group_id({len(inputs) + 2 + i}));"
                )
        code += format_str("if (verbosity >= 1)")
        code += format_str(
            '  std::cout << "Writing data into buffer objects.\\n";', strip=False
        )
        code += format_str("\nbo_instr.sync(XCL_BO_SYNC_BO_TO_DEVICE);", strip=False)
        for i in range(len(inputs)):
            code += format_str(f"bo_in{i}.sync(XCL_BO_SYNC_BO_TO_DEVICE);")
        # run kernels
        code += format_str("if (verbosity >= 1)")
        code += format_str('  std::cout << "Running Kernel.\\n";', strip=False)
        code += format_str(
            "\nauto start = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str("unsigned int opcode = 3;", strip=False)
        inbufs = ", ".join([f"bo_in{i}" for i in range(len(inputs))])
        outbufs = ", ".join([f"bo_out{i}" for i in range(len(outputs))])
        code += format_str("// gid: (opcode, instr, instr_size, ...)")
        code += format_str(
            f"auto run = kernel(opcode, bo_instr, instr_v.size(), {inbufs}, {outbufs});"
        )
        code += format_str("run.wait();")
        code += format_str(
            "\nauto end = std::chrono::high_resolution_clock::now();", strip=False
        )
        code += format_str(
            "float npu_time = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();"
        )
        code += format_str(
            'std::cout << "NPU execution time: " << npu_time << "us\\n";'
        )
        # get results
        for i, (dtype, shape) in enumerate(outputs):
            dtype = ctype_map[dtype]
            out_size = np.prod(shape)
            code += format_str(
                f"\nbo_out{i}.sync(XCL_BO_SYNC_BO_FROM_DEVICE);", strip=False
            )
            code += format_str(f"{dtype} *bufOut{i} = bo_out{i}.map<{dtype} *>();")
            code += format_str(f"for (uint32_t i = 0; i < {out_size}; i++) {{")
            code += format_str(f'  ofile << *(bufOut{i} + i) << "\\n";', strip=False)
            code += format_str("}")
        code += format_str("\n// Close files", strip=False)
        for i in range(len(inputs)):
            code += format_str(f"ifile{i}.close();")
        code += file_close_str
    return code


def get_stream_in_out(stream_info):
    """
    Computes the mapping of FIFO names to their producer (output) and consumer (input) cores.

    Parameters
    ----------
    stream_info (Dict[int, List[Tuple[str, str]]]):
        A dictionary where the key is a core ID (int), and the value is a list of tuples.
        Each tuple contains:
        - fifo_name (str): The name of the FIFO.
        - direction (str): The direction of data flow, either "in" (consumer) or "out" (producer).

    Returns
    -------
    Dict[str, Tuple[Optional[int], Optional[int]]]]:
        A dictionary where the key is a FIFO name, and the value is a tuple:
        - The first element is the producer function and the second element is the consumer function.
    """
    stream_in_out = {}
    for core, fifos in stream_info.items():
        for fifo_name, direction in fifos:
            if fifo_name not in stream_in_out:
                stream_in_out[fifo_name] = (None, None)
            if direction == "in":
                stream_in_out[fifo_name] = (stream_in_out[fifo_name][0], core)
            elif direction == "out":
                stream_in_out[fifo_name] = (core, stream_in_out[fifo_name][1])
    return stream_in_out


def get_public_funcs(mod):
    funcs = []
    top_func = None
    for func in mod.body.operations:
        if isinstance(func, func_d.FuncOp) and (
            "sym_visibility" not in func.attributes
            or func.attributes["sym_visibility"].value != "private"
        ):
            if func.attributes["sym_name"].value == "top":
                top_func = func
            else:
                funcs.append(func)
    return top_func, funcs


def inject_aie_kernels(mod):
    external_kernels = {}
    injected_kernels = set()
    with mod.context, Location.unknown():
        for func in mod.body.operations:
            external_kernels[func.attributes["sym_name"].value] = []
            for block in func.regions[0].blocks:
                for op in block.operations:
                    if (
                        op.operation.name in {"linalg.add", "linalg.mul"}
                        and len(MemRefType(op.inputs[0].type).shape) == 1
                    ):
                        op_name = op.operation.name.split(".")[1]
                        # Inject AIE kernel
                        func_type = func_d.FunctionType.get(
                            [op.inputs[0].type, op.inputs[1].type, op.outputs[0].type],
                            [],
                        )
                        dtype = str(op.inputs[0].type.element_type)
                        shape = MemRefType(op.inputs[0].type).shape
                        func_d.CallOp(
                            [],
                            FlatSymbolRefAttr.get(f"eltwise_{op_name}_{dtype}_vector"),
                            [op.inputs[0], op.inputs[1], op.outputs[0]],
                            ip=InsertionPoint(op),
                        )
                        op.erase()
                        external_kernels[func.attributes["sym_name"].value].append(
                            (op_name, dtype, shape)
                        )
                        if f"eltwise_{op_name}_{dtype}_vector" in injected_kernels:
                            continue
                        injected_kernels.add(f"eltwise_{op_name}_{dtype}_vector")
                        kernel = func_d.FuncOp(
                            f"eltwise_{op_name}_{dtype}_vector",
                            func_type,
                            ip=InsertionPoint(func),
                        )
                        kernel.attributes["sym_visibility"] = StringAttr.get("private")
    return external_kernels


def codegen_external_kernels(external_kernels):
    code = ""
    code += "// External kernels generated by Allo\n\n"
    code += "#include <stdint.h>\n"
    code += "#include <stdio.h>\n"
    code += "#include <stdlib.h>\n"
    code += "#include <type_traits>\n"
    code += "#include <aie_api/aie.hpp>\n\n"
    code += '#include "add.h"\n'
    code += '#include "mul.h"\n\n'
    code += 'extern "C" {\n\n'
    generated_kernels = set()
    for _, kernel_lst in external_kernels.items():
        for kernel, dtype, shape in kernel_lst:
            if kernel in generated_kernels:
                continue
            ctype = ctype_map[dtype]
            if "bfloat" in ctype:
                ctype = "bfloat16"
            code += f"void eltwise_{kernel}_{dtype}_vector"
            code += f"({ctype} *A_in, {ctype} *B_in, {ctype} *C_out)"
            code += " {\n"
            code += f"  eltwise_v{kernel}<{ctype}, {ctype}, {np.prod(shape)}>(A_in, B_in, C_out);\n"
            code += "}\n\n"
            generated_kernels.add(kernel)
    code += '} // extern "C"\n'
    return code, generated_kernels


def process_stream_operations(func_str, streams, inputs, outputs, stream_ele_types):
    """
    Process a function string by replacing stream_get and stream_put calls with
    corresponding formatted FIFO code.

    Parameters
    ----------
        func_str (str): The input function string to be processed.
        streams (list): List of streams (each stream is a tuple/list where the first element is the stream name).
        inputs (list): List of input identifiers.
        outputs (list): List of output identifiers.
        stream_ele_types (dict): Dictionary mapping stream names to their element types.

    Returns
    -------
        str: The resulting formatted code.
    """
    code = ""
    with format_code(indent=6):
        lines = func_str.splitlines()
        for i in range(1, len(lines) - 2):
            line = lines[i]
            # Process stream_get
            if "stream_get" in line:
                # extract the argument id from the pattern "stream_get(%arg<digits>"
                m_get = re.search(r"stream_get\(%arg(\d+)", line)
                if m_get:
                    arg_id = int(m_get.group(1))
                else:
                    continue
                # Extract the return variable
                return_var = line.split("=")[0].strip()
                stream_index = arg_id - len(inputs) - len(outputs)
                stream_name = streams[stream_index][0]
                current_indent = 6 + (len(line) - len(line.lstrip(" ")))
                with format_code(indent=current_indent):
                    # Acquire the FIFO
                    ele_type = stream_ele_types[stream_name]
                    code += format_str(
                        f"%{stream_name} = aie.objectfifo.acquire @{stream_name}(Consume, 1) : !aie.objectfifosubview<{ele_type}>"
                    )
                    code += format_str(
                        f"%local_{stream_name} = aie.objectfifo.subview.access %{stream_name}[0] : !aie.objectfifosubview<{ele_type}> -> {ele_type}"
                    )
                    # Load the value into a local variable if the element type is scalar
                    if "x" not in ele_type:
                        code += format_str(
                            f"{return_var} = memref.load %local_{stream_name}[] : {ele_type}"
                        )
                    else:
                        # Otherwise, replace the return variable with the local stream variable
                        func_str = func_str.replace(return_var, f"%local_{stream_name}")
                        lines = func_str.splitlines()
                    # Release the FIFO
                    code += format_str(
                        f"aie.objectfifo.release @{stream_name}(Consume, 1)"
                    )
            # Process stream_put
            elif "stream_put" in line:
                # Extract the argument id from the pattern "stream_put(%arg<digits>"
                m_put_id = re.search(r"stream_put\(%arg(\d+)", line)
                if m_put_id:
                    arg_id = int(m_put_id.group(1))
                else:
                    continue
                # Extract the put variable
                search_start = m_put_id.end()
                m_put_var = re.search(r"(%[^)]+)", line[search_start:])
                if m_put_var:
                    put_var = m_put_var.group(1)
                else:
                    continue
                stream_index = arg_id - len(inputs) - len(outputs)
                stream_name = streams[stream_index][0]
                ele_type = stream_ele_types[stream_name]
                current_indent = 6 + (len(line) - len(line.lstrip(" ")))
                with format_code(indent=current_indent):
                    # Acquire the FIFO
                    code += format_str(
                        f"%{stream_name} = aie.objectfifo.acquire @{stream_name}(Produce, 1) : !aie.objectfifosubview<{ele_type}>"
                    )
                    code += format_str(
                        f"%local_{stream_name} = aie.objectfifo.subview.access %{stream_name}[0] : !aie.objectfifosubview<{ele_type}> -> {ele_type}"
                    )
                    # Depending on the element type, either perform a memref.copy or a memref.store
                    if "x" in ele_type:
                        code += format_str(
                            f"memref.copy {put_var}, %local_{stream_name} : {ele_type} to {ele_type}"
                        )
                    else:
                        code += format_str(
                            f"memref.store {put_var}, %local_{stream_name}[] : {ele_type}"
                        )
                    # Release the FIFO
                    code += format_str(
                        f"aie.objectfifo.release @{stream_name}(Produce, 1)"
                    )
            else:
                code += format_str(line, strip=False)
    return code, func_str


def codegen_aie_mlir(
    mod,
    kernel_mappings,
    kernel_index_ranges,
    orig_kernel_inputs,
    orig_kernel_outputs,
    kernel_func_arg_sizes,
    kernel_func_buf_dicts,
    kernel_dist_allocs,
    external_kernels,
    stream_info,
):
    """
    Generates MLIR-AIE code with MLIR module and extra information

    Parameters
    ----------
    mod: allo._mlir.ir.Module
        The MLIR module built by allo.

    kernel_mappings: Dict[str, List[int]]
        The mapping of each kernel in the module.
        The key is the name of the kernel, and the value is a list of integers representing the mapping of the kernel.

    kernel_index_ranges: Dict[str, Tuple[int, int]]
        The index range of each kernel in the module.
        The key is the name of the kernel, and the value is a tuple of integers representing the start and end index of the kernel.

    orig_kernel_inputs: Dict[str, List[Tuple[str, List[int]]]]
        The original types of the input argument of the kernels.
        The key is the name of the kernel, and the value is a list of tuples.
        Each tuple in the list stands for the type of each argument. e.g. ('i32', [16, 16]).
        For current version, we assume all function in the module have the same input arguments.

    orig_kernel_outputs: Dict[str, List[Tuple[str, List[int]]]]
        The original types of the output argument of the kernels.
        The key is the name of the kernel, and the value is a list of tuples.
        Each tuple in the list stands for the type of each argument. e.g. ('i32', [16, 16]).
        For current version, we assume all function in the module have the same output arguments.

    kernel_func_arg_sizes: Dict[str, List[List[List[int]]]]
        The actual size of each argument that each function in each kernel will be using.
        The key is the name of the kernel, and the value is a list of lists.
        The first dim in each list stands for each function, the second dim stands for each argument, the last dim stands for shape.

    kernel_func_buf_dicts: Dict[str, List[Dict[str, Tuple[str, List[int]]]]]
        The local buffer each function creates.
        Each function in the list is a dictionary, where the key is the name of the buffer, and the value is the type of
        the element. e.g. ('i32', [16, 16]).

    kernel_dist_allocs: Dict[str, List[bool]]
        The allocation strategy for each argument in each kernel.
        More info can be find in function check_usage_intersection.

    external_kernels: Dict[str, List[str]]
        The external kernels that will be injected into the module.
        The key is the name of the function, and the value is a list of names of the external kernels.

    stream_info: Dict[str, List[Tuple[str, str]]]
        The input and output stream of each kernel.
        The key is the name of the kernel, and the value is a list of tuples.
        The first element in the tuple is the name of the stream, the second element is either 'in' or 'out'.
    """
    kernel_inputs = copy.deepcopy(orig_kernel_inputs)
    kernel_outputs = copy.deepcopy(orig_kernel_outputs)
    code = format_str("module {", indent=0)
    kernel_inputs_outputs_val = list(kernel_inputs.values()) + list(
        kernel_outputs.values()
    )
    num_tensors = sum(len(v) for v in kernel_inputs_outputs_val)
    mem_tile_size = 2 if num_tensors > 2 else 1
    device = "npu1_2col" if num_tensors > 2 else "npu1_1col"
    code += format_str(f"aie.device({device}) {{", indent=2)
    # external functions
    for func in mod.body.operations:
        if (
            isinstance(func, func_d.FuncOp)
            and "sym_visibility" in func.attributes
            and func.attributes["sym_visibility"].value == "private"
        ):
            code += format_str(str(func), indent=4)
    # create tiles
    code += format_str("%tile_shim = aie.tile(0, 0)")
    for mid in range(mem_tile_size):
        code += format_str(f"%tile_mem{mid} = aie.tile({mid}, 1)")
    # number of function declaration except top
    top_func, funcs = get_public_funcs(mod)
    buf_name_dicts = []
    # create compute tiles and buffers
    for kernel_name, (start, end) in kernel_index_ranges.items():
        mapping = kernel_mappings[kernel_name]
        func_buf_dicts = kernel_func_buf_dicts[kernel_name]
        for fid in range(start, end):
            func_name = funcs[fid].attributes["sym_name"].value
            tile_name = f"%tile_comp_{func_name}"
            code += format_str(f"{tile_name} = aie.tile(0, {fid + 2})")
            buf_dict = func_buf_dicts[fid - start]
            buf_name_dict = {}
            for i, name in enumerate(buf_dict.keys()):
                new_name = f"{tile_name}_buf{i}"
                buf_name_dict[name] = new_name
                ele_type, shape = buf_dict[name]
                str_list = list(map(str, shape))
                str_list.append(ele_type)
                buf_type = f"memref<{'x'.join(map(str, str_list))}>"
                code += format_str(f"{new_name} = aie.buffer({tile_name}) : {buf_type}")
            buf_name_dicts.append(buf_name_dict)
    # update module and args
    for kernel_name in kernel_index_ranges.keys():
        inputs = kernel_inputs[kernel_name]
        outputs = kernel_outputs[kernel_name]
        func_arg_sizes = kernel_func_arg_sizes[kernel_name]
        for arg_id, (ele_type, orig_shape) in enumerate(inputs):
            orig_ele_type = f"memref<{'x'.join(map(str, orig_shape))}x{ele_type}>"
            # assume all functions of the same kernel have the same input args
            shape = func_arg_sizes[0][arg_id]
            ele_type = f"memref<{'x'.join(map(str, shape))}x{ele_type}>"
            inputs[arg_id] = (ele_type, orig_ele_type, shape, orig_shape)
        for i, (ele_type, orig_shape) in enumerate(outputs):
            arg_id = len(inputs) + i
            orig_ele_type = f"memref<{'x'.join(map(str, orig_shape))}x{ele_type}>"
            # assume all functions of the same kernel have the same input args
            shape = func_arg_sizes[0][arg_id]
            ele_type = f"memref<{'x'.join(map(str, shape))}x{ele_type}>"
            outputs[i] = (ele_type, orig_ele_type, shape, orig_shape)
        kernel_inputs[kernel_name] = inputs
        kernel_outputs[kernel_name] = outputs
    func_strs = list(map(str, funcs))
    # update buffers
    for fid, func_str in enumerate(func_strs):
        buf_name_dict = buf_name_dicts[fid]
        # remove memref.alloc
        pattern_alloc = re.compile(r"^.*memref\.alloc.*\n?", re.MULTILINE)
        func_str = re.sub(pattern_alloc, "", func_str)
        # replace new buffer name
        pattern_boundary = r"(?<![\w.]){old}(?![\w.])"
        for name, new_name in buf_name_dict.items():
            escaped_name = re.escape(name)
            pattern = pattern_boundary.format(old=escaped_name)
            func_str = re.sub(pattern, new_name, func_str)
        func_strs[fid] = func_str
    # create input object fifos
    # connect each argument to a separate mem tile
    for kernel_name, (start, end) in kernel_index_ranges.items():
        inputs = kernel_inputs[kernel_name]
        outputs = kernel_outputs[kernel_name]
        func_arg_sizes = kernel_func_arg_sizes[kernel_name]
        mapping = kernel_mappings[kernel_name]
        dist_allocs = kernel_dist_allocs[kernel_name]
        for arg_id, (in_type, orig_in_type, shape, orig_shape) in enumerate(inputs):
            if dist_allocs[arg_id]:
                # depth=2 means double buffer
                code += format_str(
                    f"aie.objectfifo @in_sh_{kernel_name}_arg{arg_id}(%tile_shim, {{%tile_mem{arg_id}}}, 2 : i32) : !aie.objectfifo<{orig_in_type}>"
                )
                for fid in range(start, end):
                    func_name = funcs[fid].attributes["sym_name"].value
                    code += format_str(
                        f"aie.objectfifo @in_{func_name}_arg{arg_id}(%tile_mem{arg_id}, {{%tile_comp_{func_name}}}, 2 : i32) : !aie.objectfifo<{in_type}>"
                    )
                in_mem_str = ", ".join(
                    [
                        f"@in_{funcs[fid].attributes["sym_name"].value}_arg{arg_id}"
                        for fid in range(start, end)
                    ]
                )
                pe_size = end - start
                shape_prod = np.prod(shape)
                in_mem_stride = list(range(0, shape_prod * pe_size, shape_prod))
                # (src_offsets, dst_offsets)
                code += format_str(
                    f"aie.objectfifo.link [@in_sh_{kernel_name}_arg{arg_id}] -> [{in_mem_str}]([] {in_mem_stride})"
                )
            else:
                code += format_str(
                    f"aie.objectfifo @in_sh_{kernel_name}_arg{arg_id}(%tile_shim, {{%tile_mem{arg_id}}}, 2 : i32) : !aie.objectfifo<{orig_in_type}>"
                )
                in_tile_str = ", ".join(
                    [
                        f"%tile_comp_{funcs[fid].attributes["sym_name"].value}"
                        for fid in range(start, end)
                    ]
                )
                code += format_str(
                    f"aie.objectfifo @in_{kernel_name}_arg{arg_id}(%tile_mem{arg_id}, {{{in_tile_str}}}, 2 : i32) : !aie.objectfifo<{orig_in_type}>"
                )
                code += format_str(
                    f"aie.objectfifo.link [@in_sh_{kernel_name}_arg{arg_id}] -> [@in_{kernel_name}_arg{arg_id}]([] [])"
                )
        kernel_dist_allocs[kernel_name] = dist_allocs
    # create output object fifos
    for kernel_name, (start, end) in kernel_index_ranges.items():
        dist_allocs = kernel_dist_allocs[kernel_name]
        inputs = kernel_inputs[kernel_name]
        outputs = kernel_outputs[kernel_name]
        mapping = kernel_mappings[kernel_name]
        for i, (out_type, orig_out_type, out_shape, orig_out_shape) in enumerate(
            outputs
        ):
            arg_id = len(inputs) + i
            if dist_allocs[arg_id]:
                # output uses tile_mem0
                for fid in range(start, end):
                    func_name = funcs[fid].attributes["sym_name"].value
                    code += format_str(
                        f"aie.objectfifo @out_{func_name}(%tile_comp_{func_name}, {{%tile_mem0}}, 2 : i32) : !aie.objectfifo<{out_type}>"
                    )
                code += format_str(
                    f"aie.objectfifo @out_sh_{kernel_name}(%tile_mem0, {{%tile_shim}}, 2 : i32) : !aie.objectfifo<{orig_out_type}>"
                )
                out_mem_str = ", ".join(
                    [
                        f"@out_{funcs[fid].attributes["sym_name"].value}"
                        for fid in range(start, end)
                    ]
                )
                pe_size = end - start
                shape_prod = np.prod(out_shape)
                out_mem_stride = list(range(0, shape_prod * pe_size, shape_prod))
                code += format_str(
                    f"aie.objectfifo.link [{out_mem_str}] -> [@out_sh_{kernel_name}]({out_mem_stride} [])"
                )
            else:
                out_tile_str = ", ".join(
                    [
                        f"%tile_comp_{funcs[fid].attributes["sym_name"].value}"
                        for fid in range(start, end)
                    ]
                )
                code += format_str(
                    f"aie.objectfifo @out_sh_{kernel_name}(%tile_mem0, {{%tile_shim}}, 2 : i32) : !aie.objectfifo<{orig_out_type}>"
                )
                # TODO: can't specify multi-input of a fifo
                code += format_str(
                    f"aie.objectfifo @out_{kernel_name}({{{out_tile_str}}}, {{%tile_mem0}}, 2 : i32) : !aie.objectfifo<{orig_out_type}>"
                )
                code += format_str(
                    f"aie.objectfifo.link [@out_{kernel_name}] -> [@out_sh_{kernel_name}]([] [])"
                )
    # create other object fifos from top_func
    stream_in_out = get_stream_in_out(stream_info)
    stream_ele_types = {}
    for op in top_func.entry_block.operations:
        if isinstance(op, allo_d.StreamConstructOp):
            stream_name = op.attributes["name"].value
            if stream_name in stream_in_out:
                in_out = stream_in_out[stream_name]
                stream_type_str = str(op.results.types[0])
                start = stream_type_str.find("<") + 1
                end = stream_type_str.rfind(">")
                type_str, depth_str = stream_type_str[start:end].split(",")
                type_str = type_str.strip()
                if not type_str.startswith("memref"):
                    type_str = f"memref<{type_str}>"
                depth = int(depth_str.strip())
                code += format_str(
                    f"aie.objectfifo @{stream_name}(%tile_comp_{in_out[0]}, {{%tile_comp_{in_out[1]}}}, {depth} : i32) : !aie.objectfifo<{type_str}>"
                )
                stream_ele_types[stream_name] = type_str
    # create core computation
    in_args = []
    out_args = []
    arg_index = 0
    for kernel_name, (start, end) in kernel_index_ranges.items():
        mapping = kernel_mappings[kernel_name]
        inputs = kernel_inputs[kernel_name]
        outputs = kernel_outputs[kernel_name]
        dist_allocs = kernel_dist_allocs[kernel_name]
        for fid in range(start, end):
            func_str = func_strs[fid]
            func_name = funcs[fid].attributes["sym_name"].value
            streams = stream_info[func_name]
            code += format_str(
                f"%core_0_{fid + 2} = aie.core(%tile_comp_{func_name}) {{"
            )
            with format_code(indent=6):
                code += format_str("%global_c0 = arith.constant 0 : index")
                code += format_str("%global_c1 = arith.constant 1 : index")
                code += format_str(
                    "%c9223372036854775807 = arith.constant 9223372036854775807 : index"
                )
                code += format_str(
                    "scf.for %arg0 = %global_c0 to %c9223372036854775807 step %global_c1 {"
                )
                with format_code(indent=8):
                    # Acquire input fifos
                    for arg_id, (in_type, orig_in_type, _, _) in enumerate(inputs):
                        dtype = in_type if dist_allocs[arg_id] else orig_in_type
                        code += format_str(
                            f"%fifo{arg_id} = aie.objectfifo.acquire @in_{func_name if dist_allocs[arg_id] else kernel_name}_arg{arg_id}(Consume, 1) : !aie.objectfifosubview<{dtype}>"
                        )
                        code += format_str(
                            f"%local{arg_id} = aie.objectfifo.subview.access %fifo{arg_id}[0] : !aie.objectfifosubview<{dtype}> -> {dtype}"
                        )
                        func_str = func_str.replace(f"%arg{arg_id}", f"%local{arg_id}")
                    # Acquire output fifos
                    for i, (out_type, orig_out_type, _, _) in enumerate(outputs):
                        arg_id = len(inputs) + i
                        dtype = out_type if dist_allocs[arg_id] else orig_out_type
                        code += format_str(
                            f"%fifo_out = aie.objectfifo.acquire @out_{func_name if dist_allocs[arg_id] else kernel_name}(Produce, 1) : !aie.objectfifosubview<{dtype}>"
                        )
                        code += format_str(
                            f"%local_out = aie.objectfifo.subview.access %fifo_out[0] : !aie.objectfifosubview<{dtype}> -> {dtype}"
                        )
                        func_str = func_str.replace(f"%arg{arg_id}", "%local_out")
                    while " call @" in func_str:
                        func_str = func_str.replace(" call @", " func.call @")
                    # Main computation
                    stream_code, func_str = process_stream_operations(
                        func_str, streams, inputs, outputs, stream_ele_types
                    )
                    code += stream_code
                    # Release input fifos
                    for arg_id in range(len(inputs)):
                        code += format_str(
                            f"aie.objectfifo.release @in_{func_name if dist_allocs[arg_id] else kernel_name}_arg{arg_id}(Consume, 1)"
                        )
                    # Release output fifos
                    for i in range(len(outputs)):
                        arg_id = len(inputs) + i
                        code += format_str(
                            f"aie.objectfifo.release @out_{func_name if dist_allocs[arg_id] else kernel_name}(Produce, 1)"
                        )
                code += format_str("}")
                code += format_str("aie.end")
            code += "    }"
            if len(external_kernels[f"{func_name}"]) > 0:
                code += ' {link_with = "external.o"}\n'
            else:
                code += "\n"
        for _, orig_in_type, _, _ in inputs:
            in_args.append(f"%arg{arg_index}: {orig_in_type}")
            arg_index += 1
        for _, orig_out_type, _, _ in outputs:
            out_args.append(f"%arg{arg_index}: {orig_out_type}")
            arg_index += 1
    code += format_str(
        f"aiex.runtime_sequence({",".join(in_args)}, {",".join(out_args)}) {{"
    )
    with format_code(indent=6):
        arg_index = 0
        for kernel_name, (start, end) in kernel_index_ranges.items():
            inputs = kernel_inputs[kernel_name]
            outputs = kernel_outputs[kernel_name]
            dist_allocs = kernel_dist_allocs[kernel_name]
            mapping = kernel_mappings[kernel_name]
            for arg_id, (_, orig_in_type, _, orig_shape) in enumerate(inputs):
                # (x, y, memref[offset][size][stride])
                # issue_token: MM2S-false, S2MM-true
                if len(orig_shape) == 1:
                    size_n_stride = f"[1, 1, 1, {orig_shape[0]}][0, 0, 0, 1]"
                elif len(mapping) == 2 and len(orig_shape) == 2 and dist_allocs[arg_id]:
                    # now only support 2D mapping and 2D tensor
                    size_n_stride = f"[{mapping[0]}, {mapping[1]}, {orig_shape[0] // mapping[0]}, {orig_shape[1] // mapping[1]}][{orig_shape[0] // mapping[0] * orig_shape[1]}, {orig_shape[1] // mapping[1]}, {orig_shape[1]}, 1]"
                else:
                    size_n_stride = f"[1, 1, {orig_shape[0]}, {orig_shape[1]}][0, 0, {orig_shape[1]}, 1]"
                code += format_str(
                    f"aiex.npu.dma_memcpy_nd(0, 0, %arg{arg_index}[0, 0, 0, 0]{size_n_stride}) {{id = {arg_index + 1} : i64, issue_token = true, metadata = @in_sh_{kernel_name}_arg{arg_id}}} : {orig_in_type}"
                )
                arg_index += 1
            for i, (_, orig_out_type, _, orig_out_shape) in enumerate(outputs):
                arg_id = len(inputs) + i
                if len(orig_out_shape) == 1:
                    out_size_n_stride = f"[1, 1, 1, {orig_out_shape[0]}][0, 0, 0, 1]"
                elif (
                    len(mapping) == 2
                    and len(orig_out_shape) == 2
                    and dist_allocs[arg_id]
                ):
                    # now only support 2D mapping and 2D tensor
                    out_size_n_stride = f"[{mapping[0]}, {mapping[1]}, {orig_out_shape[0] // mapping[0]}, {orig_out_shape[1] // mapping[1]}][{orig_out_shape[0] // mapping[0] * orig_out_shape[1]}, {orig_out_shape[1] // mapping[1]}, {orig_out_shape[1]}, 1]"
                else:
                    out_size_n_stride = f"[1, 1, {orig_out_shape[0]}, {orig_out_shape[1]}][0, 0, {orig_out_shape[1]}, 1]"
                code += format_str(
                    f"aiex.npu.dma_memcpy_nd(0, 0, %arg{arg_index}[0, 0, 0, 0]{out_size_n_stride}) {{id = 0 : i64, metadata = @out_sh_{kernel_name}}} : {orig_out_type}"
                )
                arg_index += 1
            for arg_id in range(len(inputs)):
                code += format_str(
                    f"aiex.npu.dma_wait {{symbol = @in_sh_{kernel_name}_arg{arg_id}}}"
                )
            for i in range(len(outputs)):
                arg_id = len(inputs) + i
                code += format_str(
                    f"aiex.npu.dma_wait {{symbol = @out_sh_{kernel_name}}}"
                )
    code += format_str("}")
    code += format_str("}", indent=2)
    code += "}"
    return code


def get_kernel_index_ranges(mod, kernel_mappings):
    """
    Get the index range of each kernel in the module

    Parameters
    ----------
    mod: allo._mlir.ir.Module
        The MLIR module built by allo.

    kernel_mappings: Dict[str, List[int]]
        The mapping of each kernel in the module.
        The key is the name of the kernel, and the value is a list of integers representing the mapping of the kernel.

    Returns:
    -------
    kernel_index_ranges: Dict[str, Tuple[int, int]]
        The index range of each kernel in the module.
        The key is the name of the kernel, and the value is a tuple of integers representing the start and end index of the kernel.
    """
    kernel_names = list(kernel_mappings.keys())
    kernel_index_ranges = {}
    start = end = 0
    k_id = 0
    _, funcs = get_public_funcs(mod)
    for func in funcs:
        if isinstance(func, func_d.FuncOp):
            while not func.attributes["sym_name"].value.startswith(kernel_names[k_id]):
                kernel_index_ranges[kernel_names[k_id]] = (start, end)
                k_id += 1
                start = end
            end += 1
    if k_id < len(kernel_names):
        kernel_index_ranges[kernel_names[k_id]] = (start, end)
        k_id += 1
    while k_id < len(kernel_names):
        kernel_index_ranges[kernel_names[k_id]] = (end, end)
        k_id += 1
    return kernel_index_ranges


def check_usage_intersection(func_arg_sizes, func_arg_lower_bounds, orig_shapes):
    """
    Check if there is a non-empty intersection among all functions' usage regions for each parameter.

    Parameters:
    ----------
    func_arg_sizes: list
        Each element represents a function's usage sizes for all arguments.
        For example, [[8, 16], [16, 8], [8, 8]] means the function has three parameters,
        each being a 2D tensor with usage sizes 8x16, 16x8, and 8x8 respectively.
    func_arg_lower_bounds: list
        Has the same structure as func_arg_sizes, representing the lower bounds
        of the usage regions for each parameter for each function.
    orig_shapes: list
        Each element represents the original shape of the corresponding parameter,
        e.g., [[16, 16], [16, 16], [16, 16]]. It is assumed that all provided usage regions
        are within the bounds of the original shape.

    Returns:
    -------
    dist_allocs: list
        dist_allocs define the allocation strategy for each argument. Currently, there are only two options: True or False.
        - If True: The memory tile divides the memory of the argument and distributes it among all compute tiles using
        `aie.objectfifo.link`. This is only feasible if the total memory consumed by all compute tiles does not exceed
        the original memory allocated to the argument.
        Example: If there are 3 compute tiles, each consuming 1/3 of matrix A, this strategy can be applied.
        - If False: The memory tile assigns the entire memory of the argument to each compute tile.
    """
    num_funcs = len(func_arg_sizes)
    num_params = len(orig_shapes)
    dist_allocs = []
    for param in range(num_params):
        boxes = []
        has_intersection = False
        dims = len(orig_shapes[param])
        for f in range(num_funcs):
            box = []
            for d in range(dims):
                lower = func_arg_lower_bounds[f][param][d]
                upper = lower + func_arg_sizes[f][param][d]
                box.append((lower, upper))
            for b in boxes:
                if all(
                    max(b[d][0], box[d][0]) < min(b[d][1], box[d][1])
                    for d in range(dims)
                ):
                    has_intersection = True
                    break
            if has_intersection:
                break
            boxes.append(box)
        dist_allocs.append(not has_intersection)
    return dist_allocs


def reindex_tensor_access(mod, kernel_index_ranges, kernel_inputs, kernel_outputs):
    ctx = mod.context
    _, funcs = get_public_funcs(mod)
    # kernel->func -> arg -> dim
    kernel_func_arg_lower_bounds = {}
    kernel_func_arg_sizes = {}
    for kernel_name, (start, end) in kernel_index_ranges.items():
        func_arg_lower_bounds = []
        func_arg_sizes = []
        for fid in range(start, end):
            func = funcs[fid]
            entry_block = func.regions[0].blocks[0]
            args = entry_block.arguments
            arg_types = args.types
            # TODO: might need some specialization for scalar input arg
            lower_bounds = [
                [float("inf") for _ in range(len(arg_type.shape))]
                for arg_type in arg_types
                if "!allo.stream" not in str(arg_type)
            ]
            sizes = [
                [0 for _ in range(len(arg_type.shape))]
                for arg_type in arg_types
                if "!allo.stream" not in str(arg_type)
            ]
            # Support nested block traversal
            ops_stack = []
            for block in func.regions[0].blocks:
                ops_stack.extend(block.operations)
            while ops_stack:
                op = ops_stack.pop()
                if op.operation.name in {
                    "tensor.extract_slice",
                    "tensor.insert_slice",
                    "memref.subview",
                }:
                    operand_idx = 1 if op.operation.name == "tensor.insert_slice" else 0
                    if op.operands[operand_idx] not in args:
                        continue
                    arg_id = list(args).index(op.operands[operand_idx])
                    static_offsets = op.attributes["static_offsets"]
                    static_sizes = op.attributes["static_sizes"]
                    for i, (offset, size) in enumerate(
                        zip(static_offsets, static_sizes)
                    ):
                        lower_bounds[arg_id][i] = min(lower_bounds[arg_id][i], offset)
                        sizes[arg_id][i] = max(sizes[arg_id][i], size)
                    for region in op.regions:
                        for block in region.blocks:
                            ops_stack.extend(block.operations)
            for i, lower_bound in enumerate(lower_bounds):
                # Arguments never used with slice
                if lower_bound[0] == float("inf"):
                    # If ever used, assume using entire tensor
                    if len(list(args[i].uses)) > 0:
                        lower_bounds[i] = [0] * len(lower_bound)
                        sizes[i] = args[i].type.shape
                    else:
                        lower_bounds[i] = [0] * len(lower_bound)
                        sizes[i] = [0] * len(lower_bound)

            func_arg_lower_bounds.append(lower_bounds)
            func_arg_sizes.append(sizes)
        kernel_func_arg_lower_bounds[kernel_name] = func_arg_lower_bounds
        kernel_func_arg_sizes[kernel_name] = func_arg_sizes

    kernel_dist_allocs = {}
    for kernel_name in kernel_index_ranges.keys():
        inputs_outputs = kernel_inputs[kernel_name] + kernel_outputs[kernel_name]
        orig_shapes = [arg[-1] for arg in inputs_outputs]
        kernel_dist_allocs[kernel_name] = check_usage_intersection(
            func_arg_sizes, func_arg_lower_bounds, orig_shapes
        )

    for kernel_name, (start, end) in kernel_index_ranges.items():
        func_arg_lower_bounds = kernel_func_arg_lower_bounds[kernel_name]
        func_arg_sizes = kernel_func_arg_sizes[kernel_name]
        dist_allocs = kernel_dist_allocs[kernel_name]
        for fid in range(start, end):
            func = funcs[fid]
            entry_block = func.regions[0].blocks[0]
            args = entry_block.arguments
            lower_bounds = func_arg_lower_bounds[fid - start]
            sizes = func_arg_sizes[fid - start]
            # Support nested block traversal
            ops_stack = []
            for block in func.regions[0].blocks:
                ops_stack.extend(block.operations)
            while ops_stack:
                op = ops_stack.pop()
                if op.operation.name in {
                    "tensor.extract_slice",
                    "tensor.insert_slice",
                    "memref.subview",
                }:
                    operand_idx = 1 if op.operation.name == "tensor.insert_slice" else 0
                    if op.operands[operand_idx] not in args:
                        continue
                    arg_id = list(args).index(op.operands[operand_idx])
                    static_offsets = op.attributes["static_offsets"]
                    new_offsets = []
                    for i, offset in enumerate(static_offsets):
                        new_offset = (
                            offset - lower_bounds[arg_id][i]
                            if dist_allocs[arg_id]
                            else offset
                        )
                        new_offset_attr = IntegerAttr.get(
                            IntegerType.get_signless(64, ctx), new_offset
                        )
                        new_offsets.append(new_offset_attr)
                    op.attributes["static_offsets"] = DenseI64ArrayAttr.get(
                        new_offsets, ctx
                    )
                    for region in op.regions:
                        for block in region.blocks:
                            ops_stack.extend(block.operations)
    return kernel_func_arg_lower_bounds, kernel_func_arg_sizes, kernel_dist_allocs


def update_func_op_arg_types(
    func_op, inputs, outputs, new_shapes, context, dist_allocs, enable_tensor
):
    old_func_type = func_op.function_type
    old_result_types = old_func_type.value.results
    new_input_types = []
    inputs_outputs = inputs + outputs
    for arg_id, ((ele_type_str, _), shape) in enumerate(
        zip(inputs_outputs, new_shapes)
    ):
        elem_ty = get_element_type_from_str(ele_type_str, context)
        old_ty = old_func_type.value.inputs[arg_id]
        memref_ty = (
            RankedTensorType.get(shape, elem_ty)
            if enable_tensor
            else MemRefType.get(shape, elem_ty)
        )
        new_input_types.append(memref_ty if dist_allocs[arg_id] else old_ty)
        # Update subview results memory layout
        if not enable_tensor and dist_allocs[arg_id]:
            entry_block = func_op.regions[0].blocks[0]
            args = entry_block.arguments
            ops_stack = []
            for block in func_op.regions[0].blocks:
                ops_stack.extend(block.operations)
            while ops_stack:
                op = ops_stack.pop()
                if (
                    op.operation.name == "memref.subview"
                    and op.operands[0] == args[arg_id]
                ):
                    old_result_type = op.results.types[0]
                    strides = []
                    times = 1
                    for size in reversed(shape):
                        strides.append(times)
                        times *= size
                    strides = list(reversed(strides))
                    layout = StridedLayoutAttr.get(0, strides)
                    result = MemRefType.get(
                        old_result_type.shape,
                        old_result_type.element_type,
                        layout=layout,
                    )
                    subview = memref_d.SubViewOp(
                        source=op.source,
                        result=result,
                        static_offsets=op.static_offsets,
                        static_sizes=op.static_sizes,
                        static_strides=op.static_strides,
                        offsets=[],
                        sizes=[],
                        strides=[],
                        ip=InsertionPoint(op),
                    )
                    op.result.replace_all_uses_with(subview.result)
                    op.erase()
                else:
                    for region in op.regions:
                        for block in region.blocks:
                            ops_stack.extend(block.operations)
    # Add remaining stream arguments
    arg_id = len(inputs_outputs)
    while arg_id < len(old_func_type.value.inputs):
        new_input_types.append(old_func_type.value.inputs[arg_id])
        arg_id += 1
    new_func_type = FunctionType.get(new_input_types, old_result_types, context)
    new_type = TypeAttr.get(new_func_type, context)
    func_op.operation.attributes["function_type"] = new_type
    entry_block = func_op.entry_block
    for arg_id, block_arg in enumerate(entry_block.arguments):
        if arg_id < len(new_input_types):
            block_arg.set_type(new_input_types[arg_id])


def lower_tensor_to_memref(mod, enable_tensor):
    passes = (
        [
            # "linalg-generalize-named-ops",
            # "linalg-fuse-elementwise-ops",
            "one-shot-bufferize{bufferize-function-boundaries function-boundary-type-conversion=identity-layout-map}",
            "func.func(convert-linalg-to-affine-loops),lower-affine",
        ]
        if enable_tensor
        else [
            # "linalg-generalize-named-ops",
            # "linalg-fuse-elementwise-ops",
            "func.func(convert-linalg-to-affine-loops),lower-affine",
        ]
    )
    pipeline = f'builtin.module({",".join(passes)})'
    with mod.context:
        mlir_pass_manager.parse(pipeline).run(mod.operation)


def record_local_buffer(mod, kernel_index_ranges):
    kernel_func_buf_dicts = {}
    _, funcs = get_public_funcs(mod)
    for kernel_name, (start, end) in kernel_index_ranges.items():
        func_buf_dicts = []
        for fid in range(start, end):
            func = funcs[fid]
            buf_dict = {}
            ops_stack = []
            for block in func.regions[0].blocks:
                ops_stack.extend(block.operations)
            while ops_stack:
                op = ops_stack.pop()
                if op.operation.name == "memref.alloc":
                    name = op.result.get_name()
                    dtype, shape = get_dtype_and_shape_from_type(op.result.type)
                    buf_dict[name] = (dtype, shape)
                for region in op.regions:
                    for block in region.blocks:
                        ops_stack.extend(block.operations)
            func_buf_dicts.append(buf_dict)
        kernel_func_buf_dicts[kernel_name] = func_buf_dicts
    return kernel_func_buf_dicts


class AIEModule:
    def __init__(
        self,
        module,
        top_func_name,
        project,
        kernel_mappings,
        enable_tensor,
        stream_info,
    ):
        self.module = module
        self.top_func_name = top_func_name
        self.project = project
        self.module = module
        self.kernel_mappings = kernel_mappings
        self.enable_tensor = enable_tensor
        self.stream_info = stream_info
        self.kernel_funcs = {}
        self.kernel_inputs = {}
        self.kernel_outputs = {}

    def build(self):
        assert "MLIR_AIE_INSTALL_DIR" in os.environ, "Please set MLIR_AIE_INSTALL_DIR"
        assert "PEANO_INSTALL_DIR" in os.environ, "Please set PEANO_INSTALL_DIR"
        self.kernel_index_ranges = get_kernel_index_ranges(
            self.module, self.kernel_mappings
        )
        for i, (kernel_name, (start, _)) in enumerate(self.kernel_index_ranges.items()):
            kernel_func = self.module.body.operations[start]
            self.kernel_funcs[kernel_name] = kernel_func
            # Assume the last arguent of all kernels is the output
            inputs, _ = get_func_inputs_outputs(kernel_func)
            if i < len(self.kernel_index_ranges) - 1:
                self.kernel_inputs[kernel_name] = inputs
                self.kernel_outputs[kernel_name] = []
            else:
                self.kernel_inputs[kernel_name] = inputs[:-1]
                self.kernel_outputs[kernel_name] = [inputs[-1]]
        # Assume all functions of the same kernel have the same input and output arguments
        _, kernel_func_arg_sizes, kernel_dist_allocs = reindex_tensor_access(
            self.module,
            self.kernel_index_ranges,
            self.kernel_inputs,
            self.kernel_outputs,
        )
        with self.module.context as ctx, Location.unknown():
            for kernel_name, (start, end) in self.kernel_index_ranges.items():
                func_arg_sizes = kernel_func_arg_sizes[kernel_name]
                for fid in range(start, end):
                    func = self.module.body.operations[fid]
                    dist_allocs = kernel_dist_allocs[kernel_name]
                    shapes = func_arg_sizes[fid - start]
                    update_func_op_arg_types(
                        func,
                        self.kernel_inputs[kernel_name],
                        self.kernel_outputs[kernel_name],
                        shapes,
                        ctx,
                        dist_allocs,
                        enable_tensor=self.enable_tensor,
                    )
        external_kernels = inject_aie_kernels(self.module)
        lower_tensor_to_memref(self.module, self.enable_tensor)
        kernel_func_buf_dicts = record_local_buffer(
            self.module, self.kernel_index_ranges
        )
        code = codegen_aie_mlir(
            self.module,
            self.kernel_mappings,
            self.kernel_index_ranges,
            self.kernel_inputs,
            self.kernel_outputs,
            kernel_func_arg_sizes,
            kernel_func_buf_dicts,
            kernel_dist_allocs,
            external_kernels,
            self.stream_info,
        )
        os.makedirs(os.path.join(self.project, "build"), exist_ok=True)
        with open(os.path.join(self.project, "top.mlir"), "w", encoding="utf-8") as f:
            f.write(code)
        # compile external kernels
        kernel_code, generated_kernels = codegen_external_kernels(external_kernels)
        if len(generated_kernels) > 0:
            with open(
                os.path.join(self.project, "external.cc"), "w", encoding="utf-8"
            ) as f:
                f.write(kernel_code)
            path = os.path.join(os.path.dirname(__file__), "aie_kernels")
            cmd = f"cd {self.project} && $PEANO_INSTALL_DIR/bin/clang++ -O2 -v -std=c++20 --target=aie2-none-unknown-elf -Wno-parentheses -Wno-attributes -Wno-macro-redefined -DNDEBUG -I $(dirname $(which aie-opt))/../include -I {path} -c external.cc -o external.o"
            process = subprocess.Popen(cmd, shell=True)
            process.wait()
        # build mlir-aie
        cmd = f"cd {self.project} && PYTHONPATH=$MLIR_AIE_INSTALL_DIR/python aiecc.py --aie-generate-cdo --aie-generate-npu --no-compile-host --no-xchesscc --no-xbridge --xclbin-name=build/final.xclbin --npu-insts-name=insts.txt top.mlir"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to compile the MLIR-AIE code")
        path = os.path.dirname(__file__)
        path = os.path.join(path, "../harness/aie")
        os.system(f"cp -r {path}/* {self.project}")
        host_code = codegen_host(self.kernel_inputs, self.kernel_outputs)
        with open(os.path.join(self.project, "test.cpp"), "w", encoding="utf-8") as f:
            f.write(host_code)
        cmd = f"cd {self.project}/build && cmake .. -DTARGET_NAME=top -DMLIR_AIE_DIR=$MLIR_AIE_INSTALL_DIR/.. && cmake --build . --config Release"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to build AIE project.")
        return self

    def __call__(self, *args):
        # suppose the last argument is output
        for i, arg in enumerate(args[:-1]):
            with open(
                os.path.join(self.project, f"input{i}.data"), "w", encoding="utf-8"
            ) as f:
                f.write("\n".join([str(i) for i in arg.flatten()]))
        cmd = f"cd {self.project} && ./build/top -x build/final.xclbin -i insts.txt -k MLIR_AIE"
        process = subprocess.Popen(cmd, shell=True)
        process.wait()
        if process.returncode != 0:
            raise RuntimeError("Failed to execute AIE code.")
        # TODO: need to complete multiple outputs rules
        result = read_tensor_from_file(
            list(self.kernel_outputs.values())[-1][0][0],
            args[-1].shape,
            f"{self.project}/output.data",
        )
        args[-1][:] = result
