
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/dive_03_composition.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_dive_03_composition.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_dive_03_composition.py:


Kernel Composition
==================

**Author**: Hongzheng Chen (hzchen@cs.cornell.edu)

This document will discuss kernel composition.
In the previous tutorials, we have seen how to write a simple kernel.
However, in real applications, we often need to compose multiple kernels together.

In the following example, we define a ``matrix_add`` and a ``gemm`` kernel, and wrap them into a ``top``-level function.

.. GENERATED FROM PYTHON SOURCE LINES 16-44

.. code-block:: Python


    import allo
    from allo.ir.types import int32, float32

    M, K, N = 32, 32, 32


    def matrix_add(A: int32[M, N]) -> int32[M, N]:
        B: int32[M, N] = 0
        for i, j in allo.grid(M, N):
            B[i, j] = A[i, j] + 1
        return B


    def gemm(A: int32[M, K], B: int32[K, N]) -> int32[M, N]:
        C: int32[M, N] = 0
        for i, j in allo.grid(M, N):
            for k in allo.reduction(K):
                C[i, j] += A[i, k] * B[k, j]
        return C


    def top(A: int32[M, K], B: int32[K, N]) -> int32[M, N]:
        C = gemm(A, B)
        D = matrix_add(C)
        return D









.. GENERATED FROM PYTHON SOURCE LINES 45-47

Different teams or people can then work on different parts of the code and optimize each kernel.
We first create a schedule for the ``matrix_add`` kernel, and add several optimizations.

.. GENERATED FROM PYTHON SOURCE LINES 47-52

.. code-block:: Python


    s1 = allo.customize(matrix_add)
    s1.pipeline("j")
    print(s1.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @matrix_add(%arg0: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "s", otypes = "s"} {
        %alloc = memref.alloc() {name = "B"} : memref<32x32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32x32xi32>)
        affine.for %arg1 = 0 to 32 {
          affine.for %arg2 = 0 to 32 {
            %0 = affine.load %arg0[%arg1, %arg2] {from = "A"} : memref<32x32xi32>
            %1 = arith.extsi %0 : i32 to i33
            %c1_i32 = arith.constant 1 : i32
            %2 = arith.extsi %c1_i32 : i32 to i33
            %3 = arith.addi %1, %2 : i33
            %4 = arith.trunci %3 : i33 to i32
            affine.store %4, %alloc[%arg1, %arg2] {to = "B"} : memref<32x32xi32>
          } {loop_name = "j", pipeline_ii = 1 : ui32}
        } {loop_name = "i", op_name = "S_i_j_0"}
        return %alloc : memref<32x32xi32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 53-54

Then we create a schedule for the ``gemm`` kernel and optimize it.

.. GENERATED FROM PYTHON SOURCE LINES 54-61

.. code-block:: Python


    s2 = allo.customize(gemm)
    s2.reorder("k", "j")
    s2.buffer_at(s2.C, axis="i")
    s2.pipeline("j")
    print(s2.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @gemm(%arg0: memref<32x32xi32>, %arg1: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "ss", otypes = "s"} {
        %alloc = memref.alloc() {name = "C"} : memref<32x32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32x32xi32>)
        affine.for %arg2 = 0 to 32 {
          %alloc_0 = memref.alloc() : memref<32xi32>
          affine.for %arg3 = 0 to 32 {
            affine.store %c0_i32, %alloc_0[%arg3] : memref<32xi32>
          } {buffer, loop_name = "j_init", pipeline_ii = 1 : i32}
          affine.for %arg3 = 0 to 32 {
            affine.for %arg4 = 0 to 32 {
              %0 = affine.load %arg0[%arg2, %arg3] {from = "A"} : memref<32x32xi32>
              %1 = affine.load %arg1[%arg3, %arg4] {from = "B"} : memref<32x32xi32>
              %2 = arith.extsi %0 : i32 to i64
              %3 = arith.extsi %1 : i32 to i64
              %4 = arith.muli %2, %3 : i64
              %5 = affine.load %alloc_0[%arg4] : memref<32xi32>
              %6 = arith.extsi %5 : i32 to i65
              %7 = arith.extsi %4 : i64 to i65
              %8 = arith.addi %6, %7 : i65
              %9 = arith.trunci %8 : i65 to i32
              affine.store %9, %alloc_0[%arg4] : memref<32xi32>
            } {loop_name = "j", pipeline_ii = 1 : ui32}
          } {loop_name = "k", op_name = "S_k_0", reduction}
          affine.for %arg3 = 0 to 32 {
            %0 = affine.load %alloc_0[%arg3] : memref<32xi32>
            affine.store %0, %alloc[%arg2, %arg3] : memref<32x32xi32>
          } {buffer, loop_name = "j_back", pipeline_ii = 1 : i32}
        } {loop_name = "i", op_name = "S_i_j_0"}
        return %alloc : memref<32x32xi32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 62-63

Notice that now we only optimize the separate kernels but do not incorporate them into the top-level function, as shown in the following printed module.

.. GENERATED FROM PYTHON SOURCE LINES 63-67

.. code-block:: Python


    s = allo.customize(top)
    print(s.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @gemm(%arg0: memref<32x32xi32>, %arg1: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "ss", otypes = "s"} {
        %alloc = memref.alloc() {name = "C"} : memref<32x32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32x32xi32>)
        affine.for %arg2 = 0 to 32 {
          affine.for %arg3 = 0 to 32 {
            affine.for %arg4 = 0 to 32 {
              %0 = affine.load %arg0[%arg2, %arg4] {from = "A"} : memref<32x32xi32>
              %1 = affine.load %arg1[%arg4, %arg3] {from = "B"} : memref<32x32xi32>
              %2 = arith.extsi %0 : i32 to i64
              %3 = arith.extsi %1 : i32 to i64
              %4 = arith.muli %2, %3 : i64
              %5 = affine.load %alloc[%arg2, %arg3] {from = "C"} : memref<32x32xi32>
              %6 = arith.extsi %5 : i32 to i65
              %7 = arith.extsi %4 : i64 to i65
              %8 = arith.addi %6, %7 : i65
              %9 = arith.trunci %8 : i65 to i32
              affine.store %9, %alloc[%arg2, %arg3] {to = "C"} : memref<32x32xi32>
            } {loop_name = "k", op_name = "S_k_0", reduction}
          } {loop_name = "j"}
        } {loop_name = "i", op_name = "S_i_j_0"}
        return %alloc : memref<32x32xi32>
      }
      func.func @matrix_add(%arg0: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "s", otypes = "s"} {
        %alloc = memref.alloc() {name = "B"} : memref<32x32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32x32xi32>)
        affine.for %arg1 = 0 to 32 {
          affine.for %arg2 = 0 to 32 {
            %0 = affine.load %arg0[%arg1, %arg2] {from = "A"} : memref<32x32xi32>
            %1 = arith.extsi %0 : i32 to i33
            %c1_i32 = arith.constant 1 : i32
            %c1_i32_0 = arith.constant 1 : i32
            %2 = arith.extsi %c1_i32_0 : i32 to i33
            %3 = arith.addi %1, %2 : i33
            %4 = arith.trunci %3 : i33 to i32
            affine.store %4, %alloc[%arg1, %arg2] {to = "B"} : memref<32x32xi32>
          } {loop_name = "j"}
        } {loop_name = "i", op_name = "S_i_j_0"}
        return %alloc : memref<32x32xi32>
      }
      func.func @top(%arg0: memref<32x32xi32>, %arg1: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "ss", otypes = "s"} {
        %0 = call @gemm(%arg0, %arg1) {name = "C"} : (memref<32x32xi32>, memref<32x32xi32>) -> memref<32x32xi32>
        %1 = call @matrix_add(%0) {name = "D"} : (memref<32x32xi32>) -> memref<32x32xi32>
        return %1 : memref<32x32xi32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 68-70

Therefore, after each part has been optimized, we need to explicitly *compose* them together.
In Allo, we can use the ``.compose()`` primitive to compose the schedules together into the parent function.

.. GENERATED FROM PYTHON SOURCE LINES 70-74

.. code-block:: Python


    s.compose([s1, s2])
    print(s.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @gemm(%arg0: memref<32x32xi32>, %arg1: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "ss", otypes = "s"} {
        %alloc = memref.alloc() {name = "C"} : memref<32x32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32x32xi32>)
        affine.for %arg2 = 0 to 32 {
          %alloc_0 = memref.alloc() : memref<32xi32>
          affine.for %arg3 = 0 to 32 {
            affine.store %c0_i32, %alloc_0[%arg3] : memref<32xi32>
          } {buffer, loop_name = "j_init", pipeline_ii = 1 : i32}
          affine.for %arg3 = 0 to 32 {
            affine.for %arg4 = 0 to 32 {
              %0 = affine.load %arg0[%arg2, %arg3] {from = "A"} : memref<32x32xi32>
              %1 = affine.load %arg1[%arg3, %arg4] {from = "B"} : memref<32x32xi32>
              %2 = arith.extsi %0 : i32 to i64
              %3 = arith.extsi %1 : i32 to i64
              %4 = arith.muli %2, %3 : i64
              %5 = affine.load %alloc_0[%arg4] : memref<32xi32>
              %6 = arith.extsi %5 : i32 to i65
              %7 = arith.extsi %4 : i64 to i65
              %8 = arith.addi %6, %7 : i65
              %9 = arith.trunci %8 : i65 to i32
              affine.store %9, %alloc_0[%arg4] : memref<32xi32>
            } {loop_name = "j", pipeline_ii = 1 : ui32}
          } {loop_name = "k", op_name = "S_k_0", reduction}
          affine.for %arg3 = 0 to 32 {
            %0 = affine.load %alloc_0[%arg3] : memref<32xi32>
            affine.store %0, %alloc[%arg2, %arg3] : memref<32x32xi32>
          } {buffer, loop_name = "j_back", pipeline_ii = 1 : i32}
        } {loop_name = "i", op_name = "S_i_j_0"}
        return %alloc : memref<32x32xi32>
      }
      func.func @matrix_add(%arg0: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "s", otypes = "s"} {
        %alloc = memref.alloc() {name = "B"} : memref<32x32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32x32xi32>)
        affine.for %arg1 = 0 to 32 {
          affine.for %arg2 = 0 to 32 {
            %0 = affine.load %arg0[%arg1, %arg2] {from = "A"} : memref<32x32xi32>
            %1 = arith.extsi %0 : i32 to i33
            %c1_i32 = arith.constant 1 : i32
            %2 = arith.extsi %c1_i32 : i32 to i33
            %3 = arith.addi %1, %2 : i33
            %4 = arith.trunci %3 : i33 to i32
            affine.store %4, %alloc[%arg1, %arg2] {to = "B"} : memref<32x32xi32>
          } {loop_name = "j", pipeline_ii = 1 : ui32}
        } {loop_name = "i", op_name = "S_i_j_0"}
        return %alloc : memref<32x32xi32>
      }
      func.func @top(%arg0: memref<32x32xi32>, %arg1: memref<32x32xi32>) -> memref<32x32xi32> attributes {itypes = "ss", otypes = "s"} {
        %0 = call @gemm(%arg0, %arg1) {name = "C"} : (memref<32x32xi32>, memref<32x32xi32>) -> memref<32x32xi32>
        %1 = call @matrix_add(%0) {name = "D"} : (memref<32x32xi32>) -> memref<32x32xi32>
        return %1 : memref<32x32xi32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 75-76

We can see that the schedules for the ``matrix_add`` and ``gemm`` kernels are both correctly optimized in the top-level function.

.. GENERATED FROM PYTHON SOURCE LINES 78-81

Template Composition
--------------------
Sometimes we may define template kernels and invoke the kernel with different template arguments. Allo provides an *id* option to specify the exact kernel to be composed.

.. GENERATED FROM PYTHON SOURCE LINES 81-99

.. code-block:: Python



    def kernel[T_in, T_out, S](A: "T_in[S]") -> "T_out[S]":
        B: T_out[S] = 0
        for i in range(S):
            with allo.meta_if(T_out == int32):
                B[i] = A[i] + 1
            with allo.meta_else():
                B[i] = A[i] * 2
        return B


    def top2(A: int32[M]) -> float32[M]:
        C = kernel[int32, int32, M, "K1"](A)
        D = kernel[int32, float32, M, "K2"](C)
        return D









.. GENERATED FROM PYTHON SOURCE LINES 100-102

Specifically, the last argument of the template kernel is the *id* of the kernel. Later on we can use this ID for distinguishing different kernels during composition.
We also customize the two template kernels with different optimizations first.

.. GENERATED FROM PYTHON SOURCE LINES 102-111

.. code-block:: Python


    s1 = allo.customize(kernel, instantiate=[int32, int32, M])
    s1.unroll("i", factor=4)
    print(s1.module)

    s2 = allo.customize(kernel, instantiate=[int32, float32, M])
    s2.pipeline("i")
    print(s2.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @kernel(%arg0: memref<32xi32>) -> memref<32xi32> attributes {itypes = "s", otypes = "s"} {
        %alloc = memref.alloc() {name = "B"} : memref<32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32xi32>)
        affine.for %arg1 = 0 to 32 {
          %0 = affine.load %arg0[%arg1] {from = "A"} : memref<32xi32>
          %1 = arith.extsi %0 : i32 to i33
          %c1_i32 = arith.constant 1 : i32
          %2 = arith.extsi %c1_i32 : i32 to i33
          %3 = arith.addi %1, %2 : i33
          %4 = arith.trunci %3 : i33 to i32
          affine.store %4, %alloc[%arg1] {to = "B"} : memref<32xi32>
        } {loop_name = "i", op_name = "S_i_0", unroll = 4 : i32}
        return %alloc : memref<32xi32>
      }
    }

    module {
      func.func @kernel(%arg0: memref<32xi32>) -> memref<32xf32> attributes {itypes = "s", otypes = "_"} {
        %c0_i32 = arith.constant 0 : i32
        %0 = arith.sitofp %c0_i32 : i32 to f32
        %alloc = memref.alloc() {name = "B"} : memref<32xf32>
        linalg.fill ins(%0 : f32) outs(%alloc : memref<32xf32>)
        affine.for %arg1 = 0 to 32 {
          %1 = affine.load %arg0[%arg1] {from = "A"} : memref<32xi32>
          %2 = arith.extsi %1 : i32 to i64
          %c2_i32 = arith.constant 2 : i32
          %3 = arith.extsi %c2_i32 : i32 to i64
          %4 = arith.muli %2, %3 : i64
          %5 = arith.sitofp %4 : i64 to f32
          affine.store %5, %alloc[%arg1] {to = "B"} : memref<32xf32>
        } {loop_name = "i", op_name = "S_i_0", pipeline_ii = 1 : ui32}
        return %alloc : memref<32xf32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 112-113

Finally, we compose the two template kernels into the top-level function with the ID specified.

.. GENERATED FROM PYTHON SOURCE LINES 113-119

.. code-block:: Python


    s = allo.customize(top2)
    s.compose(s1, id="K1")
    s.compose(s2, id="K2")
    print(s.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @kernel_K1(%arg0: memref<32xi32>) -> memref<32xi32> attributes {itypes = "s", otypes = "s"} {
        %alloc = memref.alloc() {name = "B"} : memref<32xi32>
        %c0_i32 = arith.constant 0 : i32
        linalg.fill ins(%c0_i32 : i32) outs(%alloc : memref<32xi32>)
        affine.for %arg1 = 0 to 32 {
          %0 = affine.load %arg0[%arg1] {from = "A"} : memref<32xi32>
          %1 = arith.extsi %0 : i32 to i33
          %c1_i32 = arith.constant 1 : i32
          %2 = arith.extsi %c1_i32 : i32 to i33
          %3 = arith.addi %1, %2 : i33
          %4 = arith.trunci %3 : i33 to i32
          affine.store %4, %alloc[%arg1] {to = "B"} : memref<32xi32>
        } {loop_name = "i", op_name = "S_i_0", unroll = 4 : i32}
        return %alloc : memref<32xi32>
      }
      func.func @kernel_K2(%arg0: memref<32xi32>) -> memref<32xf32> attributes {itypes = "s", otypes = "_"} {
        %c0_i32 = arith.constant 0 : i32
        %0 = arith.sitofp %c0_i32 : i32 to f32
        %alloc = memref.alloc() {name = "B"} : memref<32xf32>
        linalg.fill ins(%0 : f32) outs(%alloc : memref<32xf32>)
        affine.for %arg1 = 0 to 32 {
          %1 = affine.load %arg0[%arg1] {from = "A"} : memref<32xi32>
          %2 = arith.extsi %1 : i32 to i64
          %c2_i32 = arith.constant 2 : i32
          %3 = arith.extsi %c2_i32 : i32 to i64
          %4 = arith.muli %2, %3 : i64
          %5 = arith.sitofp %4 : i64 to f32
          affine.store %5, %alloc[%arg1] {to = "B"} : memref<32xf32>
        } {loop_name = "i", op_name = "S_i_0", pipeline_ii = 1 : ui32}
        return %alloc : memref<32xf32>
      }
      func.func @top2(%arg0: memref<32xi32>) -> memref<32xf32> attributes {itypes = "s", otypes = "_"} {
        %0 = call @kernel_K1(%arg0) {name = "C"} : (memref<32xi32>) -> memref<32xi32>
        %1 = call @kernel_K2(%0) {name = "D"} : (memref<32xi32>) -> memref<32xf32>
        return %1 : memref<32xf32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 120-121

We can see from the printed module that the loop in the first kernel is unrolled by a factor of 4, and the loop in the second kernel is pipelined.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.779 seconds)


.. _sphx_glr_download_gallery_dive_03_composition.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: dive_03_composition.ipynb <dive_03_composition.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: dive_03_composition.py <dive_03_composition.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: dive_03_composition.zip <dive_03_composition.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
