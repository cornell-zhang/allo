{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Vivado/Vitis HLS Backend\n\n**Author**: Hongzheng Chen (hzchen@cs.cornell.edu)\n\n\nIn this tutorial, we will demonstrate how to leverage the Allo DSL to generate\n[Vivado/Vitis HLS](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vitis/vitis-hls.html) C++ code for FPGA.\n\n## Import Allo\nFirst, we import the necessary packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import allo\nfrom allo.ir.types import float32\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algorithm Definition\nWe again define a general matrix multiplication (GEMM) in this tutorial.\nHowever, we will make some changes to demonstrate more features of the DSL.\n\nWe can define the constants as follows, which denotes the matrix sizes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "M, N, K = 32, 32, 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we define the main computation of the GEMM but use ``float32`` as the\ndata type. Notice that users can easily leverage the previously defined arguments\n(e.g., ``M``, ``N``, and ``K``) to construct the matrices, and Allo will\nautomatically captures the global variables.\n\nSince Allo has a strict type system, we need to be careful about the\ndata types of the variables. To initialize matrix ``C`` with all zeros, we\nneed to pass in a floating-point value ``0.0`` instead of an integer.\n\nWe also use the ``allo.reduction`` API to denote the reduction axis. The\nreduction axis is the loop iterator that is used to accumulate the result.\nIn this example, we use ``k`` as the reduction axis, which means the\ncomputation of ``C[i, j]`` will be accumulated along the ``k`` dimension.\nThis annotation is necessary for later optimizations, since Allo leverages\nthis information to generate correct intermediate buffers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gemm(A: float32[M, K], B: float32[K, N]) -> float32[M, N]:\n    C: float32[M, N] = 0.0\n    for i, j in allo.grid(M, N):\n        for k in allo.reduction(K):\n            C[i, j] += A[i, k] * B[k, j]\n    return C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scalar-Vector Product for GEMM\n\nNext, we create a schedule for the GEMM and start to optimize the program.\nWe try to implement the **interleaving accumulation** technique presented in\n[this paper](https://arxiv.org/abs/1805.08288), which is also viewed as\nthe **scalar-vector product** since it changes the computation order of the\noriginal dot-product.\n\n<img src=\"file://../_static/scalar-vector-product.png\" width=\"600\">\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To get more rational of this technique, please refer to the above mentioned\n   paper from Torsten Hoefler's group.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = allo.customize(gemm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first reorder the inner reduction loop with the middle loop.\nThis is used to change the computation order of matrix multiplication.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s.reorder(\"k\", \"j\")\nprint(s.module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This reordering seems to be easy, but it is impossible in the old Allo,\n   since the previous Allo directly generate reduction variables which make\n   the ``j`` loop becomes imperfect, while MLIR only supports reordering perfect\n   loops.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we create a new buffer for the output tensor ``C``.\nWe provide a ``.buffer_at()`` primitive for users to quickly create a new buffer\nalong a specific axis. Since Allo has attached all the tensors to the function,\nwe can directly use ``<schedule>.<tensor>`` to access a specific tensor in the schedule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s.buffer_at(s.C, axis=\"i\")\nprint(s.module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above generated code, we can see that Allo automatically\ncreates an intermediate buffer ``%1`` for ``C`` and attach it inside the ``i`` loop.\nAlso two additional loop nested named ``j_init`` and ``j_back`` are created to\ninitialize and write the intermediate buffer back to output tensor.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, we pipeline the ``j`` loop in order to achieve the best performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s.pipeline(\"j\")\nprint(s.module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Codegen for Vivado/Vitis HLS\nSimilar to the CPU execution, we only need to change the target of the ``.build()`` function\nin order to target different backends. Here, we use ``vhls`` as the target to generate\nVivado/Vitis HLS code, which will directly returns the generated code as a string.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "code = s.build(target=\"vhls\")\nprint(code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the generated code preserves the same structure as the IR, and inserts\nnecessary headers and pragmas for Vivado/Vitis HLS. The generated code can be directly passed\nto Vivado/Vitis HLS to generate RTL designs.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Vivado HLS was the previous name of Vitis HLS (before 2020.1). The previous HLS code\n   can still run on the latest Vitis HLS, but the performance of the generated RTL design\n   and the estimated reports may be different, as the newer version of Vitis HLS provides better\n   automatic optimizations.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also provide an easy way to invoke Vitis HLS from Allo. Users can simply provide\nthe synthesis mode that are supported by Vitis HLS (e.g., ``sw_emu``, ``hw_emu``, and ``hw``),\nand the target project folder name. Allo will automatically generate\nthe HLS project and invoke the compiler to generate the RTL design.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>- ``sw_emu``: Software emulation mode, which is similar to C simulation that compiles the program using C compiler and runs it on the CPU. Depending on the size of your input data, this mode may take within one minute.\n   - ``hw_emu``: Hardware emulation mode, which is similar to co-simulation that compiles the program into RTL design using HLS compiler and runs the RTL with the test bench on the FPGA emulator. Since it needs to go through the HLS synthesis flow, it may take several minutes to finish.\n   - ``hw``: Hardware mode, which compiles the program into RTL design using HLS, goes through placement and routing, generates the bitstream, and finally executes on FPGA. This mode may take several hours to finish.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = s.build(target=\"vitis_hls\", mode=\"hw_emu\", project=\"gemm.prj\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running the above instruction, we can see a ``gemm.prj`` folder is generated in the current directory:\n\n- ``host.cpp``: The host (CPU) OpenCL code that invokes the generated accelerator.\n- ``kernel.cpp``: The generated accelerator code.\n- ``Makefile``: Defined some shorthands for compiling the project.\n\nTo generate the hardware design and see the performance estimation, we need to first\nprepare the input data. Allo supports NumPy inputs even for hardware programs,\nso we can just create two NumPy arrays ``np_A`` and ``np_B`` for inputs.\nSince the C++ design cannot support returning a new array, we also need to\nexplicitly create an output array ``allo_C`` and pass it to the function.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You need to configure the [Vitis HLS](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vitis/vitis-hls.html) and [XRT](https://github.com/Xilinx/XRT) environment before proceeding to the next step.\n   For Zhang group students, we have the Vitis environment configured on the server, so you can directly\n   ``source /work/shared/common/allo/vitis_2023.2_u280.sh`` to set up the environment, which\n   targets the AMD U280 FPGA board.</p></div>\n\n```python\nnp_A = np.random.random((M, K)).astype(np.float32)\nnp_B = np.random.random((K, N)).astype(np.float32)\nallo_C = np.zeros((M, N), dtype=np.float32)\nmod(np_A, np_B, allo_C)\nnp.testing.assert_allclose(allo_C, np.matmul(np_A, np_B), rtol=1e-5, atol=1e-5)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After executing the above command, you can check the following report under ``gemm.prj/_x.hw_emu.xilinx_u250_gen3x16_xdma_4_1_202210_1/gemm/gemm/gemm/solution/syn/report/csynth.rpt``.\n\n```python\n+--------------------------------------------------+---------+-----------+----------+---------+------+----------+---------+---------+-------------+------------+-----+\n|                      Modules                     | Latency |  Latency  | Iteration|         | Trip |          |         |         |             |            |     |\n|                      & Loops                     | (cycles)|    (ns)   |  Latency | Interval| Count| Pipelined|  BRAM   |   DSP   |      FF     |     LUT    | URAM|\n+--------------------------------------------------+---------+-----------+----------+---------+------+----------+---------+---------+-------------+------------+-----+\n|+ gemm                                            |    39934|  1.331e+05|         -|    39935|     -|        no|  6 (~0%)|  5 (~0%)|  19074 (~0%)|  29069 (2%)|    -|\n| + gemm_Pipeline_VITIS_LOOP_44_1_VITIS_LOOP_45_2  |     1026|  3.420e+03|         -|     1026|     -|        no|        -|        -|     36 (~0%)|   169 (~0%)|    -|\n|  o VITIS_LOOP_44_1_VITIS_LOOP_45_2               |     1024|  3.413e+03|         2|        1|  1024|       yes|        -|        -|            -|           -|    -|\n| o l_S_buf0_buf0_l_0_l_buf0_l_1                   |     1025|  3.416e+03|         3|        1|  1024|       yes|        -|        -|            -|           -|    -|\n| o l_S_buf1_buf1_l_0_l_buf1_l_1                   |     1025|  3.416e+03|         3|        1|  1024|       yes|        -|        -|            -|           -|    -|\n| o l_S_i_j_0_i                                    |    35616|  1.187e+05|      1113|        -|    32|        no|        -|        -|            -|           -|    -|\n|  + gemm_Pipeline_l_j_init                        |       34|    113.322|         -|       34|     -|        no|        -|        -|      8 (~0%)|    50 (~0%)|    -|\n|   o l_j_init                                     |       32|    106.656|         1|        1|    32|       yes|        -|        -|            -|           -|    -|\n|  + gemm_Pipeline_l_S_k_0_k_l_j                   |     1039|  3.463e+03|         -|     1039|     -|        no|        -|  5 (~0%)|    759 (~0%)|   494 (~0%)|    -|\n|   o l_S_k_0_k_l_j                                |     1037|  3.456e+03|        15|        1|  1024|       yes|        -|        -|            -|           -|    -|\n|  + gemm_Pipeline_l_j_back                        |       34|    113.322|         -|       34|     -|        no|        -|        -|     15 (~0%)|    78 (~0%)|    -|\n|   o l_j_back                                     |       32|    106.656|         2|        1|    32|       yes|        -|        -|            -|           -|    -|\n| o l_S_result2_result2_l_0_l_result2_l_1          |     1026|  3.420e+03|         4|        1|  1024|       yes|        -|        -|            -|           -|    -|\n+--------------------------------------------------+---------+-----------+----------+---------+------+----------+---------+---------+-------------+------------+-----+\n```\nFrom the above output, we can clearly see that all the loops inside the GEMM kernel (marked as ``o``) are pipelined\nwith Initiation Interval (II) equal to 1. You can also find more detailed information under the ``report`` folder.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## On-board Execution\nAfter optimizing the design and make sure everything works correctly,\nwe can push the generated RTL design to the backend synthesis flow to generate\nthe bitstream for FPGA. In Allo, we can directly change the target to ``hw``\nto launch the backend synthesis job. It may take several hours to generate the final\nbitstream, so it would be better to run it using [tmux](https://github.com/tmux/tmux/wiki).\n\n```python\nmod = s.build(target=\"vitis_hls\", mode=\"hw\", project=\"gemm.prj\")\nmod(np_A, np_B, allo_C)\nnp.testing.assert_allclose(allo_C, np.matmul(np_A, np_B), rtol=1e-5, atol=1e-5)\n```\nFinally, you should be able to see the generated bitstream ``.xclbin`` under the ``gemm.prj/build_dir.hw.xilinx_u280_gen3x16_xdma_1_202211_1`` folder\n(actual board name may be different), and the above test should pass.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get more detailed information on the resource usage and performance of the generated design,\nyou can check the following files:\n\n- ``gemm.prj/build_dir.hw.xilinx_u280_gen3x16_xdma_1_202211_1/gemm.xclbin``: The generated bitstream.\n- ``gemm.prj/build_dir.hw.xilinx_u280_gen3x16_xdma_1_202211_1/gemm.link.xclbin.info``: Frequency of the actual design, which can be found in ``DATA_CLK``. By default, it is 300MHz.\n- ``gemm.prj/_x.hw.xilinx_u280_gen3x16_xdma_1_202211_1/reports/gemm/hls_reports/gemm_csynth.rpt``: The HLS synthesis report.\n- ``gemm.prj/_x.hw.xilinx_u280_gen3x16_xdma_1_202211_1/reports/link/imp/impl_1_full_util_routed.rpt``: The full utilization report after placement and routing. You can find the following resource usage:\n\n  - LUT: ``1. CLB Logic -- CLB LUTs``\n  - FF: ``1. CLB Logic -- CLB Registers -- Register as Flip Flop``\n  - BRAM: ``3. BLOCKRAM -- Block RAM Tile``\n  - DSP: ``4. ARITHMETIC -- DSPs``\n\n- ``gemm.prj/_x.hw.xilinx_u280_gen3x16_xdma_1_202211_1/reports/link/imp/impl_1_slr_util_routed.rpt``: The per SLR utilization report after placement and routing.\n- ``gemm.prj/_x.hw.xilinx_u280_gen3x16_xdma_1_202211_1/logs/gemm/gemm_vitis_hls.log``: The log file of the Vitis HLS.\n- ``gemm.prj/_x.hw.xilinx_u280_gen3x16_xdma_1_202211_1/logs/link/v++.log``: The log file of the Vivado backend synthesis.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}