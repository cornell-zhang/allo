{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MLIR Translation Guide\n\n**Author**: Hongzheng Chen (hzchen@cs.cornell.edu)\n\nThis guide will give some examples on how to invoke the MLIR toolchain to\nverify the correctness of a handwritten or generated MLIR program.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import allo\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define an MLIR program with linalg dialect\nBased on the [MLIR](https://mlir.llvm.org/docs/LangRef/) syntax, we can define\nan MLIR program as follows. Currently our frontend is not able to generate this\nlinalg program, but we can still use it to invoke the MLIR toolchain.\n\nBasically, linalg dialect provides lots of high-level operations, and they are\nmore like the NumPy operations, so we do not need to explicitly express the\nfor loops inside the program, which may be easier to conduct program transformations\nfor specific backends.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_mlir_program = \"\"\"\nfunc.func @matmul(%A: memref<32x32xi32>, %B: memref<32x32xi32>) -> memref<32x32xi32> {\n  %C = memref.alloc() : memref<32x32xi32>\n  %c0_i32 = arith.constant 0 : i32\n  linalg.fill ins(%c0_i32 : i32) outs(%C : memref<32x32xi32>)\n  linalg.matmul ins(%A, %B: memref<32x32xi32>, memref<32x32xi32>)\n                outs(%C: memref<32x32xi32>)\n  return %C: memref<32x32xi32>\n}\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>For more linalg examples, please refer to the [linalg test suite](https://github.com/llvm/llvm-project/tree/main/mlir/test/Dialect/Linalg).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We wrap the MLIR parser in allo, so we can directly invoke it to parse the MLIR\nprogram.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = allo.invoke_mlir_parser(test_mlir_program)\nprint(mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above result should be exactly the same as what we defined in the MLIR program,\nmeaning the MLIR program is valid. Otherwise, for example, if omit the return value\nof ``C``, you can see the following error message:\n\n```python\nloc(\"-\":8:3): error: 'func.return' op has 0 operands, but enclosing function (@matmul) returns 1\nTraceback (most recent call last):\n  File \"tutorials/developer_02_mlir.py\", line 47, in <module>\n    mod = allo.invoke_mlir_parser(test_mlir_program)\n  File \"/scratch/users/hc676/allo/allo/module.py\", line 33, in invoke_mlir_parser\n    module = Module.parse(str(mod), ctx)\nValueError: Unable to parse module assembly (see diagnostics)\n```\nThe first line gives the error message and the exact location (line 8, column 3) of the error.\nThen we know that there is a problem in the return value of our MLIR code, which helps us debug the program.\n\nTo further check what causes the error, we can print out the generic form of the MLIR program.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod.operation.print(\n    large_elements_limit=2,\n    enable_debug_info=True,\n    pretty_debug_info=True,\n    print_generic_op_form=True,\n    use_local_scope=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The generic form of the MLIR program is a more detailed representation of the MLIR program.\nHowever, if you see this form in your customized MLIR pass, it means your generated IR may not pass the MLIR verifier.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also wrap the LLVM execution engine in allo, so we can directly invoke it to execute the MLIR program.\nThe ``LLVMMoudle`` class takes the MLIR module and the name of the top function as input.\nThen we can directly invoke the module with random inputs, and see if the result is correct.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To execute the MLIR with an LLVM backend, we need to lower the MLIR program to LLVM dialect first.\n   This is done inside the ``LLVMModule`` class, and you can check the details [here](https://github.com/cornell-zhang/allo/blob/main/allo/module.py).\n   However, we only include several lowering passes from commonly used dialects in the module,\n   so not all the programs can be directly lowered. You will see some examples that cannot be lowered later.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "llvm_mod = allo.LLVMModule(mod, \"matmul\")\nnp_A = np.random.randint(0, 10, size=(32, 32), dtype=np.int32)\nnp_B = np.random.randint(0, 10, size=(32, 32), dtype=np.int32)\nallo_C = llvm_mod(np_A, np_B)\nnp.testing.assert_array_equal(allo_C, np_A @ np_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We verify the correctness of our handwritten MLIR program, but we definitely don't want users to write\nthese tedious IR code by hand, so we need to think about how to raise the abstraction level and let\nusers write programs in a more friendly way. One thing we can do is to provide high-level programming\nabstractions like NumPy that has lots of tensor-based operations instead of elementwise ones.\nTherefore, the frontend interface may look like this:\n\n```python\ndef kernel(A: int32[32, 32], B: int32[32, 32]) -> int32[32, 32]:\n    C = allo.matmul(A, B)\n    return C\n```\nLater, we want to figure out a way to lower this high-level program to the MLIR program we defined above.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define an MLIR program with Tensor dialect\nNot only for computation, we also need to raise the abstraction level for memory management.\nCurrently we explicitly use ``memref`` to allocate memory and pass them to the operations.\nHowever, as users already write tensor programs, we should generate tensor interfaces instead.\nThanks to the [tensor dialect](https://mlir.llvm.org/docs/Dialects/TensorOps), we can\neasily leverage it to conduct slicing, reshaping, and other tensor operations. Following\nshows an example of how to use the tensor dialect to define a matmul program:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensor_program = \"\"\"\nfunc.func @matmul(%A: tensor<32x32xi32>, %B: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %C = tensor.generate {\n      ^bb0(%i : index, %j : index):\n          %c0_i32 = arith.constant 0 : i32\n          tensor.yield %c0_i32 : i32\n  } : tensor<32x32xi32>\n  %1 = linalg.matmul ins(%A, %B: tensor<32x32xi32>, tensor<32x32xi32>)\n                outs(%C: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %1 : tensor<32x32xi32>\n}\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is very similar to the original one, but the main difference is that we use ``tensor``\ninstead of ``memref`` to define the input and output of the operations.\nAgain, we can invoke the MLIR parser to check if the program is valid.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = allo.invoke_mlir_parser(tensor_program)\nprint(mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It outputs without any error, so we know that the program is valid.\nAnd we can also invoke the LLVM execution engine trying to execute the program.\n\n```python\nllvm_mod = allo.LLVMModule(mod, \"matmul\")\n```\nYou will see the following error message:\n\n```\npython3: llvm-project/mlir/lib/Dialect/Linalg/Transforms/Loops.cpp:209:\n         mlir::FailureOr<llvm::SmallVector<mlir::Operation*, 4> > linalgOpToLoopsImpl(mlir::PatternRewriter&, mlir::linalg::LinalgOp)\n         [with LoopTy = mlir::AffineForOp]: Assertion `linalgOp.hasBufferSemantics() && \"expected linalg op with buffer semantics\"' failed.\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, the program cannot be lowered to LLVM dialect, because we have not added\nthe lowering pass from tensor dialect to LLVM dialect, and that is something we need to do next.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}